Solving  
Domain-Specific 
Problems Using 
LLMs
Authors: Christopher Semturs,  
Shekoofeh Azizi, Scott Coull,  
Umesh Shankar  
and Wieland Holfelder 

Solving Domain-Specific Problems Using LLMs2
September 2024
Acknowledgements
Reviewers and Contributors
Johan Schalkwyk
Sami Lachgar
Dale Webster
Yun Liu
Connie Fan
Chris Grier
Antonio Gulli
Anant Nawalgaria
Curators and Editors
Antonio Gulli
Anant Nawalgaria
Grace Mollison 
Technical Writer
Karen Lin
Designers
Michael Lanning
Luxi Chen 
Introduction  4
SecLM and the future of cybersecurity  5
 Challenges in cybersecurity  6
 How GenAI can tackle the challenges in cybersecurity  7
 SecLM: An API for cybersecurity tasks  10
  Security-focused large language models  12
  A flexible planning and reasoning framework  16
MedLM and the future of health tech  20
 The potential for GenAI in medical Q&A  20
 The opportunities  21
 The scientific starting point  22
  How to evaluate: quantitative and qualitative  24
 Evaluation in real clinical environments  30
 Task- vs. domain-specific models  31
 Training strategies for Med-PaLM 2  32
Summary  35
Endnotes  36Table of contents
Solving Domain-Specific Problems Using LLMs4
September 2024Introduction
Large language models (LLMs) have emerged as powerful tools for tackling complex 
challenges in numerous domains. While early iterations focused on general-purpose tasks, 
recent developments have highlighted the potential of fine-tuning LLMs to address specific 
problems within specialized fields. This whitepaper explores these concepts in two distinct 
domains: cybersecurity and medicine. Each showcases the unique ability of LLMs to enhance 
existing workflows and unlock new possibilities.
Cybersecurity presents a number of unique challenges for LLMs, including a scarcity of 
publicly available data, a wide diversity of highly technical concepts, and information about 
threats that change on a daily basis. Additionally, sensitive use cases, like malware analysis, 
necessitate specific considerations for model development. We address these challenges ... recent developments 
have highlighted the potential 
of fine-tuning LLMs 
to address specific problems 
within specialized fields.
Solving Domain-Specific Problems Using LLMs5
September 2024by focusing on cybersecurity-specific content and tasks, pairing security-focused language 
models with a suite of supporting techniques to offer improved performance for vital tasks 
like threat identification and risk analysis.
In the field of medicine, LLMs face a different set of obstacles, such as the vast and ever-
evolving nature of medical knowledge and the need to apply said knowledge in a context-
dependent manner that makes accurate diagnosis and treatment a continual challenge. 
LLMs like Med-PaLM, customized for medical applications, demonstrate the ability to answer 
complex medical questions and provide insightful interpretations of medical data, showing 
potential for supporting both clinicians and patients.
Through the lens of these two distinct domains, in this whitepaper we will explore the 
challenges and opportunities presented by specialized data, technical language, and 
sensitive use cases. By examining the unique paths taken by SecLM and Med-PaLM, we 
provide insights into the potential of LLMs to revolutionize various areas of expertise.
SecLM and the future of cybersecurity
Security practitioners face a myriad of challenges, including new and evolving threats, 
operational toil, and a talent shortage. Specialized Generative AI (Gen AI) can help address 
these challenges by automating repetitive tasks, freeing up time for more strategic activities, 
and providing new opportunities to access knowledge.
Solving Domain-Specific Problems Using LLMs6
September 2024Challenges in cybersecurity 
In the movies, we often see information security reduced to the caricature of hoodie-clad 
and headset-wearing hackers with ill intent, armed with ruggedized laptops, tapping away 
furiously until we hear the two magic words: “I’m in.” 
To the extent that you even see the defenders, they are in reactive mode-think war rooms, 
empty coffee cups, people barking orders, and monitors showing the attacker’s every move 
in real-time. 
That is Hollywood; we live in the real world.
In reality, the people who practice cybersecurity - the developers, system administrators, 
SREs, and many junior analysts to whom our work here is dedicated - have the Sisyphean 
task of keeping up with the latest threats and trying to protect complex systems against 
them. Many practitioners’ days are largely filled with repetitive or manual tasks, such as 
individually triaging hundreds of alerts, that take valuable time away from developing more 
strategic defenses. The momentum is definitely not in the defender’s favor; attackers are 
adopting advanced technologies, including artificial intelligence,1 to extend their reach and 
quicken the pace of exploitation. And there are definitely no monitors showing the attacker’s 
every move!
Based on our experience working with users and partners, we see three major challenges in 
the security industry today: threats, toil, and talent . 
• New and evolving threats : The threat landscape is constantly changing, with new 
and increasingly sophisticated attacks emerging all the time. This makes it difficult for 
defenders to keep up with the latest information, and conversely for practitioners to sift 
through that flood of data to identify what’s relevant to them and take action.
Solving Domain-Specific Problems Using LLMs7
September 2024• Operational toil : People working in security operations or DevOps roles often spend a 
significant amount of time on repetitive manual tasks that could be automated or assisted. 
This leads to overload and takes away time from more strategic activities. Excessive focus 
on minutiae also prevents analysts and engineers from seeing the bigger picture that is 
key to securing their organizations.
• Talent shortage : There is a shortage of skilled security professionals, making it difficult 
for organizations to find the people they need to protect their data and systems. Often, 
people enter security-focused roles without much training and with little spare time to 
expand their skills on the job.
Without the ability to address these three challenges, it will be difficult to keep up with the 
demands of modern cybersecurity systems.
How GenAI can tackle the challenges in cybersecurity
We envision a world where novices and security experts alike are paired with AI expertise 
to free themselves from repetition and toil, accomplish tasks that seem impossible to us 
today, and provide new opportunities to share knowledge. Large language models (LLMs) 
and adjacent GenAI techniques can meaningfully improve the working lives of both security 
novices and experienced practitioners. Indeed, in many cases, we have already found that 
GenAI is useful to solve a number of real-world security problems in our challenge areas:
Solving Domain-Specific Problems Using LLMs8
September 2024Persona(e) Challenges faced How Gen AI can help
Security analystAnalysts not familiar with 
each tool’s bespoke schema 
and query language.Translate natural-language 
queries into a domain-specific 
security event query language and 
rules language.
Investigating, clustering, and 
triaging incoming alerts is 
time-consuming and requires 
multiple steps and tools.Autonomous capabilities to 
perform investigation, grouping, 
and classification, incorporating 
context and real-time tool use.
Hard to assemble the right 
series of tailored steps to 
remediate an issue.Personalized, case-specific 
remediation planning in 
user environments.
Threat Researcher or 
System AdministratorAn unknown and obfuscated 
artifact (such as a script 
or binary) is discovered 
and can’t be easily 
analyzed manually.Automated reverse engineering 
with LLM-powered code analysis 
with tool use for de-obfuscation 
and decompilation. Explain, 
analyze, and classify potentially 
malicious artifacts.
CISO teamManual work required to 
identify and summarize the 
most likely threats facing 
the organization.Generate a readable document 
or slide deck, applying the latest 
threat intelligence and findings 
from security tools to the 
specific organization.
IT Administrator 
Dedicated Security TeamHard to understand all the 
ways an attacker could 
access sensitive resources.Identify potential or actual attack 
paths, highlighting key elements 
and remediations.
Solving Domain-Specific Problems Using LLMs9
September 2024Application DevelopersChallenging to determine 
the right places to fuzz-test 
an application.Identify which locations to 
fuzz-test and generate the 
appropriate code.
Application Developers & 
IT AdministratorsKeep access policies 
aligned to the principle of 
least privilege.Given historical access patterns 
and current configuration, 
construct a configuration file 
modification that grants a more 
minimal set of roles.
A person responsible for an 
application or systemPeople don’t always 
understand security concepts 
or how to apply them to their 
environments; they have to 
know how to break a problem 
down, ask questions in many 
places, and then combine 
them to obtain an answer.Give an answer that reflects 
authoritative security expertise 
and, using integrations, is relevant 
to the user’s working environment.
To tackle these problems in a meaningful and holistic way, however, we need a 
multi-layered approach:
• Top layer: existing security tools that understand the relevant context and data, and 
can actuate necessary changes;
• Middle layer: a security-specialized model API with advanced reasoning and 
planning capabilities;
• Bottom layer: datastores of authoritative security intelligence and 
operational expertise
Notably, one of the key benefits of LLMs is their ability to process and synthesize vast 
amounts of heterogenous data – an important capability in the increasingly siloed world 
of cybersecurity data. We seek to leverage that capability to solve challenging security 
Solving Domain-Specific Problems Using LLMs10
September 2024problems, whether by assisting human analysts or through autonomous agents, by combining 
relevant context and authoritative sources with a flexible planning framework in a single API, 
which we call SecLM.
This API offers rich planning capabilities that combine LLMs and other ML models, Retrieval-
Augmented Generation (RAG) to ground results in authoritative data, and tool use to perform 
actions or look up relevant information. We argue that this holistic approach is critical 
because accuracy is so important in security and LLMs alone cannot inherently solve all 
security problems.
SecLM: An API for cybersecurity tasks
Our vision of the SecLM API is to provide a ‘one-stop shop’ for getting answers to security 
questions, regardless of their level of complexity. That is, the engineer or analyst can pose 
questions and refer to data sources with natural language, and expect an answer that 
automatically incorporates the necessary information. However, security problems often 
require a lot of information to be gathered and analyzed using domain-specific reasoning, 
often by experts across several disciplines.
Ideally, one can ask the SecLM API a question in a zero-shot manner and get a high-quality 
response without fussing over prompting or manually integrating external data. In order to 
achieve this in a coherent and seamless manner, it is important to have a well-designed API 
that interacts with LLMs and traditional ML models, the user’s data, and other services to 
accurately complete the task at hand. Due to the complex nature of these security problems, 
we must aim to address the following key requirements:
Solving Domain-Specific Problems Using LLMs11
September 2024• Freshness: The model should be able to access the latest threat and vulnerability 
data, which changes on a daily basis. Due to its cost and duration (often days), 
retraining the model on a daily or hourly basis to incorporate the latest data is not a 
feasible approach.
• User-specific data: The model should be able to operate on the user’s own security 
data within the user’s environment without the risk of exposing that sensitive data 
to others or the infrastructure provider. This rules out any centralized training on 
user data.
• Security expertise: The model should be able to understand high-level security 
concepts and terminology, and break them into manageable pieces that are useful 
when solving the problem. For instance, decomposing a high-level attack strategy (e.g., 
lateral movement) into its constituent components for search or detection.
• User-specific data: The model should be able to reason about the provided security 
data in a multi-step fashion by combining different data sources, techniques, and 
specialized models to solve security problems.
SecLM addresses these challenges through the use of security-specialized LLMs, traditional 
ML models, and a flexible planning framework that enables dynamic use of tools and 
interaction among multiple domain-specialized agents to reason over the provided data. 
Here, we will briefly discuss our approach to training security-specialized models and 
designing the planning framework that drives the SecLM API.
Solving Domain-Specific Problems Using LLMs12
September 2024Security-focused large language models
One of the things we observed in applying LLMs to security is that general-purpose models 
didn’t peform as well as we needed on some security tasks. The reasons for this fall into 
three categories:
• Lack of publicly available security data : LLMs are data-hungry, requiring large pre-
training corpora for best results. At the same time, security data is sensitive so we cannot 
use real security data in training. Moreover, what little data is available publicly is usually 
concentrated on a small number of the most popular security products or on generic 
security content that lacks connection to concrete application. 
• Limited depth of security content : Similarly, there is a certain highly technical language 
that is used to talk about security or express security insights, often crossing disciplines 
from low-level computer science concepts to high-level policy and intelligence analysis. 
To be effective, security LLMs must seamlessly blend this language, connect them to 
their underlying technical concepts, and synthesize relevant, accurate output for security 
analysts and engineers to consume. While there are some high-quality, in-depth articles 
that explain how to address well-known vulnerabilities or attacks, thousands of new 
threats emerge each year.
• Sensitive use cases : There are some use cases in security that general purpose models 
do not handle by design such as abuse areas like malware or phishing. In most cases, 
general-purpose LLMs would actively work to avoid incorporating such tasks or related 
data for fear of increasing risk of misuse or abuse. However, these cases are crucial for 
security practitioners looking to secure their systems, to analyze artifacts, or even for 
testing purposes. 
Taken together, these challenges motivate the development of security-focused LLMs 
that operate across as many security platforms and environments as the humans they will 
ultimately support. To this end, we develop specialized LLMs that have been trained on a 
variety of cybersecurity-specific content and tasks.
Solving Domain-Specific Problems Using LLMs13
September 2024This broad set of supported tasks means that we have to take into account multiple use 
cases and environments when making design decisions, such as choosing the model size and 
composition of training tasks. For example, an LLM with hundreds of billions of parameters 
may maximize reasoning and abstraction capabilities, but might not be ideal for latency-
sensitive or high-volume tasks, like summarizing and categorizing security events.
To ensure the model generalizes to new tasks and security products not directly visible in the 
training data, we have to be very careful with the training regime used to create the models. 
As an example, consider that for many task areas, such as translating natural language into a 
domain-specific query language, it is highly likely that any training data we have will contain 
only a fraction of the eventual targets for our users. In this case, without careful curation 
of the training data, we may inadvertently eliminate the ability of the model to generalize 
to new tasks or data sources that are important to users. Likewise, some data sources are 
particularly sensitive or proprietary and should not be included in generalized training of 
the model. Instead, these data sources should be incorporated into a specialized derivative 
model (using a lightweight, parameter-efficient process) that does not degrade the overall 
performance of the core security-specialized model.
The training process, shown in Figure 1, demonstrates how we leverage each phase of 
training to target specific tasks and types of data to balance performance, generalization, 
and separation of proprietary data. 
Solving Domain-Specific Problems Using LLMs14
September 2024
Figure 1. High-level training flow for core SecLM and specialized derivative models 
As pre-training is the most expensive and time-consuming stage, it makes sense to start 
from a robust foundational model with exposure to the broadest set of training data possible, 
including billions or even trillions of tokens of general text, code, and structured data across 
dozens of languages and formats. This gives us the added benefit of multilingual support, 
which is an important feature for threat intelligence use cases and international users.
From this foundational model, we apply a phase of continued pre-training where we 
incorporate a large collection of open source and licensed content from security blogs, 
threat intelligence reports, detection rules, information technology books, and more. This 
helps develop the specialized language and core technology understanding necessary to 
Solving Domain-Specific Problems Using LLMs15
September 2024perform the broad range of tasks that SecLM models will be trained on in the supervised 
fine-tuning phase. Here, proprietary data is compartmentalized within specific tasks that 
mirror those performed by security experts on a day-to-day basis, including analysis of 
malicious scripts, explanation of command line invocations, explanation of security events, 
summarization of threat intelligence reports, and generation of queries for specialized 
security event management technologies.
Given the diversity of downstream tasks that are expected of the model, evaluating its 
performance can be a challenging exercise, particularly when some categories of tasks may 
experience inherent trade-offs. For this reason, the fine-tuned model is evaluated using a 
number of complementary methods. Several of our downstream tasks, such as malware 
classification and certain types of simple security-focused question answering, can be 
framed as classification problems and a standard battery of classification metrics can be 
used to concretely quantify the performance on those tasks. For other, less quantifiable 
tasks, we can leverage a set of golden responses that we can use to calculate similarity-
based metrics (e.g., ROUGE,2 BLEU,3 BERTScore4), but we can also compare across models 
using automated side-by-side preference evaluations using a separate (oftentimes larger) 
LLM. Finally, given the highly technical nature of security problems and the importance of 
accuracy in our tasks, we rely on expert human evaluators to score outputs using a Likert 
scale and side-by-side preference evaluation. Taken together, these metrics provide us with 
the guidance needed to ensure our fine-tuning training has improved overall model quality, 
and help us direct future changes in model training.
At the conclusion of the fine-tuning stage, we have a model capable of performing many 
of the same core tasks as security experts. However, because of our need to ensure 
generalization across a wide range of user environments and the inherent trade-off among 
some security tasks, the model may still require the use of in-context learning examples, 
retrieval-augmented generation, and parameter-efficient tuning (PET) methods. For example, 
if a new user wanted to leverage SecLM to query and analyze data on a new security platform 
Solving Domain-Specific Problems Using LLMs16
September 2024that was not present during core training, it is likely that the model may need in-context 
examples to help generalize to the new system. Similarly, if a user wanted to incorporate 
specialized knowledge about their network and assets or better align model behavior with 
human security experts, it would be best added via PET adapters trained on their sensitive 
data. Retrieval-augmented generation, meanwhile, allows us to pull in the freshest and most 
recent threat information for the model to process, rather than relying on stale data ingested 
during less frequent training runs.
A flexible planning and reasoning framework
As you might imagine, actually building the underlying framework that orchestrates the 
planning and execution of these complex tasks requires solving some difficult systems 
engineering and machine learning challenges. The example, shown in Figure 2, illustrates how 
SecLM's specialized models can be tied into a broader ecosystem to best leverage fresh, 
user-specific data and authoritative security expertise in a natural and seamless way.
Solving Domain-Specific Problems Using LLMs17
September 2024
Figure 2. SecLM platform leveraging multi-step reasoning to answer a broad, high-level question about 
advanced persistent threat actor activity 
In Figure 2, we have a fairly broad, high-level question regarding the tactics, techniques, and 
procedures (TTPs) of an advanced persistent threat (APT) group, in this example ‘APT41’. The 
analyst asking this question needs to understand what those TTPs are and discover potential 
indications of them in their own network. To answer this question, the SecLM API needs to 
invoke a complex, multi-step planning process to break down the problem into individual 
tasks: 1) Retrieve the necessary information, 2) Extract and synthesize that information, 3) 
Use the information to query the relevant events from the user’s Security Information and 
Event Management (SIEM) product. In the SecLM reasoning framework, this plan can be 
generated statically by security experts or in real-time through a combination of expert 
guidance and highly-capable LLMs using chain-of-thought style prompting.
First, the SecLM API planner retrieves the most recent information about “APT41” from one of 
possibly many of the user’s threat intelligence subscriptions. That raw response is processed 
to extract TTP information and possible indicators of compromise from the voluminous threat 
Solving Domain-Specific Problems Using LLMs18
September 2024intelligence data. Next, a specialized SecLM fine-tuned (using PET) for the query language 
of the SIEM is used to translate those TTPs into concrete clauses in the appropriate syntax 
and using the appropriate schema. Using that query, the API can then directly retrieve 
the matching security events from the SIEM, and finally use SecLM to aggregate all of the 
available information into a comprehensible final response for the analyst.
Overall, the SecLM API would save the analyst in the above example substantial time - 
possibly hours - by automating multiple tedious steps across several different security 
services and systems. Meanwhile, the analyst’s time and attention are available to consider 
the results and plan for follow-up investigations or remediation steps, which may also be 
assisted by the SecLM API. While this is one example of how the SecLM API automatically 
plans and orchestrates operations across multiple models and retrieval sources, there are 
a multitude of such use cases where tool use (e.g., code execution), retrieval-augmented 
generation, specialized models, and long-term memory (e.g., storage of user preferences) 
can help solve challenging security problems and answer difficult questions that save users 
valuable time, even autonomously with the use of agents..
The prompt and response shown in Figure 3 provide another concrete example of how the 
SecLM API can leverage multiple tools and models to solve an otherwise time-consuming 
problem for security analysts and system administrators alike, in this case by automatically 
decoding and analyzing a PowerShell script for malicious activity.  To demonstrate the value 
of our platform, we recently completed a side-by-side analysis with security operations 
and threat intelligence experts, where we compared the end-to-end SecLM platform 
against standalone, general-purpose LLMs on cybersecurity-focused tasks, such as attack 
path analysis, alert summarization, and general security question answering similar to the 
PowerShell example shown here. The results demonstrated a clear preference for SecLM, 
with win rates between 53% and 79% across the security-focused tasks, and underscore the 
importance of a full-featured platform in the domain of cybersecurity.
Solving Domain-Specific Problems Using LLMs19
September 2024
Figure 3. An example response from the SecLM platform using a base64 decoding tool and the SecLM model 
to analyze an obfuscated PowerShell command used in a ‘living off the land’ attack 
In this section, we have seen how a holistic approach that combines large language models 
(LLMs) and authoritative data sources with a flexible planning framework can help security 
practitioners by gathering, aggregating, and intelligently processing security data. We have 
also seen how SecLM and its supporting infrastructure are being built to provide a one-stop 
security platform for experts, junior analysts, and systems administrators. These advances, 
combined with human expertise, can transform the practice of security, obtaining superior 
results with less toil for the people who do it.
Solving Domain-Specific Problems Using LLMs20
September 2024MedLM and the future of health tech
Recent advances in AI for natural language processing (NLP) and foundation models have 
enabled rapid research into novel capabilities in the medical field. This section will dive 
deeper into the challenges of the medical field, and how MedLM solutions can help here - a 
family of foundation models fine-tuned for the healthcare industry. In particular, this section 
illustrates how it started with a specific GenAI model, Med-PaLM, to address these needs.
The potential for GenAI in medical Q&A
Medical question-answering (QA) has always been a grand challenge in artificial intelligence 
(AI). The vast and ever-evolving nature of medical knowledge, combined with the need for 
accurate and nuanced reasoning, has made it difficult for AI systems to achieve human-level 
performance on medical QA tasks.
However, large language models (LLMs) trained on massive datasets of text have shown 
promising results on a variety of medical QA benchmarks. LLMs are able to understand and 
apply complex medical concepts in a way that was not possible for previous generations of 
AI systems.
In addition, the increasing availability of medical data and the growing field of medical NLP 
have created new opportunities for innovation in medical QA. Researchers are now able to 
develop systems that can answer medical questions from a variety of sources, including 
medical textbooks, research papers, and patient records.
Solving Domain-Specific Problems Using LLMs21
September 2024This combination of technical capabilities and data availability provides the groundwork for 
models like Med-PaLM, an LLM aligned and fine-tuned based on the PaLM family of models. 
The development of Med-PaLM is only the start of a journey with the goal of improving health 
outcomes by making the technology available to researchers, clinicians, and other users.
The opportunities
Gen AI has the potential to fundamentally transform the medical field in both diagnostic and 
non-diagnostic aspects, in numerous ways. For example:
• Empowering users to ask questions in the context of the medical history in their health 
record such as “what are good weekend activities for me to consider, given the surgery I 
underwent two weeks ago?”
• Triaging of incoming messages to clinicians from patients by comprehensively 
understanding the urgency and categorizing the type of incoming message given the 
full context of the patient's health history, and flagging or prioritizing the message 
appropriately. 
• Enhancing the patient intake process by moving beyond a fixed set of questions and 
instead adapting based on the patient's responses. This allows for more efficient and 
comprehensive data collection and provides a more cohesive summary to the clinical staff. 
• Implementing a technology that actively monitors patient-clinician conversations and 
provides actionable feedback to the clinician, helping them understand what they 
did great in the interaction and where they might want to improve. Similarly, the same 
technology can help the patient with any questions they might have for the clinician before 
concluding their visit.
• Enabling clinicians to better tackle unfamiliar scenarios or diseases by providing an on-
demand curbside consult or reference materials, similar to having a colleague available for 
conferences as needed.
Solving Domain-Specific Problems Using LLMs22
September 2024This list represents merely a small selection from a vast array of possibilities, illustrating the 
extensive range of options previously considered unattainable with earlier technologies.
The field of medicine also serves as a use case with a strong culture and need for 
responsible innovation. Medical applications are regulated due to the importance of patient 
safety. While GenAI systems can be used to develop new diagnostic tools, treatment plans, 
and educational materials, it is important to validate the safety and efficacy of such systems  
before their implementation in clinical practice. This means that scientific experimentation 
requires a thoughtful, phased approach with retrospective studies (i.e., using de-identified 
data from past cases so that research does not impact patient care) happening before 
prospective studies (i.e., running the model on newly collected data in a specific setting of 
interest, sometimes interventionally so that impact on patient care can be measured).
The scientific starting point
Many AI systems developed for medicine today lack the ability to interact with users, but 
instead produce structured outputs such as “yes” or “no”, or a numerical output. While this 
type of output is useful in many scenarios for clinicians, this output is inflexible. Models also 
need to be created for every application, which slows down innovation. 
In our view,5 medicine revolves around caring for people, and needs to be human-centric. As 
such, an ambitious goal would be a flexible AI system that can interact with people and assist 
in many different scenarios while taking into account the appropriate context.              To 
create such a system, it is essential to incorporate a wide range of experiences, perspectives, 
and expertise when building AI systems. Data and algorithms should go       hand in hand with 
language and interaction, empathy, and compassion.
The objective behind this project is to enhance the effectiveness, helpfulness, and safety     
of AI models in medicine by incorporating natural language and facilitate interactivity for and 
between clinicians, researchers, and patients. To bring this vision to life, we took the initial 
Solving Domain-Specific Problems Using LLMs23
September 2024step in reimagining conversational AI systems in medicine with Med-PaLM, Google's LLM 
designed to provide high-quality, authoritative answers to medical questions. The QA task 
in particular was a great candidate for starting the journey, as it combines evaluations of 
reasoning capabilities and understanding, and allows for extensive evaluations across many 
dimensions on the outputs.
The recent progress in foundation models,6 such as LLMs, as large pre-trained AI systems 
that can be easily adapted for various domains and tasks presents an opportunity to 
rethink the development and use of AI in medicine on a broader scale. These expressive 
and interactive models hold significant potential to make medical AI more performant, safe, 
accessible, and equitable by flexibly encoding, integrating, and interpreting medical data 
at scale.
Here is a description of how Med-PaLM improved over time:
• Our first version of Med-PaLM, described in a preprint in late 2022 and published in 
Nature in July 2023,7 was the first AI system to exceed the passing mark on US Medical 
License Exam (USMLE)-style questions.8 The study also evaluated long-form answers and 
described a comprehensive evaluation framework.
• In March 2023, Med-PaLM 2 was announced and described in a preprint.9 It demonstrated 
rapid advancements, both on USMLE-style questions and on long-form answers. Med-
PaLM 2 achieves an accuracy of 86.5% on USMLE-style questions, a 19% leap over our 
own results from Med-PaLM. As evaluated by physicians, the model's long-form answers 
to consumer medical questions improved substantially compared to earlier versions of 
Med-PaLM or the underlying non-medically tuned base models. It also demonstrated 
how fine-tuning and related techniques can truly harness the power of LLMs in a domain-
specific way. 
Solving Domain-Specific Problems Using LLMs24
September 2024These advances reflect our belief that innovation can take major strides in a short period of 
time, and be done responsibly and with rigor.
How to evaluate: quantitative and qualitative
Developing accurate and authoritative medical question-answering AI systems has been a 
long-standing challenge marked by several research advances over the past few decades. 
While the task is broad and spans various dimensions including logical reasoning and the 
retrieval of medical knowledge, tackling USMLE-style questions has gained prominence 
as a widely acceptable and challenging benchmark for evaluating medical question 
answering performance.
Figure 4 shows an example of a USMLE-style question. Individuals taking the test are given 
a concise patient profile that includes information such as their symptoms and prescribed 
medications. A medical question is presented based on the provided scenario, and test-
takers are required to choose the correct response from multiple choices.
Solving Domain-Specific Problems Using LLMs25
September 2024
Figure 4. An example of a USMLE-style question 
Correctly answering the question requires the individual taking the test to comprehend  
symptoms, interpret a patient’s test results, engage in intricate reasoning regarding the 
probable diagnosis, and ultimately select the correct choice for the most suitable disease, 
test, or treatment combination. In summary, a combination of medical comprehension and 
understanding, knowledge retrieval, and reasoning is vital for success. It takes years of 
education and training for clinicians to develop the knowledge needed to consistently answer 
these questions accurately.
Solving Domain-Specific Problems Using LLMs26
September 2024As every clinician will attest to, merely passing the USMLE does not indicate proficiency 
in diagnosing or managing patients clinically. Instead, USMLE is a specific assessment 
of knowledge and reasoning based on concrete scenarios. Nevertheless, USMLE serves 
as a useful benchmark since the answer is typically documented and evaluation can be 
conducted programmatically at scale. This contributed to its historical popularity as a 
benchmark in scientific research as a grand challenge in the past, which makes it so powerful 
to demonstrate how technology facilitates significant advancements.
Figure 5. Med-PaLM 2 reached expert-level performance on the MedQA medical exam benchmark 
Med-PaLM was the first AI model to exceed the passing mark, reaching the performance of 
67%, and Med-PaLM 2 was the first AI model to reach 86.5%, which indicates expert-level 
performance (Figure 5).
Crucially, to establish a more meaningful connection to potential future developments and 
enable the detailed analysis required for real-world clinical applications, the scope of the 
evaluation methods proposed by Med-PaLM framework extends beyond mere accuracy in 
Solving Domain-Specific Problems Using LLMs27
September 2024multiple-choice questions. The evaluation extends to qualitative assessment of factuality, 
use of expert knowledge in reasoning, helpfulness, health equity, and potential harm when 
providing long-form answers to open-ended questions.
The rubric for evaluation by expert clinicians includes:
• How does the answer relate to the consensus in the scientific and clinical community?
• What is the extent of possible harm?
• What is the likelihood of possible harm?
• Does the answer contain any evidence of correct reading comprehension?
• Does the answer contain any evidence of correct recall of knowledge?
• Does the answer contain any evidence of correct reasoning steps?
• Does the answer contain any evidence of incorrect reading comprehension?
• Does the answer contain any evidence of incorrect recall of knowledge?
• Does the answer contain any evidence of incorrect reasoning steps?
• Does the answer contain any content it shouldn’t?
• Does the answer omit any content it shouldn’t?
• Does the answer contain info that is inapplicable or inaccurate for any particular 
medical demographic?
• How well does the answer address the intent of the question?
• How helpful is this answer to the user? Does it enable them to draw a conclusion or help 
clarify next steps?
Solving Domain-Specific Problems Using LLMs28
September 2024Figure 6 shows the evaluation rubric applied to an example output by Med-PaLM 2.
Figure 6. Example of clinician review of Med-PaLM 2
Solving Domain-Specific Problems Using LLMs29
September 2024The human evaluation for Med-PaLM follows this procedure:
• Each question is presented to both Med-PaLM and a board-certified physician.
• Both Med-PaLM and the physician independently provide their answers.
• Those answers are then presented in a blinded way (i.e., who provided each answer is not 
indicated) to separate raters.
• Additionally, direct side-by-side comparisons were conducted, such as determining which 
answer is better between A and B (where A and B are blinded and could refer to physician-
provided or outputs from different AI models).
It is important to emphasize that the evaluation primarily focuses on the substance over the 
style / delivery. In certain instances, a clinician’s response may be concise yet effectively 
meets the evaluation criteria, while in other scenarios, a more detailed but verbose answer 
may be more appropriate.
Our human evaluation results as of May 2023 indicate that the answers provided by 
our models compare well to those from physicians across several critical clinically 
important axes.
Since conducting evaluations with scientific rigor requires the involvement of expert laborers, 
such as board-certified physicians, the process is notably costlier than evaluating multiple-
choice questions. It is promising to see that other studies10 have adopted and expanded upon 
the suggested framework for the purpose of being comparative and aligned with AI safety. 
The expert evaluation plays a vital role critical in discerning style (i.e., delivery) and content 
as well as correctness.
We also learned that more work remains, including improvements along specific evaluation 
axes where physicians’ performance remained superior.
Solving Domain-Specific Problems Using LLMs30
September 2024The detailed results are the cornerstone of understanding and identifying areas in need of  
future scientific modeling and evaluation, as well as determining the feasibility of the next 
step in our journey.
Although quantitative and qualitative improvements can be made in order to achieve 
perfect performance on benchmarks, the technology can still provide practical value in 
real-world settings.
Evaluation in real clinical environments
The integration of technology into the clinical environment is a well-established area, and 
Google has gained its own expertise5 in the field through screening for diabetic retinopathy. 
One of the main insights learned is that achieving high performance on retrospective 
datasets does not automatically translate into clinical performance. It is imperative to 
carefully validate AI solutions in real-world environments in a meticulous manner to ensure 
their robustness and reliability.
Each technology integrated into a patient’s journey, whether it falls under regulatory 
oversight or not, is encouraged to adhere to these scientific steps:
• Retrospective evaluation : Evaluate the technology against real-world data collected 
from past cases.
• Prospective observational (non-interventional) : Evaluate on newly collected real-world 
data, but ensure that the outputs of the technology do not impact patient care or safety. 
An example is feeding live data into the technology and then having the appropriate 
experts evaluate the technology’s output.
Solving Domain-Specific Problems Using LLMs31
September 2024• Prospective interventional : Deploy the technology within a live clinical environment 
with consented patients and influence patient care and potentially health outcomes. 
This step requires a detailed and IRB-approved study protocol and care taken to ensure 
patient safety.
These steps are crucial not just for assessing the model's performance on new unseen data 
but also, more significantly, for evaluating the effectiveness of the end-to-end system when 
integrated into real workflows. Occasionally, the optimal way to use GenAI models like Med-
PaLM may diverge from initial assumptions, and introducing a new tool into a clinical workflow 
might require unexpected adjustments to the overall process.11,12 End-to-end assessment is 
essential for understanding the role and benefit of the technology and tailoring AI solutions 
to meet the needs effectively.
Task- vs. domain-specific models
Med-PaLM7 highlighted the significance and value of a specialized model for the medical 
domain. Med-PaLM 2, an aligned and fine-tuned iteration of PaLM 2 tailored to medical 
knowledge, achieves a ninefold enhancement in precise reasoning compared to the 
baseline.13 However, it's crucial to recognize that excelling in one medical domain task doesn't 
necessarily guarantee and imply success in a different medical domain task. For instance, 
does a great general medical QA system also perform well on a mental health assessment 
task? While it's reasonable to assume that a demonstrated understanding of clinical 
knowledge can generalize effectively to tasks heavily relying on this knowledge, each specific 
task requires validation and possible adaptation, such as the measurement of psychiatric 
functioning,14 before proceeding further.
Solving Domain-Specific Problems Using LLMs32
September 2024The medical domain also extends well beyond textual information. The practice of medicine is 
inherently multi-modal and incorporates information from images, electronic health records, 
sensors, wearables, genomics, and more. Multimodal versions15 of MedLM and related 
approaches16,17,18 are in early stages of research, and follow the same validation principles and 
workflow integration approach. We will be observing the multimodal-enabled set of use-
cases evaluated and deployed in the field.
Lastly, a medically specialized model can be applied not only to clinical use cases that relate 
directly to patient care, but also to use cases that benefit from leveraging medical knowledge 
in a flexible way. An example is in scientific discovery, where Med-PaLM can be used to 
accurately identify genes associated with biomedical traits.19 We'll be exploring a  breadth 
of possibilities with vertical-specific models, and we expect new applications and ideas to 
emerge in the field over the next few years. We’re also exploring safe and responsible ways 
to bring these models to the healthcare industry. With MedLM, a suite of models fine-tuned 
for healthcare use cases, built on Med-PaLM 2, we’re making solutions commercially available 
so healthcare organizations can build GenAI use cases suitable for their workflows.
Training strategies for Med-PaLM 2
Med-PaLM 2 is an advancement of the base LLM model PaLM 2, Google's enhanced LLM with 
substantial performance improvements on multiple LLM benchmark tasks. To tailor Med-
PaLM 2 for medical applications, instruction fine-tuning7 was performed using MultiMedQA,7 
including MedQA, MedMCQA, HealthSearchQA, LiveQA, and MedicationQA datasets. Dataset 
mixture ratios were empirically determined. 
To enhance the specialized variant of Med-PaLM 2 focusing on multiple-choice questions, 
a range of prompting strategies including few-shot prompting, chain-of-thought (CoT) 
prompting, and self-consistency were employed. CoT involves augmenting each few-shot 
example in a prompt with a step-by-step explanation towards the final answer, allowing 
Solving Domain-Specific Problems Using LLMs33
September 2024the language model to condition on its own intermediate outputs for multi-step problem-
solving. Self-consistency plays a role in enhancing the model's performance on multiple-
choice questions by sampling multiple explanations and answers from the model, with the 
final answer determined by a majority vote among the generated options. These strategies 
collectively improve the model's ability to reason and provide more accurate responses to 
complex and multi-faceted queries.
Another noteworthy methodological improvement is the introduction of ensemble refinement 
(ER), which builds on other techniques that involve conditioning an LLM on its own 
generations before producing a final answer. In the first stage, multiple possible explanations 
and answers are stochastically generated via temperature sampling. In the second stage, 
the model is conditioned on the original prompt, question, and generated contents from 
the first stage, resulting in the production of a refined explanation and answer. This process 
facilitated the effective aggregation of answers, extending its utility beyond questions with 
a limited set of potential answers, thereby enhancing the overall performance of the model. 
The overall mechanism of ensemble refinement is depicted in Figure 7.
Figure 7. Ensemble refinement (ER) in Med-PaLM 2.  This approach involves conditioning an LLM on multiple 
potential reasoning pathways it generates, facilitating the answer refinement and improvement
Solving Domain-Specific Problems Using LLMs34
September 2024The goal behind the inception of the Med-PaLM research effort was to improve health 
outcomes via using and advancing emerging AI technologies. Achieving expert-level 
performance in medical QA tasks was the first step, with many more to follow in close 
collaboration with the clinical community as we progress on this journey. 
Our health research experience at Google demonstrated repeatedly that technology is often 
not the sole challenge in applying AI productively to healthcare. Instead, many other factors, 
including thoughtful evaluation strategies and working on clinically meaningful applications 
in partnership with clinicians and a broad cross-functional team, are pivotal to success.5 This 
valuable insight is likely applicable to other vertical domains as well.
As AI technology matures and moves closer to practical use cases and real-world scenarios, 
careful multi-step evaluations, including both retrospective and prospective assessments, 
are beneficial to better understand the real role and benefits of the technology in the whole 
workflow. Guidance by a clinical partner improves the chances of building the right solution 
for better health outcomes. Many promising applications lie in the collaboration of healthcare 
workers and technology, combining the strengths of both. It is also important to use GenAI 
systems in a way that is respectful of patients' autonomy and privacy.
For the foreseeable future, it is reasonable to assume that models customized for specific 
applications or domains will yield better results, and we are tracking trends and any 
convergence in performance between general and specific models in the years ahead. For 
Med-PaLM specifically, our research progress will be tracked at the Med-PaLM research 
webpage.20 We aim to make progress more broadly in the field of using AI and GenAI for the 
betterment of patients, clinicians, and researchers.
Solving Domain-Specific Problems Using LLMs35
September 2024Summary
This whitepaper explores the potential of LLMs in tackling complex challenges within specific 
domains, with a particular focus on healthcare and cybersecurity.
• Cybersecurity : The ever-evolving landscape of cyber threats demands innovative 
solutions. SecLM, an LLM designed for cybersecurity, acts as a force multiplier for 
security professionals by intelligently processing vast amounts of data. This empowers 
them to analyze and respond to threats more effectively. The vision for SecLM is to create 
a comprehensive platform that caters to the diverse needs of security practitioners, 
regardless of their expertise. The combination of LLMs and human expertise has the 
potential to revolutionize the field of cybersecurity, achieving superior results with 
less effort.
• Healthcare : Healthcare data is increasing in quantity and complexity, leading to a 
need for innovative solutions to render medical information more helpful, useful, and 
accessible. MedLM, a family of models fine-tuned for the healthcare industry, can help 
unlock knowledge and make medicine more effective. MedLM is built on Med-PaLM, 
an LLM developed for medical applications. Med-PaLM has demonstrated expert-level 
performance in medical question-and-answering tasks. This achievement is just the first 
step in a journey towards improving health outcomes through the utilization of GenAI. The 
key takeaway from this research is that technology alone is not enough. Collaboration 
with the clinical community and careful multi-step evaluations are crucial for successful 
application of LLMs in healthcare. Going forward, vertical-specific models like the MedLM 
foundation models are expected to yield even better results for specific applications of 
interest, furthering the potential of AI in healthcare.
This whitepaper showcases the possibilities of LLMs in solving domain-specific problems. By 
leveraging the power of these advanced models, combined with human expertise and careful 
implementation, we can tackle complex challenges and achieve breakthrough advancements 
in various fields, for the benefit of peoples’ lives.
Solving Domain-Specific Problems Using LLMs36
September 2024Endnotes
1. Cantos, J., et al., 2023. Threat Actors are Interested in Generative AI, but Use Remains Limited. [online] 
Available at: https://cloud.google.com/blog/topics/threat-intelligence/threat-actors-generative-ai-limited/ .
2. Lin, C.Y., et al., 2003. Automatic Evaluation of Summaries Using n-gram Co-occurrence Statistics. [online] 
Available at: https://aclanthology.org/N03-1020.pdf .
3. Papineni, K., et al., 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. [online] Available 
at: https://aclanthology.org/P02-1040.pdf .
4. Zhang, T., et al., 2019. BERTScore: Evaluating Text Generation with BERT. [online] Available at: https://
openreview.net/attachment?id=SkeHuCVFDr&name=original_pdf .
5. Google, 2023. 5 myths about medical AI, debunked. [online] Available at: https://blog.google/technology/
health/5-myths-about-medical-ai-debunked/ .
6. Bommasani, R., et al., 2021. On the opportunities and risks of foundation models. arXiv preprint 
arXiv:2108.07258. [online] Available at: https://arxiv.org/pdf/2108.07258 .
7. Singhal, K., et al., 2023. Large language models encode clinical knowledge. Nature, 620(7972), pp.172-180. 
[online] Available at: https://www.nature.com/articles/s41586-023-06291-2 .
8. Jin, D., et al., 2021. What disease does this patient have? a large-scale open domain question answering 
dataset from medical exams. Applied Sciences, 11(14), p.6421.
9. Singhal, K., et al., 2023. Towards expert-level medical question answering with large language models. arXiv 
preprint arXiv:2305.09617. [online] Available at: https://arxiv.org/abs/2305.09617 .
10. Bernstein, I.A., et al., 2023. Comparison of ophthalmologist and large language model chatbot responses 
to online patient eye care questions. JAMA Network Open, 6(8), pp.e2330320-e2330320. [online] Available at: 
https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808557 .
11. Beede, E., et al., 2019. A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the 
Detection of Diabetic Retinopathy. [online] Available at: https://dl.acm.org/doi/abs/10.1145/3313831.3376718 .
12. Pedersen, S., et al., 2021. Redesigning Clinical Pathways for Immediate Diabetic Retinopathy Screening 
Results. NEJM Catalyst, July. [online] Available at: https://catalyst.nejm.org/doi/pdf/10.1056/CAT.21.0096 .
13. Google, 2023. Google I/O Keynote 2023. [online] Available at:  https://www.youtube.com/live/
cNfINi5CNbY?si=jQFi-Y3mG0rGD3Xd&t=810 .
Solving Domain-Specific Problems Using LLMs37
September 202414. Galatzer-Levy, I.R., et al., 2023. The Capability of Large Language Models to Measure Psychiatric 
Functioning. arXiv preprint arXiv:2308.01834. [online] Available at: https://arxiv.org/abs/2308.01834 .
15. Tu, T., et al., 2023. Towards generalist biomedical AI. arXiv preprint arXiv:2307.14334. [online] Available at: 
https://arxiv.org/abs/2307.14334 .
16. Liu, X., et al., 2023. Large Language Models are Few-Shot Health Learners. arXiv:2305.15525. [online] 
Available at: https://arxiv.org/abs/2305.15525 .
17. Belyaeva, A., et al., 2023. Multimodal LLMs for health grounded in individual-specific data. arXiv:2307.09018. 
[online] Available at: https://arxiv.org/abs/2307.09018 .
18. Shawn, X., et al., 2022. ELIXR: Towards a general purpose X-ray artificial intelligence system through 
alignment of large language models and radiology vision encoders. arXiv:2308.01317. [online] Available at: 
https://arxiv.org/abs/2308.01317 .
19. Tu, T., et al., 2023. Genetic Discovery Enabled by a Large Language Model. [online] Available at: https://www.
biorxiv.org/content/10.1101/2023.11.09.566468v1.full.pdf .
20. Med-PaLM, [n.d.]. Homepage. [online] Available at: https://g.co/research/medpalm .
Embeddings  
& Vector Stores
Authors: Anant Nawalgaria  
and Xiaoqi Ren
Embeddings & Vector Stores2
September 2024
Reviewers and Contributors
Antonio Gulli
Grace Mollison
Ruiqi Guo
Iftekhar Naim
Jinhyuk Lee
Alan Li
Patricia Florissi
Andrew Brook
Omid Fatemieh
Zhuyun Dai
Lee Boonstra
Per Jacobsson
Siddhartha Reddy Jonnalagadda
Xi Cheng
Raphael Hoffmann
Curators and Editors
Antonio Gulli
Anant Nawalgaria
Grace Mollison Technical Writer
Joey Haymaker
Designer
Michael Lanning Acknowledgements
Introduction  5
Why embeddings are important  6
 Types of embeddings  9
  Text embeddings  9
   Word embeddings  11
   Document embeddings  15
    Shallow BoW models  16
    Deeper pretrained large language models  18
  Image & multimodal embeddings  22
  Structured data embeddings  24
   General structured data  24
   User/item structured data  25
  Graph embeddings  25
 Training Embeddings  26Table of contents

Vector search  28
 Important vector search algorithms  29
  Locality sensitive hashing & trees  30
  Hierarchical navigable small worlds  33
  ScaNN  34
Vector databases  37
 Operational considerations  39
Applications  40
 Q & A with sources (retrieval augmented generation)  42
Summary  46
Endnotes  48
Embeddings & Vector Stores5
September 2024Introduction
Modern machine learning thrives on diverse data—images, text, audio, and more. This 
whitepaper explores the power of embeddings, which transform this heterogeneous data into 
a unified vector representation for seamless use in various applications.
We'll guide you through:
• Understanding Embeddings:  Why they are essential for handling multimodal data and 
their diverse applications.
• Embedding Techniques:  Methods for mapping different data types into a common 
vector space.These low-dimensional numerical 
representations of real-world data 
significantly helps efficient large-
scale data processing and storage 
by acting as means of lossy 
compression of the original data.
Embeddings & Vector Stores6
September 2024• Efficient Management:  Techniques for storing, retrieving, and searching vast collections 
of embeddings.
• Vector Databases:  Specialized systems for managing and querying embeddings, 
including practical considerations for production deployment.
• Real-World Applications:  Concrete examples of how embeddings and vector databases 
are combined with large language models (LLMs) to solve real-world problems.
Throughout the whitepaper, code snippets provide hands-on illustrations of key concepts.
Why embeddings are important
In essence, embeddings are numerical representations of real-world data such as text, 
speech, image, or videos. They are expressed as low-dimensional vectors where the 
geometric distances of two vectors in the vector space is a projection of the relationships 
between the two real-world objects that the vectors represent. In other words they help you 
with providing compact representations of data of different types, while simultaneously also 
allowing you to compare two different data objects and tell how similar or different they are 
on a numerical scale: for example: The word ‘computer’ has a similar meaning to the picture 
of a computer, as well as the word ’laptop’ but not to the word ‘car’. These low-dimensional 
numerical representations of real-world data significantly helps efficient large-scale data 
processing and storage by acting as means of lossy compression of the original data while 
retaining its important properties.
Embeddings & Vector Stores7
September 2024One of the key applications for embeddings is retrieval and recommendations, where the 
result is usually from a massive search space. For example, Google Search is a retrieval  with 
the  search space of the whole internet. Today’s retrieval and recommendation systems’ 
success depends on the following:
1. Precomputing the embeddings for billions items of the search space.
2. Mapping query embeddings to the same embedding space.
3. Efficient computing and retrieving of the nearest neighbors of the query embeddings in 
the search space.
Embeddings also shine in the world of multimodality. Most applications work with large 
amounts of data of various modalities: text, speech, image, and videos to name a few. 
Because every entity or object is represented in its own unique format, it’s very difficult 
to project these objects into the same vector space that is both compact and informative. 
Ideally, such a representation would capture as much of the original object’s characteristics 
as possible. An embedding  refers to the projected vector of an object from an input space to 
a relatively low-dimensional vector space. Each vector is a list of floating point numbers.
Embeddings & Vector Stores8
September 2024
Figure 1. Projecting objects/content into a joint vector space with semantic meaning
Ideally the embeddings are created so they place objects with similar semantic properties 
closer in the embedding space (a low-dimensional vector space where items can be 
projected). The embeddings can then be used as a condensed, meaningful input in 
downstream applications. For example, you can use them as features for ML models, 
recommender systems, search engines, and many more. So your data not only gets a 
compact numerical representation, but this representation also preserves the semantic 
meanings for a specific task or across a variety of tasks. The fact that these representations 
are task-specific means you can generate different embeddings for the same object, 
optimized for the task at hand. 
Embeddings & Vector Stores9
September 2024Types of embeddings
Embeddings aim to obtain a low dimensional representation of the original data while 
preserving most of the ‘essential information’. The types of data an embedding represents 
can be of various different forms.  Below you’ll see some standard techniques used for 
different types of data, including text and image.
Text embeddings
Text embeddings are used extensively as part of natural language processing (NLP). They 
are often used to embed the meaning of natural language in machine learning for processing 
in various downstream applications such as text generation, classification, sentiment 
analysis, and more. These embeddings broadly fall into two categories: token/word and 
document embeddings.
Before diving deeper into these categories, it’s important to understand the entire lifecycle 
of text: from its input by the user to its conversion to embeddings. 
Figure 2. The process of turning text into embeddings
It all starts with the input string which is split into smaller meaningful pieces called tokens.  
This process is called tokenization . Commonly, these tokens are wordpieces, characters, 
words, numbers, and punctuations using one of the many existing tokenization techniques.1 
After the string is tokenized, each of these tokens is then assigned a unique integer value 
Embeddings & Vector Stores10
September 2024usually in the the range: [0, cardinality of the total number of tokens in the corpus]. For 
example, for a 16 word vocabulary the IDs would range between 0-15. This value is also 
referred to as token ID. These tokens can be used to represent each string as a sparse 
numerical vector representation of documents used for downstream tasks directly, or after 
one-hot encoding. One-hot encoding is a binary representation of categorical values where 
the presence of a word is represented by 1, and its absence by 0. This ensures that the token 
IDs are treated as categorical values as they are, but often results in a dense vector the size 
of the vocabulary of the corpus. Snippet 1 and Figure 3 show an example of how this can be 
done using Tensorflow.
# Tokenize the input string data
from tensorflow.keras.preprocessing.text import Tokenizer
data = [
  "The earth is spherical.",
  "The earth is a planet.",
  "I like to eat at a restaurant." ]
# Filter the punctiations, tokenize the words and index them to integers  
tokenizer = Tokenizer(num_words= 15, filters= '!"#$%&()*+,-./:;<=>?[\\]^_'{|}~\t\n' , lower=True, 
split=' ')
tokenizer.fit_on_texts(data)
# Translate each sentence into its word-level IDs, and then one-hot encode those IDs 
ID_sequences = tokenizer.texts_to_sequences(data)
binary_sequences = tokenizer.sequences_to_matrix(ID_sequences)
print("ID dictionary:\n" , tokenizer.word_index)
print("\nID sequences:\n" , ID_sequences)
print("\n One-hot encoded sequences:\n" , binary_sequences )
Snippet 1. Tokenizing,  indexing and one-hot encoding strings
Embeddings & Vector Stores11
September 2024
Figure 3. Output of Snippet 1
However, since these Integer IDs (or their corresponding one-hot encoded vectors) are 
assigned randomly to words, they lack any inherent semantic meaning. This is where 
embeddings are much more useful. Although it’s possible to embed character and sub-word 
level tokens as well, let us look at word and document embeddings to understand some of 
the methods behind them.
Word embeddings
In this section, you’ll see a few word embedding techniques and algorithms to both train 
and use word embeddings. While there are many ML driven algorithms developed over 
time optimized for different objectives, the most common ones are GloVe,2 SWIVEL,3 and 
Word2Vec.4 Word embeddings or sub-word embeddings can also be directly obtained from 
hidden layers of language models. However, the embeddings will be different for the same 
word in different contexts of the text. This section focuses on lightweight, context-free 
word embedding and leaves the context-aware document embeddings for the document 
embeddings section. Word embedding can be directly applied to downstream tasks like 
named entity extraction and topic modeling.
Word2Vec is a family of model architectures that operates on the principle of “the semantic 
meaning of a word is defined by its neighbors”, or words that frequently appear close to each 
other in the training corpus. This method can be both used to train your own embeddings 
Embeddings & Vector Stores12
September 2024from large datasets or be quickly integrated through one of the readily available pre-trained 
embeddings available online.5 The embeddings for each word - which are essentially fixed 
length vectors - are randomly initialized to kick off the process, resulting in a matrix of shape 
(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table 
after the training process is completed using one of the following methods (see Figure 4). 
• The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using 
the embeddings of the surrounding words as input. This method is agnostic to the order 
of the surrounding words in the context. This approach is fast to train and is slightly more 
accurate for frequent words.
• The skip-gram approach: The setup is inverse of that of CBOW, with the middle word 
being used to predict the surrounding words within a certain range. This approach is 
slower to train but works well with small data and is more accurate for rare words.
Figure 4. Diagram explaining how CBOW and Skip-Gram methods work
Embeddings & Vector Stores13
September 2024The Word2Vec algorithms can also be extended to the sub-word level, which has been the 
inspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec 
is that although it accounts well for local statistics of words within a certain sliding window, it 
does not capture the global statistics (words in the whole corpus). This shortcoming is what 
methods like the GloVe algorithm address.
GloVe is a word embedding technique that leverages both global and local statistics of words. 
It does this by first creating a co-occurrence matrix, which represents the relationships 
between words. GloVe then uses a factorization technique to learn word representations 
from the co-occurrence matrix. The resulting word representations are able to capture both 
global and local information about words, and they are useful for a variety of NLP tasks.
In addition to GloVE, SWIVEL is another approach which leverages the co-occurrence 
matrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative 
Sampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into 
account the co-occurrence of words within a fixed window of its neighboring words. 
Furthermore, SWIVEL also considers unobserved co-occurrences and handles it using a 
special piecewise loss, boosting its performance with rare words. It is generally considered 
only slightly less accurate than GloVe on average, but is considerably faster to train. This is 
because it leverages distributed training by subdividing the Embedding vectors into smaller 
sub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 
below demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and 
visualizing them in a 2D space, and computing nearest neighbors.
Word embeddings can be directly used in some downstream tasks like Named Entity 
Recognition (NER).
Embeddings & Vector Stores14
September 2024from gensim.models import Word2Vec 
import gensim.downloader as api
import pprint
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np
def tsne_plot(models, words, seed= 23):
  "Creates a TSNE models & plots for multiple word models for the given words"
  plt.figure(figsize=( len(models)* 30, len(models)* 30))
  model_ix = 0
  for model in models:
    labels = []
    tokens = []
    for word in words:
      tokens.append(model[word])
      labels.append(word)
    tsne_model = TSNE(perplexity= 40, n_components= 2, init='pca', n_iter= 2500, random_state=seed) 
    new_values = tsne_model.fit_transform(np.array(tokens))
    x = []
    y = []
    for value in new_values:
      x.append(value[ 0])
      y.append(value[ 1])
    model_ix += 1
    plt.subplot( 10, 10, model_ix)
    for i in range (len(x)):
      plt.scatter(x[i],y[i])
      plt.annotate(labels[i],
            xy=(x[i], y[i]),
            xytext=( 5, 2),
            textcoords= 'offset points' ,
            ha= 'right',
            va= 'bottom' )
  plt.tight_layout()
  plt.show()
v2w_model = api.load( 'word2vec-google-news-300' )
glove_model = api.load( 'glove-twitter-25' )
print("words most similar to 'computer' with word2vec and glove respectively:" )
pprint.pprint( v2w_model.most_similar( "computer" )[:3])
pprint.pprint( glove_model.most_similar( "computer" )[:3]) 
pprint.pprint( "2d projection of some common words of both models" )
sample_common_words= list(set(v2w_model.index_to_key[ 100:10000 ]) 
                        & set(glove_model.index_to_key[ 100:10000 ]))[:100]
tsne_plot([v2w_model, glove_model], sample_common_words)
Snippet 2. Loading and plotting GloVe and Word2Vec embeddings in 2D
Embeddings & Vector Stores15
September 2024Figure 5 Shows semantically similar words are clustered differently for the two algorithms
Figure 5. 2D visualization of pre-trained GloVe and Word2Vec word embeddings
Document embeddings
Embedding documents to low-dimensional dense embedding has attracted long-lasting 
interests since the 1980s. Document embeddings can be used in various applications, 
including semantic search, topic discovery, classification, and clustering to embed 
the meaning of a series of words in paragraphs and documents and use it for various 
Embeddings & Vector Stores16
September 2024downstream applications. The evolution of the embeddings models can mainly be 
categorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained 
large language models.
Shallow BoW models
Early document embedding works follow the bag-of-words (BoW) paradigm, assuming a 
document is an unordered collection of words. These early works include latent semantic 
analysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses 
a co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a 
bayesian network to model the document embeddings. Another famous bag-of-words family 
of document embeddings is TF-IDF (term frequency-inverse document frequency) based 
models, which are statistical models that use the word frequency to represent the document 
embedding. TF-IDF-based models can either be a sparse embedding, which represents the 
term-level importance, or can be combined with word embeddings as a weighting factor to 
generate a dense embedding for the documents. For example, BM25, a TF-IDF-based bag-
of-words model, is still a strong baseline in today’s retrieval benchmarks.9
However,  the bag-of-words paradigm also has two major weaknesses: both the word 
ordering and the semantic meanings are ignored. BoW models fail to capture the sequential 
relationships between words, which are crucial for understanding meaning and context. 
Inspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document 
embeddings using (shallow) neural networks. The Doc2Vec model adds an additional 
‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec 
as illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other 
word embeddings to predict a random word in the paragraph. After training, for existing 
paragraphs or documents, the learned embeddings can be directly used in downstream 
tasks. For a new paragraph or document, extra inference steps need to be performed to 
generate the paragraph or document embedding.
Embeddings & Vector Stores17
September 2024
Figure 6. Doc2vec CBOW model
Snippet 3 below shows how you can train your own doc2Vec models on a custom corpus:
from gensim.test.utils import common_texts
from gensim.models.Doc2Vec import Doc2Vec, TaggedDocument
from gensim.test.utils import get_tmpfile
#train model on a sequence of documents tagged with their IDs
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate (common_texts)]
model = Doc2Vec(documents, vector_size= 8, window= 3, min_count= 1, workers= 6)
# persist model to disk, and load it to infer on new documents
model_file = get_tmpfile( "Doc2Vec_v1" )
model.save(model_file)
model = Doc2Vec.load(model_file)  
model.infer_vector([ "human", "interface" ])
Snippet 3. Self-supervised Training and inference using Doc2Vec on private corpus
Embeddings & Vector Stores18
September 2024The success of applying neural networks in the embedding world inspired an increasing 
interest in using deep neural networks to generate embeddings. 
Deeper pretrained large language models
Motivated by the development of deep neural networks, different embedding models and 
techniques were proposed, and the state-of-the-art models are refreshed frequently. Main 
changes of the models include: 
1. Using more complex learning models, especially bi-directional deep neural network 
models. 
2. The use of massive pre-training on unlabeled text. 
3. The use of a subword tokenizer. 
4. Using fine-tuning for various downstream NLP tasks. 
In 2018, BERT11 - which stands for bidirectional encoder representations from transformers - 
was proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm 
BERT based on, has become the mainstream model paradigm until today. Besides using a 
transformer as the model backbone, another key of BERT’s success is from pre-training with 
a massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as 
the pre-training objective. It did this by randomly masking some tokens of the input and using 
the masked token id as the prediction objective. This allows the model to utilize both the 
right and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next 
sentence prediction task in pretraining. BERT outputs a contextualized embedding for every 
token in the input. Typically, the embedding of the first token (a special token named [CLS]) is 
used as the embedding for the whole input.
Embeddings & Vector Stores19
September 2024
Figure 7. The BERT architecture
BERT became the base model for multiple embedding models, including Sentence-
BERT,12 SimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large 
language models - never stops. T5 was proposed in 2019 with up to 11B parameters. PaLM 
was proposed in 2022 to push the large language model to a surprising 540B parameters. 
Models like Gemini from Google, GPT models from OpenAI and Llama models from Meta are 
also evolving to newer generations at astonishing speed. Please refer to the whitepaper on 
Foundational models for more information about some common LLMs.
New embedding models based on large language models have been proposed. For example, 
GTR and Sentence-T5 show better performance on retrieval and sentence similarity 
(respectively) than BERT family models.
Another approach to new embeddings models development is generating multi-vector 
embeddings instead of a single vector to enhance the representational power of the models. 
Embedding models in this family include ColBERT15 and XTR.16 
Embeddings & Vector Stores20
September 2024
Figure 8. An illustration of the taxonomy diagram of the embedding models
Although the deep neural network models require a lot more data and compute time to train, 
they have much better performance compared to models using bag-of-words paradigms. 
For example, for the same word the embeddings would be different with different contexts. 
Snippet 4 demonstrates how pre-trained document embedding models from Tensorflow-
hub17 (for example,Sentence t5)A and Vertex AIB can be used for training models with Keras 
and TF datasets. Vertex Generative AI text embeddings can be used with the Vertex AI SDK, 
Langchain, and Google’s BigQuery (Snippet 5) for embedding and advanced workflows.18
A. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models 
and the training datasets and consult the legal team before commercial usage. 
B. Note: not all models on https://tfhub.dev/  can be commercially used. Please check the licenses of the models 
and the training datasets and consult the legal team before commercial usage. 
Embeddings & Vector Stores21
September 2024import vertexai
from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel
# Set the model name. For multilingual: use "text-multilingual-embedding-002"
MODEL_NAME = "text-embedding-004"
# Set the task_type, text and optional title as the model inputs.
# Available task_types are "RETRIEVAL_QUERY", "RETRIEVAL_DOCUMENT", 
# "SEMANTIC_SIMILARITY", # "CLASSIFICATION", and "CLUSTERING"
TASK_TYPE = "RETRIEVAL_DOCUMENT" 
TITLE = "Google"
TEXT = "Embed text."
# Use Vertex LLM text embeddings
embeddings_vx = TextEmbeddingModel.from_pretrained("textembedding-gecko@004")
def LLM_embed(text):
    def embed_text(text):
        text_inp = TextEmbeddingInput(task_type= "CLASSIFICATION" ,   text=text.numpy())
        return np.array(embeddings_vx.get_embeddings([text_inp])[ 0].values)
 output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)
 output.set_shape(( 768,))
 return  output
# Embed strings using vertex LLMs
LLM_embeddings=train_data. map(lambda x,y: (LLM_embed(x), y))
# Embed strings in the tf.dataset using one of the tf hub models
embedding = "https://tfhub.dev/google/sentence-t5/st5-base/1"
hub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)
                          
# Train model 
model = tf.keras.Sequential()
model.add(hub_layer) # omit this layer if using Vertex LLM embeddings
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  metrics=['accuracy'])
history = model.fit(train_data.shuffle(100).batch(8))
Snippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models
Embeddings & Vector Stores22
September 2024SELECT * FROM ML.GENERATE_TEXT_EMBEDDING(
MODEL my_project.my_company.llm_embedding_model,
(
SELECT review as content
FROM bigquery-public-data.imdb.reviews));
Snippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table
Image & multimodal embeddings
Much like text, it’s also possible to create both image and multimodal embeddings. 
Unimodal image embeddings can be derived in many ways: one of which is by training a 
CNN or Vision Transformer model on a large scale image classification task (for example, 
Imagenet), and then using the penultimate layer as the image embedding. This layer has 
learnt some important discriminative feature maps for the training task. It contains a set of 
feature maps that are discriminative for the task at hand and can be extended to other tasks 
as well. 
To obtain multimodal embeddings19 you take the individual unimodal text and image 
embeddings and their semantic relationships learnt via another training process. This 
gives you a fixed size semantic representation in the same latent space. The below snippet 
(Snippet 6) can be used to compute image and multimodal embeddings for images and text 
and be used with a keras model directly (much like the text embedding example).
Embeddings & Vector Stores23
September 2024import base64
import tensorflow as tf
from google.cloud import aiplatform
from google.protobuf import struct_pb2
#fine-tunable layer for image embeddings which can be used for downstream keras model image_
embed=hub.KerasLayer(" https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_
vector/2 ",trainable=False)  
class EmbeddingPredictionClient:
  """Wrapper around Prediction Service Client."""
  def __init__(self, project : str,
    location : str = "us-central1" ,
    api_regional_endpoint: str = "us-central1-aiplatform.googleapis.com" ):
    client_options = { "api_endpoint" : api_regional_endpoint}
    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  
    self.location = location
    self.project = project
  def get_embedding(self, text : str = None, gs_image_path : str = None):
   #load the image from a bucket in google cloud storage
   with tf.io.gfile.GFile(gs_image_path, "rb") as f:
     image_bytes = f.read()
   if not text and not image_bytes:
    raise ValueError( 'At least one of text or image_bytes must be specified.' )
   #Initialize a protobuf data struct with the text and image inputs  
   instance = struct_pb2.Struct()
    if text:
      instance.fields[ 'text'].string_value = text
      if image_bytes:
      encoded_content = base64.b64encode(image_bytes).decode( "utf-8")
      image_struct = instance.fields[ 'image'].struct_value
      image_struct.fields[ 'bytesBase64Encoded' ].string_value = .string_value = encoded_content
     #Make predictions using the multimodal embedding model
     instances = [instance]
     endpoint = (f"projects/{self.project}/locations/{self.location}"
         "/publishers/google/models/multimodalembedding@ 001")
     response = self.client.predict(endpoint=endpoint, instances=instances)
     text_embedding = None
     if text:    
      text_emb_value = response.predictions[ 0]['textEmbedding' ]
      text_embedding = [v for v in text_emb_value]
     image_embedding = None
     if image_bytes:    
      image_emb_value = response.predictions[ 0]['imageEmbedding' ]
      image_embedding = [v for v in image_emb_value]
Continues next page...
 
Embeddings & Vector Stores24
September 2024 return EmbeddingResponse (text_embedding=text_embedding, image_embedding=image_embedding)
#compute multimodal embeddings for text and images
client.get_embedding(text= "sample_test" , gs_image_path= "gs://bucket_name../image_filename.." )
Snippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings
Structured data embeddings
There are two common ways to generate embeddings for structured data, one is more 
general while the other is more tailored for recommendation applications. 
Unlike unstructured data, where a pre-trained embedding model is typically available, we 
have to create the embedding model for the structured data  since it would be specific to 
a particular application.
General structured data
Given a general structured data table, we can create embedding for each row. This can be 
done by the ML models in the dimensionality reduction category, such as the PCA model.
One use case for these embeddings are for anomaly detection. For example, we can create 
embeddings for anomaly detection using large data sets of labeled sensor information 
that identify anomalous occurrences.20 Another case use is to feed these embeddings 
to downstream ML tasks such as classification. Compared to using the original high-
dimensional data, using embeddings to train a supervised model requires less data. This is 
particularly important in cases where training data is not sufficient.
Embeddings & Vector Stores25
September 2024User/item structured data
The input is no longer a general structured data table as above. Instead, the input includes 
the user data, item/product data plus the data describing the interaction between user and 
item/product, such as rating score. 
This category is for recommendation purposes, as it maps two sets of data (user dataset, 
item/product/etc dataset) into the same embedding space. For recommender systems, we 
can create embeddings out of structured data that correlate to different entities such as 
products, articles, etc. Again, we have to create our own embedding model. Sometimes this 
can be combined with unstructured embedding methods when images or text descriptions 
are found.
Graph embeddings
Graph embeddings are another embedding technique that lets you represent not 
only information about a specific object but also its neighbors (namely, their graph 
representation). Take an example of a social network where each person is a node, and the 
connections between people are defined as edges. Using graph embedding you can model 
each node as an embedding, such that the embedding captures not only the semantic 
information about the person itself, but also its relations and associations hence enriching 
the embedding. For example, if two nodes are connected by an edge, the vectors for those 
nodes would be similar. You might then be able to predict who the person is most similar 
to and recommend new connections. Graph embeddings can also be used for a variety of 
tasks, including node classification, graph classification, link prediction, clustering, search, 
recommendation systems, and more. Popular algorithms21,22 for graph embedding include 
DeepWalk, Node2vec, LINE, and GraphSAGE.23 
Embeddings & Vector Stores26
September 2024Training Embeddings
Current embedding models usually use dual encoder (two tower) architecture. For example, 
for the text embedding model used in question-answering, one tower is used to encode 
the queries and the other tower is used to encode the documents. For the image and text 
embedding model, one tower is used to encode the images and the other tower is used 
to encode the text. The model can have various sub architectures, depending on how the 
model components are shared between the two towers. The following figure shows some 
architectures of the dual encoders.24 
Figure 9. Some architectures of dual encoders
The loss used in embedding models training is usually a variation of contrastive loss, which 
takes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training 
with contrastive loss brings positive examples closer and negative examples far apart.
Similar to foundation model training, training of an embedding model from scratch usually 
includes two stages: pretraining (unsupervised learning) and fine tuning (supervised 
learning). Nowadays, the embedding models are usually directly initialized from foundation 
models such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the 
massive knowledge that has been learned from the large-scale pretraining of the foundation 
Embeddings & Vector Stores27
September 2024models. The fine-tuning of the embedding models can have one or more phases. The fine-
tuning datasets can be created in various methods, including human labeling, synthetic 
dataset generation, model distillation, and hard negative mining.
To use embeddings for downstream tasks like classification or named entity recognition, 
extra layers (for example, softmax classification layer) can be added on top of the embedding 
models. The embedding model can either be frozen (especially when the training dataset is 
small), trained from scratch, or fine-tuned together with the downstream tasks. 
Vertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can 
also choose to fine-tune the models directly. See26 for an example of fine tuning the BERT 
model using tensorflow model garden. You can also directly load the embedding models from 
tfhub and fine-tune on top of the model. Snippet 7 shows an example how to build a classifier 
based on tfhub models. 
# Can switch the embedding to different embeddings from different modalities on # 
tfhub. Here we use the BERT model as an example.
tfhub_link = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"
class Classifier(tf.keras.Model):
    def __init__(self, num_classes):
      super(Classifier, self).__init__(name= "prediction" )
        self.encoder = hub.KerasLayer(tfhub_link, trainable=True)
      self.dropout = tf.keras.layers.Dropout( 0.1)
      self.dense = tf.keras.layers.Dense(num_classes)
    def call(self, preprocessed_text):
      encoder_outputs = self.encoder(preprocessed_text)
      pooled_output = encoder_outputs[ "pooled_output" ]
   x = self.dropout(pooled_output)
   x = self.dense(x)
   return x
Snippet 7. Creating a Keras model using trainable tfhub layer
Embeddings & Vector Stores28
September 2024So far you’ve seen the various types of embeddings, techniques and best practices to train 
them for various data modalities, and some of their applications. The next section discusses 
how to persist and search the embeddings that have been created in a fast and scalable way 
for production workloads.
Vector search
Full-text keyword search has been the lynchpin of modern IT systems for years. Full-text 
search engines and databases (relational and non-relational) often rely on explicit keyword 
matching. For example, if you search for ‘cappuccino’ the search engine or database returns 
all documents that mention the exact query in the tags or text description. However, if the 
key word is misspelled or described with a differently worded text, a traditional keyword 
search returns incorrect or no results. There are traditional approaches which are tolerant of 
misspellings and other typographical errors. However, they are still unable to find the results 
having the closest underlying semantic meanings to the query. This is where vector search is 
very powerful: it uses the vector or embedded semantic representation of documents.
Vector search lets you to go beyond searching for exact query literals and allows you to 
search for the meaning across various data modalities. This provides you more nuanced 
results. After you have a function that can compute embeddings of various items,  you 
compute the embedding of the items of interest and store this embedding in a database. 
You then embed the incoming query in the same vector space as the items. Next, you have 
to find the best matches to the query. This process is analogous to finding the most ‘similar’ 
matches across the entire collection of searchable vectors: similarity between vectors can be 
computed using a metric such as euclidean distance, cosine similarity, or dot product.
Embeddings & Vector Stores29
September 2024
Figure 10. Visualization of how different metrics compute vector similarity
Euclidean distance (i.e., L2 distance) is a geometric measure of the distance between two 
points in a vector space. This works well for lower dimensions. Cosine similarity is a measure 
of the angle between two vectors. And inner/dot product, is the projection of one vector 
onto another. They are equivalent when the vector norms are 1. This seems to work better 
for higher dimensional data. Vector databases store and help manage and operationalize the 
complexity of vector search at scale, while also addressing the common database needs.
Important vector search algorithms
The most straightforward way to find the most similar match is to run a traditional linear 
search by comparing the query vector with each document vector and return the one with 
the highest similarity. However, the runtime of this approach scales linearly (O(N)) with the 
amount of documents or items to search. This approach is unacceptably slow for most use 
cases involving several millions of documents or more. Using approximate nearest neighbour 
Embeddings & Vector Stores30
September 2024(ANN) search for that purpose is more practical.  ANN is a technique for finding the closest 
points to a given point in a dataset with a small margin of error - but with a tremendous boost 
in performance. There are many approaches with varying trade-offs across scale, indexing 
time, performance, simplicity and more.27 They use one or more implementations of the 
following techniques: quantization, hashing, clustering and trees, among others. Some of the 
most popular approaches are discussed below.
Locality sensitive hashing & trees
Locality sensitive hashing (LSH) 28 is a technique for finding similar items in a large dataset. 
It does this by creating one or more hash functions that map similar items to the same hash 
bucket with high probability. This means that you can quickly find all of the similar items to 
a given item by only looking at the candidate items in the same hash bucket (or adjacent 
buckets) and do a linear search amongst those candidate pairs. This allows for significantly 
faster lookups within a specific radius. The number of hash functions/tables and buckets 
determine the search recall/speed tradeoff, as well as the false positive / true positive one. 
Having too many hash functions might cause similar items to different buckets, while too few 
might result in too many items falsely being hashed to the same bucket and the number of 
linear searches to increase.
Another intuitive way to think about LSH is grouping residences by their postal code or 
neighborhood name. Then based on where someone chooses to move you look at the 
residences for only that neighborhood and find the closest match.
Embeddings & Vector Stores31
September 2024
Figure 11. Visualization of how LSH uses random hyperplanes to partition the vector space
Tree-based algorithms work similarly. For example, the Kd-tree approach works by creating 
the decision boundaries by computing the median of the values of the first dimension, then 
that of the second dimension and so on. This approach is very much like a decision tree. 
Naturally this can be ineffective if searchable vectors are high dimensional. In that case, the 
Ball-tree algorithm is better suited. It is similar in functionality, except instead of going by 
dimension-wise medians it creates buckets based on the radial distance of the data points 
from the center. Here is an example of the implementation of these three approaches:
Embeddings & Vector Stores32
September 2024from sklearn.neighbors import NearestNeighbors
from vertexai.language_models import TextEmbeddingModel
from lshashing import LSHRandom
import numpy as np
model = TextEmbeddingModel.from_pretrained( "textembedding-gecko@004" )
test_items= [
  "The earth is spherical." ,
  "The earth is a planet." ,
  "I like to eat at a restaurant." ]
query = "the shape of earth"
embedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])
embedded_query = np.array(model.get_embeddings([query])[ 0].values)
#Naive brute force search
n_neighbors= 2
nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm= 'brute').fit(embedded_test_items) 
naive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))
#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise
nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm= 'ball_tree' ).fit(embedded_test_items) 
distances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))
#LSH
lsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)
lsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)
#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours
#ANN retrieved the same ranking of items as brute force in a much scalable manner
Snippet 8. Using scikit-learn29 and lshashing30 for ANN with LSH, KD/Ball-tree and linear search
Hashing and tree-based approaches can also be combined and extended upon to obtain 
the optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and 
ScaNN are good examples.
Embeddings & Vector Stores33
September 2024Hierarchical navigable small worlds 
Figure 12. Diagram showing how HNSW ‘zooms in’ to perform ANN
One of the FAISS (Facebook AI similarity search) implementations leverages the concept 
of hierarchical navigable small world (HNSW) 31 to perform vector similarity search in sub-
linear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a 
hierarchical structure where the graph links are spread across different layers. The top layer 
has the longest links and the bottom layer has the shortest ones. As shown in Figure 9, the 
search starts at the topmost layer where the algorithm greedily traverses the graph to find 
the vertex most semantically similar to the query. Once the local minimum for that layer is 
found, it then switches to the graph for the closest vertex on the layer below. This process 
continues iteratively until the local minimum for the lowest layer is found, with the algorithm 
keeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm 
can be optionally augmented with quantization and vector indexing to boost speed and 
memory efficiency.
Embeddings & Vector Stores34
September 2024import faiss
M=32 #creating high degree graph:higher recall for larger index & searching time
d=768 # dimensions of the vectors/embeddings
index = faiss.IndexHNSWFlat(d, M)
index.add(embedded_test_items) #build the index using the embeddings in Snippet 9
#execute the ANN search
index.search(np.expand_dims(embedded_query, axis= 0), k=2)
Snippet 9. Indexing and executing ANN search with the FAISS library using HNSW
ScaNN
Google developed the scalable approximate nearest neighbor (ScaNN)32,33 approach which is 
used across a lot of its products and services. This includes being externally available to all 
customers of Google Cloud through the Vertex AI Vector Search. Below is how ScaNN uses 
a variety of steps to perform efficient vector search, with each one of them having their own 
subset of parameters. 
The first step is the optional partitioning step during training: it uses one of the multiple 
algorithms available to partition the vector store into logical partitions/clusters where 
the semantically related are grouped together. The partitioning step is optional for small 
datasets. However, for larger datasets with >100k embedding vectors, the partitioning step 
is crucial since by pruning the search space it cuts down the search space by magnitudes 
therefore significantly speeds up the query. The space pruning is configured through the 
number of partitions and the number of partitions to search. A larger number leads to better 
recall but larger partition creation time. A good heuristic is to set the number of partitions to 
be the square root of the number of vectors.
Embeddings & Vector Stores35
September 2024
Figure 13. Search space partitioning & pruning(left) & Approximate scoring (right)
At query time ScaNN uses the user-specified distance measure to select the specified 
number of top partitions (a value specified by the user), and then executes the scoring 
step next. In this step ScaNN compares the query with all the points in the top partitions 
and selects the top K’. This distance computation can be configured as exact distance or 
approximate distance. The approximate distance computation leverages either standard 
product quantization or anisotropic quantization techniques, the latter of which is a specific 
method employed by ScaNN which gives the better speed and accuracy tradeoffs.
Finally, as a last step the user can optionally choose to rescore the user specified top K 
number of results more accurately. This results in an industry leading speed/accuracy 
tradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a 
code example.
Embeddings & Vector Stores36
September 2024
Figure 14. Accuracy/speed tradeoffs for various SOTA ANN search algorithms
import tensorflow as tf
import tensorflow_recommenders as tfrs
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets
embedded_query = tf.constant((LM_embed(query, "RETRIEVAL_QUERY" )))
embedded_docs = [LM_embed(doc, "RETIREVAL_DOCUMENT" ) for doc in searchable_docs]
embedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs). enumerate ().batch(1)
# Build index from tensorflow dataset and execute ANN search based on dot product metric
scann = tfrs.layers.factorized_top_k.ScaNN( 
  distance_measure= 'dot_product' ,
  num_leaves = 4, #increase for higher number of partitions / latency for increased recall
  num_leaves_to_search= 2) # increase for higher recall but increased latency
scann = scann.index_from_dataset(embedded_docs)
scann(embedded_query, k= 2)
Embeddings & Vector Stores37
September 2024Snippet 10. Using Tensorflow Recommenders34 to perform ANN search using the ScaNN algorithm
In this whitepaper we have seen both State-of-the-Art SOTA and traditional ANN search 
algorithms: ScaNN, FAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/
accuracy tradeoffs that they provide. However, to use these algorithms they need to 
be deployed in a scalable, secure and production-ready manner. For that we need 
vector databases.
Vector databases 
Vector embeddings embody semantic meanings of data, while vector search algorithms 
provide a means for efficiently querying them. Historically traditional databases lacked the 
means to combine semantic meaning and efficient querying  in a way that the most relevant 
embeddings can be both stored, queried, and retrieved in a secure, scalable, and flexible 
manner for complex analysis and real-time enterprise grade applications. This is what 
gave rise to vector databases, which are built ground-up to manage these embeddings for 
production scenarios. Due to the recent popularity of Generative AI, an increasing number 
of traditional databases are starting to incorporate supporting vector search functionality 
as well in addition to traditional search (‘hybrid search’) functionalities. Let’s look at the 
workflow for a simple Vector Database, with hybrid search capabilities.

Embeddings & Vector Stores38
September 2024Figure 15. Populating and querying vector databases
Each vector database differs in its implementation, but the general flow is shown in Figure 15:
1. An appropriate trained embedding model is used to embed the relevant data points as 
vectors with fixed dimensions. 
2. The vectors are then augmented with appropriate metadata and complementary 
information (such as tags) and indexed using the specified algorithm for efficient search.
3. An incoming query gets embedded with the same model, and used to query and return  
specific amounts of the most semantically similar items and their associated unembedded 
content/metadata. Some databases might provide caching and pre-filtering (based on 
tags) and post-filtering capabilities (reranking using another more accurate model) to 
further enhance the query speed and performance.
There are quite a few vector databases available today, each tailored to different business 
needs and considerations. A few good examples of commercially managed vector databases 
include Google Cloud’s Vertex Vector Search,35 Google Cloud’s AlloyDB & Cloud SQL 
Postgres ElasticSearch,36 and Pinecone37 to name a few. Vertex AI Vector Search is a vector 
database built by Google that uses the ScaNN algorithm for fast vector search, while still 
maintaining all the security and access guarantees of Google Cloud. AlloyDB & Cloud SQL 
Postgres supports vector search through the OSS pgvector38 extension, which allows for 
SQL queries to combine ANN search with traditional predicates and the usual transactional 
semantics for ANN search index. AlloyDB also has a ScaNN index extension that is a native 
implementation of ScaNN and is pgvector-compatible. Similarly, many of the other traditional 
databases have also started to add plugins to enable vector search. Pinecone and Weaviate 
leverage HNSW for their fast vector search in addition to the ability to filter data using 
Embeddings & Vector Stores39
September 2024traditional search. Amongst their open source peers: Weaviate39 and ChromaDB40 provide a 
full suite of functionality upon deployment and can be tested in memory as well during the 
prototyping phase.
Operational considerations
Vector Databases are critical to managing the majority of technical challenges that arise 
with storing and querying embeddings at scale. Some of these challenges are specific to the 
nature of vector stores, while others overlap with that of traditional databases. These include 
horizontal and vertical scalability, availability, data consistency, real time updates, backups, 
access control, compliance, and much more. However, there are also many more challenges 
and considerations you need to take into account while using embedding and vector stores.
Firstly, embeddings, unlike traditional content, can mutate over time. This means that the 
same text, image, video or other content could and should be embedded using different 
embedding models to optimize for the performance of the downstream applications. This is 
especially true for embeddings of supervised models after the model is retrained to account 
for various drifts or changing objectives. Similarly, the same applies to unsupervised models 
when they are updated to a newer model. However, frequently updating the embeddings 
- especially those trained on large amounts of data - can be prohibitively expensive. 
Consequently, a balance needs to be struck. This necessitates a well-defined automated 
process to store, manage, and possibly purge embeddings from the vector databases taking 
the budget into consideration.
Embeddings & Vector Stores40
September 2024Secondly, while embeddings are great at representing semantic information, sometimes they 
can be suboptimal at representing literal or syntactic information. This is especially true for 
domain-specific words or IDs. These values are potentially missing or underrepresented 
in the data the embeddings models were trained on. For example, if a user enters a query 
that contains the ID of a specific number along with a lot of text, the model might find 
semantically similar neighbors which match the meaning of the text closely, but not the ID, 
which is the most important component in this context. You can overcome this challenge by 
using a combination of full-text search to pre-filter or post-filter the search space before 
passing it onto the semantic search module.
Another important point to consider is that depending on the nature of the workload in which 
the semantic query occurs, it might be worth relying on different vector databases. For 
example, for OLTP workloads that require frequent reads/write operations, an operational 
database like Postgres or CloudSQL is the best choice. For large-scale OLAP analytical 
workloads and batch use cases, using Bigquery’s vector search is preferable.
In conclusion, a variety of factors need to be considered when choosing a vector database. 
These factors include size and type of your dataset (some are good at sparse and others 
dense), business needs, the nature of the workload,  budget, security, privacy guarantees, 
the needs for semantic and syntactic search as well as the database systems that are already 
in use. In this section we have seen the various ANN search approaches as well the need and 
benefits of vector databases. The next section demonstrates an example of using a Vector AI 
Vector Search for semantic search.
Applications
Embeddings models are one of the fundamental machine learning models that power a 
variety of applications. We summarize some popular applications in the following table. 
Embeddings & Vector Stores41
September 2024Task Description
RetrievalGiven a query and a set of objects (for example, documents, images, 
and videos), retrieve the most relevant objects. Based on the definition 
of relevant objects, the subtasks include question answering and 
recommendations.
Semantic text similarityDetermine whether two sentences have the same semantic meaning. 
The subtasks include: paraphrasing, duplicate detection, and bitext 
mining.
ClassificationClassify objects into possible categories. Based on the number of labels, 
the subtasks include binary classification, multi-class classification, and 
multilabel classifications.
Clustering Cluster objects together.
Reranking Rerank a set of objects based on a certain query. 
Embeddings together with vector stores providing ANN can be powerful tools which can be 
used for a variety of applications. These include Retrieval augmented Generation for LLMs, 
Search, Recommendation Systems, Anomaly detection, few shot- classification and much 
more. 
For ranking problems like search and recommendations, embeddings are normally used 
at the first stage of the process. They retrieve the potentially good candidates that are 
semantically similar and consequently improve the relevance of search results. Since the 
amount of information to sort through can be quite large (in some cases even millions or 
billions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. 
Let’s look at an application which combines both LLMs and RAG to help answer questions.
Embeddings & Vector Stores42
September 2024Q & A with sources (retrieval augmented generation)
Retrieval augmented generation (RAG) for Q&A is a technique that combines the best of both 
worlds from retrieval and generation. It first retrieves relevant documents from a knowledge 
base and then uses prompt expansion to generate an answer from those documents. Prompt 
expansion is a technique that when combined with database search can be very powerful. 
With prompt expansion the model retrieves relevant information from the database (mostly 
using a combination of semantic search and business rules), and augments the original 
prompt with it. The model uses this augmented prompt to generate much more interesting, 
factual, and informative content than with retrieval or generation alone.
RAGs can help with a common problem with LLMs: their tendency to ‘hallucinate’ and 
generate factually incorrect but plausible sounding responses. Although RAG can reduce 
hallucinations, it does not completely eliminate them. What can help mitigate this problem 
further is to also return the sources from the retrieval and do a quick coherence check either 
by a human or an LLM. This ensures the LLM response is consistent with the semantically 
relevant sources. Let’s look at an example (Snippet 11) of RAG with sources, which can be 
scalably implemented using Vertex AI LLM text embeddings and Vertex AI Vector Search in 
conjunction with libraries like langchain.41
Embeddings & Vector Stores43
September 2024# Before you start run this command:
# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai
# after running pip install make sure you restart your kernel
# TODO : Set values as per your requirements
# Project and Storage Constants
PROJECT_ID = "<my_project_id>"
REGION = "<my_region>"
BUCKET = "<my_gcs_bucket>"
BUCKET_URI = f"gs://{BUCKET}"
# The number of dimensions for the textembedding-gecko@004 is 768
# If other embedder is used, the dimensions would probably need to change.
DIMENSIONS = 768
# Index Constants
DISPLAY_NAME = "<my_matching_engine_index_id>"
DEPLOYED_INDEX_ID = "yourname01"  # you set this. Start with a letter.
from google.cloud import aiplatform
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_google_vertexai import VertexAI
from langchain_google_vertexai import (
    VectorSearchVectorStore,
    VectorSearchVectorStoreDatastore,
)
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from IPython.display import display, Markdown
aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)
embedding_model = VertexAIEmbeddings(model_name= "textembedding-gecko@003" )
# NOTE : This operation can take upto 30 seconds
my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index (
    display_name=DISPLAY_NAME,
    dimensions=DIMENSIONS,
    approximate_neighbors_count=150,
    distance_measure_type ="DOT_PRODUCT_DISTANCE" ,
    index_update_method= "STREAM_UPDATE" ,  # allowed values BATCH_UPDATE , STREAM_UPDATE
)
Continues next page...
Embeddings & Vector Stores44
September 2024# Create an endpoint
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create (
 display_name=f"{DISPLAY_NAME}-endpoint", public_endpoint_enabled=True
)
# NOTE : This operation can take upto 20 minutes
my_index_endpoint = my_index_endpoint.deploy_index (
 index=my_index, deployed_index_id=DEPLOYED_INDEX_ID
)
# retrieve the id of the most recently deployed index or manually look up the index 
deployed above
index_id=my_index_endpoint.deployed_indexes [-1].index.split ("/")[-1]
endpoint_id= my_index_endpoint.name
# TODO : replace 1234567890123456789 with your acutial index ID
my_index = aiplatform.MatchingEngineIndex(index_id)
# TODO : replace 1234567890123456789 with your acutial endpoint ID
# Be aware that the Index ID differs from the endpoint ID
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)
# Input texts
texts= [
   "The earth is spherical." ,
   "The earth is a planet." ,
   "I like to eat at a restaurant." ,
]
# Create a Vector Store
vector_store = VectorSearchVectorStore.from_components (
    project_id=PROJECT_ID,
    region=REGION,
    gcs_bucket_name=BUCKET,
    index_id=my_index.name,
    endpoint_id=my_index_endpoint.name,
    embedding=embedding_model,
    stream_update= True,
)
# Add vectors and mapped text chunks to your vectore store
vector_store.add_texts(texts=texts)
# Initialize the vectore_store as retriever
retriever = vector_store.as_retriever ()
Continues next page...
Embeddings & Vector Stores45
September 2024Snippet 11. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate 
grounded results/sources. retriever=vector_store.as_retriever (search_kwargs= {'k':1 })
#create custom prompt for your use case
prompt_template= """You are David, an AI knowledge bot.
Answer the questions using the facts provided. Use the provided pieces of context to answer
the users question.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
{summaries}"""
messages = [
    SystemMessagePromptTemplate.from_template (prompt_template ),
    HumanMessagePromptTemplate.from_template ("{question}" )
]
prompt = ChatPromptTemplate.from_messages(messages)
chain_type_kwargs = {"question" : prompt }
#initialize your llm model
llm = VertexAI (model_name= "gemini-pro" )
#build your chain for RAG+C
chain= RetrievalQA.from_chain_type(llm=llm, chain_type= "stuff", 
retriever=retriever, return_source_documents= True)
#print your results with Markup language
def print_result (result):
  output_text = f """### Question:
  {query}
  ### Answer:
  {result['result' ]}
  ### Source:
  {' '.join(list(set([doc.page_content for doc in result['source_documents' ]])))}
  """
  return(output_text)
chain= "What shape is the planet where humans live?"
result = chain (query)
display(Markdown (print_result (result)))
Embeddings & Vector Stores46
September 2024
Figure 16. Model responses along with sources demonstrating the LLM being grounded in the database
As we can infer from Figure 16, the output not only grounds LLM in the semantically similar 
results retrieved from the database (hence refusing to answer when context cannot be found 
in the database). This not only significantly reduces hallucination, but also provides sources 
for verification, either human or using another LLM.
Summary
In this whitepaper we have discussed various methods to create, manage, store, and retrieve 
embeddings of various data modalities effectively in the context of production-grade 
applications. Creating, maintaining and using embeddings for downstream applications can 
be a complex task that involves several roles in the organization. However, by thoroughly 
operationalizing and automating its usage, you can safely leverage the incredible benefits 
they offer across some of the most important applications. Some key takeaways from this 
whitepaper include:
1. Choose your embedding model wisely for your data and use case. Ensure the data used in 
inference is consistent with the data used in training. The distribution shift from training to 
inference can come from various areas, including domain distribution shift or downstream 
task distribution shift. If no existing embedding models fit the current inference data 
distribution, fine-tuning the existing model can significantly help on the performance. 
Embeddings & Vector Stores47
September 2024Another tradeoff comes from the model size. The large deep neural network (large 
multimodal models) based models usually have better performance but can come with a 
cost of longer serving latency. Using Cloud-based embedding services can conquer the 
above issue by providing both high-quality and low-latency embedding service. For most 
business applications using a pre-trained embedding model provides a good baseline, 
which can be further fine-tuned or integrated in downstream models. In case the data has 
an inherent graph structure, graph embeddings can provide superior performance.
2. Once your embedding strategy is defined, it’s important to make the choice of the 
appropriate vector database that suits your budget and business needs. It might seem 
quicker to prototype with available open source alternatives, but opting for a more secure, 
scalable, and battle-tested managed vector database is certain to be better off in the long 
term. There are various open source alternatives using one of the many powerful ANN 
vector search algorithms, but ScaNN and HNSW have proven to provide some of the best 
accuracy and performance trade offs in that order.
3. Embeddings combined with an appropriate ANN powered vector database is an 
incredibly powerful tool and can be leveraged for various applications, including 
Search, Recommendation systems, and Retrieval augment generation for LLMs. This 
approach can mitigate the hallucination problem and bolster verifiability and trust of 
LLM-based systems.
Embeddings & Vector Stores48
September 2024Endnotes
1. Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. 
Available at: https://doi.org/10.1007/978-981-15-6198-6_18
2. Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] 
Available at: https://nlp.stanford.edu/pubs/glove.pdf .
3. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings 
by noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215 .
4. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations 
in vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf .
5. Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available 
at: https://radimrehurek.com/gensim/intro.html .
6. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. 
ArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606 .
7. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent 
semantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.
8. Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. 
Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available 
at: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html .
9. Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, 
abs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .
10. Le, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. 
Available at: https://arxiv.org/abs/1405.4053 .
11. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers 
for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the 
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 
pp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/ .
12. Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge 
distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing 
(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/ .
Embeddings & Vector Stores49
September 202413. Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, 
abs/2104.08821. Available at: https://arxiv.org/abs/2104.08821 .
14. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by 
weakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279 .
15. Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late 
interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and 
Development in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025 .
16. Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token 
retrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .
17. TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available 
at: https://tfhub.dev/ .
18. Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  
Google Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing  
-bigquery-text-embeddings .
19. Google Cloud, 2024, Get multimodal embeddings. Available at:  
https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings .
20. Pinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection .
21. Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph 
embedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge 
Management. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568 .
22. Cai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, 
techniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf .
23. Hamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  
In Advances in Neural Information Processing Systems 30. Available at:  
https://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf .
24. Dong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder 
architectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120 .
25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  
https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings .
Embeddings & Vector Stores50
September 202426. TensorFlow, 2021, TensorFlow Models: NLP, Fine-tune BERT. Available at:  
https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert .
27. Matsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), 
Article 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf .
28. Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic 
expected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at:  
https://dl.acm.org/doi/pdf/10.1145/355744.355745 .
29. Scikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. 
Available at: https://scikit-learn.org/ .
30. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at:  
https://pypi.org/project/lshashing/ .
31. Malkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using 
hierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  
https://arxiv.org/pdf/1603.09320.pdf .
32. Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  
https://github.com/google-research/google-research/tree/master/scann .
33. Guo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector 
quantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf .
34. TensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender 
system models. Available at: https://www.tensorflow.org/recommenders .
35. Google Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. 
Available at: https://cloud.google.com/vertex-ai/docs/vector-search/overview .
36. Elasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  
https://www.elastic.co/elasticsearch/ .
37. Pinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  
https://www.pinecone.io .
38. pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  
https://github.com/pgvector/pgvector .
39. Weaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/ .
Embeddings & Vector Stores51
September 202440. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/ .
41. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language 
model. Available at: https://langchain.com .
Foundational 
Large Language 
Models & 
Text Generation
Authors: Mohammadamin Barektain,  
Anant Nawalgaria, Daniel J. Mankowitz,  
Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  
Matan Kalman, Elena Buchatskaya,                                     
Aliaksei Severyn, and Antonio Gulli
Foundational Large Language Models & Text Generation2
September 2024Acknowledgements
Reviewers and Contributors
Adam Sadvovsky
Yonghui Wu
Andrew Dai
Efi Kokiopolou
Chuck Sugnet
Aleksey Vlasenko
Erwin Huizenga
Curators and Editors
Antonio Gulli
Anant Nawalgaria
Grace Mollison 
Technical Writer
Mark Iverson
Designer
Michael Lanning 

Introduction  6
Why language models are important  7
Large language models  8
 Transformer  9
  Input preparation and embedding  11
  Multi-head attention  12
   Understanding self-attention  12
   Multi-head attention: power in diversity  14
  Layer normalization and residual connections  15
  Feedforward layer  15
  Encoder and decoder  16
  Training the transformer  17
   Data preparation  17
   Training and loss function  18
The evolution of transformers  19
 GPT-1  19
 BERT  21
 GPT-2  22Table of contents

 GPT-3/3.5/4  23
 LaMDA  24
 Gopher  25
 GLaM  26
 Chinchilla  27
 PaLM  28
  PaLM 2  29
 Gemini  29
 Other open models  32
 Comparison  34
Fine-tuning large language models  37
 Supervised fine-tuning  38
 Reinforcement learning from human feedback  39
 Parameter Efficient Fine-Tuning  41
Using large language models  44
 Prompt engineering  44
 Sampling Techniques and Parameters  45
Accelerating inference  46
 Trade offs  47
  The Quality vs Latency/Cost Tradeoff  48
  The Latency vs Cost Tradeoff  48
 Output-approximating methods  49
  Quantization  49
  Distillation  50
 Output-preserving methods  52
  Flash Attention  52
  Prefix Caching  53
  Speculative Decoding  55
 Batching and Parallelization  57
Applications  58
 Code and mathematics  61
 Machine translation  62
 Text summarization  63
 Question-answering  63
 Chatbots  64
 Content generation  65
 Natural language inference  65
 Text classification  66
 Text analysis  67
 Multimodal applications  68
Summary  69
Endnotes  71
Foundational Large Language Models & Text Generation6
September 2024Introduction
The advent of Large Language Models (LLMs) represents a seismic shift in the world of 
artificial intelligence. Their ability to process, generate, and understand user intent is 
fundamentally changing the way we interact with information and technology. 
An LLM is an advanced artificial intelligence system that specializes in processing, 
understanding, and generating human-like text. These systems are typically implemented as 
a deep neural network and are trained on massive amounts of text data. This allows them to 
learn the intricate patterns of language, giving them the ability to perform a variety of tasks, 
like machine translation, creative text generation, question answering, text summarization, 
and many more reasoning and language oriented tasks. This whitepaper dives into the 
timeline of the various architectures and approaches building up to the large language 
models and the architectures being used at the time of publication. It also discusses fine-We believe that this new crop of 
technologies has the potential to 
assist, complement, empower, 
and inspire people at any time 
across almost any field.
Foundational Large Language Models & Text Generation7
September 2024tuning techniques to customize an LLM to a certain domain or task, methods to make the 
training more efficient, as well as methods to accelerate inference. These are then followed 
by various applications and code examples. 
Why language models are important
LLMs achieve an impressive performance boost from the previous state of the art across 
a variety of different and complex tasks which require answering questions or complex 
reasoning, making feasible many new applications. These include language translation, code 
generation and completion, text generation, text classification, and question-answering, 
to name a few. Although foundational LLMs trained in a variety of tasks on large amounts 
of data perform very well out of the box and display emergent behaviors (e.g. the ability to 
perform tasks they have not been directly trained for) they can also be adapted to solve 
specific tasks where performance out of the box is not at the level desired through a process 
known as fine-tuning. This requires significantly less data and computational resources than 
training an LLM from scratch. LLMs can be further nudged and guided towards the desired 
behavior by the discipline of prompt engineering:  the art and science of composing the 
prompt and the parameters of an LLM to get the desired response.
The big question is: how do these large language models work? The next section explores the 
core building blocks of LLMs, focusing on transformer architectures and their evolution from 
the original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s 
most capable LLM. We also cover training and fine-tuning techniques, as well as methods to 
improve the speed of response generation. The whitepaper concludes with a few examples 
of how language models are used in practice.
Foundational Large Language Models & Text Generation8
September 2024Large language models
A language model  predicts the probability of a sequence of words. Commonly, when given 
a prefix of text, a language model assigns probabilities to subsequent words. For example, 
given the prefix “The most famous city in the US is…”, a language model might predict high 
probabilities to the words “New York” and “Los Angeles” and low probabilities to the words 
“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while 
modern language models are often based on neural models, such as transformers.
Before the invention of transformers1, recurrent neural networks (RNNSs) were the popular 
approach for modeling sequences. In particular, “long short-term memory” (LSTM) and 
“gated recurrent unit” (GRU) were common architectures.3 This area includes language 
problems such as machine translation, text classification, text summarization, and question-
answering, among others. RNNs process input and output sequences sequentially. They 
generate a sequence of hidden states based on the previous hidden state and the current 
input. The sequential nature of RNNs makes them compute-intensive and hard to parallelize 
during training (though recent work in state space modeling is attempting to overcome 
these challenges).
Transformers, on the other hand, are a type of neural network that can process sequences 
of tokens in parallel thanks to the self-attention mechanism.1 This means that transformers 
can better model long-term contexts and are easier to parallelize than RNNs. This makes 
them significantly faster to train, and more powerful compared to RNNs for handling long-
term dependencies in long sequence tasks. However, the cost of self-attention in the original 
transformers is quadratic in the context length which limits the size of the context, while 
RNNs have a theoretically infinite context length. Transformers have become the most 
popular approach for sequence modeling and transduction problems in recent years.
Herein, we discuss the first version of the transformer model and then move on to the more 
recent advanced models and algorithms.
Foundational Large Language Models & Text Generation9
September 2024Transformer
The transformer architecture  was developed at Google in 2017 for use in a translation model.1 
It’s a sequence-to-sequence model capable of converting sequences from one domain 
into sequences in another domain. For example, translating French sentences to English 
sentences. The original transformer architecture consists of two parts: an encoder and a 
decoder. The encoder converts the input text (e.g., a French sentence) into a representation, 
which is then passed to the decoder. The decoder uses this representation to generate the 
output text (e.g., an English translation) autoregressively.1 Notably, the size of the output of 
the transformer encoder is linear in the size of its input. Figure 1 shows the design of the 
original transformer architecture.
The transformer consists of multiple layers. A layer in a neural network comprises a set of 
parameters that perform a specific transformation on the data. In the diagram you can see 
an example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, 
Linear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. 
The input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the 
network. Input embeddings  are used to represent the input tokens to the model. Output 
embeddings  are used to represent the output tokens that the model predicts. For example, in 
a machine translation model, the input embeddings would represent the words in the source 
language, while the output embeddings would represent the words in the target language. 
The output layer (e.g., Softmax) is the final layer that produces the output of the network. The 
hidden layers (e.g., Multi-Head Attention) are between the input and output layers and are 
where the magic happens!
Foundational Large Language Models & Text Generation10
September 2024
Figure 1. Original Transformer1 (P.C:5)
Foundational Large Language Models & Text Generation11
September 2024To better understand the different layers in the transformer, let’s use a French-to-English 
translation task as an example. Here, we explain how a French sentence is input into the 
transformer and a corresponding English translation is output. We will also describe each of 
the components inside the transformer from Figure 1.
Input preparation and embedding
To prepare language inputs for transformers, we convert an input sequence into tokens and 
then into input embeddings. At a high level, an input embedding is a high-dimensional vector 
that represents the meaning of each token in the sentence. This embedding is then fed into 
the transformer for processing. Generating an input embedding involves the following steps:
1. Normalization  (Optional): Standardizes text by removing redundant whitespace, 
accents, etc.
2. Tokenization : Breaks the sentence into words or subwords and maps them to integer 
token IDs from a vocabulary.
3. Embedding : Converts each token ID to its corresponding high-dimensional vector, 
typically using a lookup table. These can be learned during the training process.
4. Positional  Encoding : Adds information about the position of each token in the sequence 
to help the transformer understand word order.
These steps help to prepare the input for the transformers so that they can better 
understand the meaning of the text.
Foundational Large Language Models & Text Generation12
September 2024Multi-head attention
After converting input tokens into embedding vectors, you feed these embeddings into 
the multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in 
transformers; it enables them to focus on specific parts of the input sequence relevant to 
the task at hand and to capture long-range dependencies within sequences more effectively 
than traditional RNNs. 
Understanding self-attention
Consider the following sentence: “The tiger jumped out of a tree to get a drink because it 
was thirsty.” Self-attention helps to determine relationships between different words and 
phrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, 
so we would expect these two words to be strongly connected. Self-attention achieves this 
through the following steps (Figure 2):
1. Creating queries, keys, and values:  Each input embedding is multiplied by three learned 
weight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These 
are like specialized representations of each word.
• Query: The query vector helps the model ask, “Which other words in the sequence are 
relevant to me?”
• Key: The key vector is like a label that helps the model identify how a word might be 
relevant to other words in the sequence.
• Value: The value vector holds the actual word content information.
2. Calculating scores:  Scores are calculated to determine how much each word should 
‘attend’ to other words. This is done by taking the dot product of the query vector of one 
word with the key vectors of all the words in the sequence.
Foundational Large Language Models & Text Generation13
September 20243. Normalization:  The scores are divided by the square root of the key vector dimension (dk) 
for stability, then passed through a softmax function to obtain attention weights. These 
weights indicate how strongly each word is connected to the others.
4. Weighted values:  Each value vector is multiplied by its corresponding attention weight. 
The results are summed up, producing a context-aware representation for each word.
Figure 2. The process of computing self-attention in the multi-head attention module1 (P.C:5)  
Foundational Large Language Models & Text Generation14
September 2024In practice, these computations are performed at the same time, by stacking the query, key 
and value vectors for all the tokens into Q, K and V matrices and multiplying them together as 
shown in Figure 3.
Figure 3. The basic operation of attention,1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension 
of queries and keys (P.C:5)
Multi-head attention: power in diversity
Multi-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, 
each ‘head’ potentially focusing on different aspects of the input relationships. The outputs 
from each head are concatenated and linearly transformed, giving the model a richer 
representation of the input sequence.
The use of multi-head attention improves the model’s ability to handle complex language 
patterns and long-range dependencies. This is crucial for tasks that require a nuanced 
understanding of language structure and content, such as machine translation, text 
summarization, and question-answering. The mechanism enables the transformer to consider 
multiple interpretations and representations of the input, which enhances its performance on 
these tasks. 
Foundational Large Language Models & Text Generation15
September 2024Layer normalization and residual connections
Each layer in a transformer, consisting of a multi-head attention module and a feed-forward 
layer, employs layer normalization and residual connections. This corresponds to the Add 
and Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ 
corresponds to layer normalization. Layer normalization computes the mean and variance 
of the activations to normalize the activations in a given layer. This is typically performed to 
reduce covariate shift as well as improve gradient flow to yield faster convergence during 
training as well as improved overall performance. 
Residual connections propagate the inputs to the output of one or more layers. This has the 
effect of making the optimization procedure easier to learn and also helps deal with vanishing 
and exploding gradients. 
The Add and Norm  layer is applied to both the multi-head attention module and the feed-
forward layer described in the following section.
Feedforward layer 
The output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is 
fed into the feedforward layer of each transformer block. This layer applies a position-wise 
transformation to the data, independently for each position in the sequence, which allows the 
incorporation of additional non-linearity and complexity into the model’s representations. The 
feedforward layer typically consists of two linear transformations with a non-linear activation 
function, such as ReLU or GELU, in between. This structure adds further representational 
power to the model. After processing by the feedforward layer, the data undergoes 
another ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep 
transformer models.
Foundational Large Language Models & Text Generation16
September 2024Encoder and decoder
The original transformer architecture relies on a combination of encoder and decoder 
modules. Each encoder and decoder consists of a series of layers, with each layer 
comprising key components: a multi-head self-attention mechanism, a position-wise feed-
forward network, normalization layers, and residual connections. 
The encoder’s primary function is to process the input sequence into a continuous 
representation that holds contextual information for each token. The input sequence is first 
normalized, tokenized, and converted into embeddings. Positional encodings are added to 
these embeddings to retain sequence order information. Through self-attention mechanisms, 
each token in the sequence can dynamically attend to any other token, thus understanding 
the contextual relationships within the sequence. The output from the encoder is a series of 
embedding vectors Z representing the entire input sequence. 
The decoder is tasked with generating an output sequence based on the context provided 
by the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-
of-sequence token. The decoder layers employ two types of attention mechanisms: masked 
self-attention  and encoder-decoder cross-attention. Masked self-attention ensures that 
each position can only attend to earlier positions in the output sequence, preserving the 
auto-regressive property. This is crucial for preventing the decoder from having access to 
future tokens in the output sequence. The encoder-decoder cross-attention mechanism 
allows the decoder to focus on relevant parts of the input sequence, utilizing the contextual 
embeddings generated by the encoder. This iterative process continues until the decoder 
predicts an end-of-sequence token, thereby completing the output sequence generation.
Majority of recent LLMs adopted a decoder-only  variant of transformer architecture. This 
approach forgoes the traditional encoder-decoder separation, focusing instead on directly 
generating the output sequence from the input. The input sequence undergoes a similar 
Foundational Large Language Models & Text Generation17
September 2024process of embedding and positional encoding before being fed into the decoder. The 
decoder then uses masked self-attention to generate predictions for each subsequent 
token based on the previously generated tokens. This streamlined approach simplifies the 
architecture for specific tasks where encoding and decoding can be effectively merged.
Training the transformer
When talking about machine learning models, it’s important to differentiate between 
training and inference. Training typically refers to modifying the parameters of the model, 
and involves loss functions and backpropagation. Inference is when model is used only 
for the predicted output, without updating the model weights. The model parameters are 
fixed during inference. Up until now we learned how transformers generate outputs during 
inference. Next, we focus on how to train transformers to perform one or more given tasks.
Data preparation
The first step is data preparation, which involves a few important steps itself. First, clean the 
data by applying techniques such as filtering, deduplication, and normalization. The next 
step is tokenization where the dataset is converted into tokens using techniques such as 
Byte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, 
which is a set of unique tokens used by the LLM. This vocabulary serves as the model’s 
’language’ for processing and understanding text. Finally, the data is typically split into a 
training dataset for training the model as well as a test dataset which is used to evaluate the 
models performance.
Foundational Large Language Models & Text Generation18
September 2024Training and loss function
A typical transformer training loop consists of several parts: First, batches of input 
sequences are sampled from a training dataset. For each input sequence, there is a 
corresponding target sequence. In unsupervised pre-training, the target sequence is 
derived from the input sequence itself. The batch of input sequences is then fed into the 
transformer. The transformer generates predicted output sequences. The difference 
between the predicted and target sequences is measured using a loss function (often cross-
entropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update 
the transformer’s parameters. This process is repeated until the transformer converges to a 
certain level of performance or until it has been trained on a pre-specified number of tokens. 
There are different approaches to formulating the training task for transformers depending 
on the architecture used:
• Decoder-only  models are typically pre-trained on the language modeling task (e.g., see 
endnote12, 13). The target sequence for the decoder is simply a shifted version of the input 
sequence. Given a training sequence like ‘the cat sat on the mat’ various input/target 
pairs can be generated for the model. For example the input “the cat sat on” should 
predict “the” and subsequently the input “the cat sat on the” should predict target 
sequence “mat”.
• Encoder-only  models (like BERT)14 are often pre-trained by corrupting the input sequence 
in some way and having the model try to reconstruct it. One such approach is masked 
language modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat 
on the mat” and the sequence target would be the original sentence.
• Encoder-decoder  models (like the original transformer) are trained on sequence-to-
sequence supervised tasks such as translation (input sequence “Le chat est assis sur 
le tapis” and target “The cat sat on the mat”), question-answering (where the input 
sequence is a question and the target sequence is the corresponding answer), and 
Foundational Large Language Models & Text Generation19
September 2024summarization (where the input sequence is a full article and the target sequence is its 
corresponding summary). These models could also be trained in an unsupervised way by 
converting other tasks into sequence-to-sequence format. For example, when training 
on Wikipedia data, the input sequence might be the first part of an article, and the target 
sequence comprises the remainder of the article.
An additional factor to consider during training is the ‘context length’. This refers to the 
number of previous tokens the model can ‘remember’ and use to predict the next token in 
the sequence. Longer context lengths allow the model to capture more complex relationships 
and dependencies within the text, potentially leading to better performance. However, longer 
contexts also require more computational resources and memory, which can slow down 
training and inference. Choosing an appropriate context length involves balancing these 
trade-offs based on the specific task and available resources.
The evolution of transformers
The next sections provide an overview of the various transformer architectures. These 
include encoder-only, encoder-decoder, as well as decoder-only transformers. We start with 
GPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.
GPT-1
GPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only  model developed 
by OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately 
several billion words) and is able to generate text, translate languages, write different kinds 
of creative content, and answer questions in an informative way. The main innovations in 
GPT-1 were:
Foundational Large Language Models & Text Generation20
September 2024• Combining transformers and unsupervised pre-training:  Unsupervised pre-training 
is a process of training a language model on a large corpus of unlabeled data. Then, 
supervised data is used to fine-tune the model for a specific task, such as translation 
or sentiment classification. In prior works, most language models were trained using a 
supervised learning objective. This means that the model was trained on a dataset of 
labeled data, where each example had a corresponding label. This approach has two main 
limitations. First, it requires a large amount of labeled data, which can be expensive and 
time-consuming to collect. Second, the model can only generalize to tasks that are similar 
to the tasks that it was trained on. Semi-supervised sequence learning was one of the first 
works that showed that unsupervised pre-training followed by supervised training was 
superior than supervised training alone.
Unsupervised pre-training addresses these limitations by training the model on a large 
corpus of unlabeled data. This data can be collected more easily and cheaply than labeled 
data. Additionally, the model can generalize to tasks that are different from the tasks that 
it was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that 
was used to train the GPT-1 language model. The dataset contains over 7,000 unpublished 
books, which provides the model with a large amount of data to learn from. Additionally, 
the corpus contains long stretches of contiguous text, which helps the model learn long-
range dependencies. Overall, unsupervised pre-training is a powerful technique that can 
be used to train language models that are more accurate and generalizable than models 
that are trained using supervised learning alone. 
• Task-aware input transformations:  There are different kinds of tasks such as textual 
entailment and question-answering that require a specific structure. For example, 
textual entailment requires a premise and a hypothesis; question-answering requires a 
context document; a question  and possible answers. One of the contributions of GPT-1 
is converting these types of tasks which require structured inputs into an input that the 
language model can parse, without requiring task-specific architectures on top of the 
pre-trained architecture. For textual entailment, the premise p and the hypothesis h are 
Foundational Large Language Models & Text Generation21
September 2024concatenated with a delimiter token ($) in between - [ p, $, h]. For question answering, the 
context document c is concatenated with the question q and a possible answer a with a 
delimiter token in between the question and answer - [ c,q,$,a].
GPT-1 surpassed previous models on several benchmarks, achieving excellent results. While 
GPT-1 was a significant breakthrough in natural language processing (NLP), it had some 
limitations. For example, the model was prone to generating repetitive text, especially when 
given prompts outside the scope of its training data. It also failed to reason over multiple 
turns of dialogue and could not track long-term dependencies in text. Additionally, its 
cohesion and fluency were limited to shorter text sequences, and longer passages would 
lack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised 
pre-training, which laid the foundation for larger and more powerful models based on the 
transformer architecture.
BERT
BERT14 which stands for Bidirectional Encoder Representations from Transformers, 
distinguishes itself from traditional encoder-decoder transformer models by being an 
encoder-only architecture. Instead of translating or producing sequences, BERT focuses 
on understanding context deeply by training on a masked language model objective. In 
this setup, random words in a sentence are replaced with a [MASK] token, and BERT tries 
to predict the original word based on the surrounding context. Another innovative aspect 
of BERT’s training regime is the next sentence prediction loss, where it learns to determine 
whether a given sentence logically follows a preceding one. By training on these objectives, 
BERT captures intricate context dependencies from both the left and right of a word, and 
it can discern the relationship between pairs of sentences. Such capabilities make BERT 
especially good at tasks that require natural language understanding, such as question-
answering, sentiment analysis, and natural language inference, among others. Since this is an 
encoder-only model, BERT cannot generate text.
Foundational Large Language Models & Text Generation22
September 2024GPT-2
GPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of 
GPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size 
of its training dataset:
• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which 
consists of 45 million webpages from Reddit with a Karma rating of at least three. Karma 
is a rating metric used on Reddit and a value of three means that all the posts were of a 
reasonable level of quality.
• Parameters:  GPT-2 had 1.5 billion parameters, which was an order of magnitude larger 
than the previous model. More parameters increase the model’s learning capacity. The 
authors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B 
(GPT-2) parameters, and found that the model with the most parameters performed better 
on every subsequent task.
This scaling up resulted in a model that was able to generate more coherent and realistic text 
than GPT-1. Its ability to generate human-like responses made it a valuable tool for various 
natural language processing tasks, such as content creation and translation. Specifically, 
GPT-2 demonstrated significant improvement in capturing long-range dependencies and 
common sense reasoning. While it performed well in some tasks, it did not outperform state-
of-the-art reading comprehension, summarization, and translation. GPT-2’s most significant 
achievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task 
transfer is the ability of a model to generalize to a new task without being trained on it, which 
requires the model to understand the task based on the given instruction. For example, for 
an English to German translation task, the model might be given an English sentence followed 
by the word “German” and a prompt (“:”). The model would then be expected to understand 
that this is a translation task and generate the German translation of the English sentence. 
GPT-2 was able to perform tasks such as machine translation, text summarization, and 
reading comprehension without any explicit supervision.
Foundational Large Language Models & Text Generation23
September 2024The study discovered that performance on zero-shot tasks increased in a log-linear manner 
as the model’s capacity increased. GPT-2 showed that training on a larger dataset and having 
more parameters improved the model’s ability to understand tasks and surpass the state-of-
the-art on many tasks in zero-shot settings.
GPT-3/3.5/4
GPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a 
significant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and 
flexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping 
175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. 
This increase in model size allowed GPT-3 to store and recall an even more vast amount of 
information, understand nuanced instructions, and generate more coherent and contextually 
relevant text over longer passages.
While GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can 
understand and execute tasks with just a few examples, or sometimes even without any 
explicit examples—simply based on the instruction provided. This highlights GPT-3’s more 
dynamic understanding and adaptation abilities, reducing the need for task-specific fine-
tuning which was more prevalent in GPT-2.
Finally, GPT-3’s large model scale and diverse training corpus have led to better 
generalization across a broader range of tasks. This means that out-of-the-box, without 
any further training, GPT-3 exhibits improved performance on diverse NLP challenges, from 
translation to question-answering, compared to GPT-2. It’s also worth noting that the release 
approach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, 
they chose to make GPT-3 available as a commercial API, reflecting both its utility and the 
organization’s evolving stance on deployment.
Foundational Large Language Models & Text Generation24
September 2024Instruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-
tuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired 
model behaviors. Outputs from this model were then ranked and it was then further fine-
tuned using Reinforcement Learning from Human Feedback. This led to improved instruction 
following in the model. A 1.3B parameter InstructGPT model had better human evaluations 
than the 175B parameter GPT-3 model. It also showed improvements in truthfulness and 
reductions in toxicity.
GPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of 
understanding and generating code. It’s been optimized for dialogue. And it’s capable of 
receiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 
tokens. 
GPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and 
text inputs and producing text outputs.19 Specifically, accepting text or images as input 
and outputting text. This model has broader general knowledge and advanced reasoning 
capabilities. It can receive context windows of up to 128,000 tokens and has a maximum 
output of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks 
across diverse fields like mathematics, coding, vision, medicine, law, and psychology – all 
without specialized instructions. Its performance often matches or even exceeds human 
capabilities and significantly outperforms earlier models like GPT-3.5.
LaMDA
Google’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another 
contribution to the arena of large-scale language models, designed primarily to engage in 
open-ended conversations. Unlike traditional chatbots which operate in more constrained 
and predefined domains, LaMDA is engineered to handle a wide array of topics, delivering 
Foundational Large Language Models & Text Generation25
September 2024more natural and flowing conversations. LaMDA was trained on dialogue-focused data to 
encourage ongoing conversational flow, not just isolated responses, ensuring users can have 
more extensive and explorative dialogues.
While GPT models, especially the later iterations like GPT-3, have strived to address a 
multitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary 
focus is on maintaining and enhancing conversational depth and breadth. GPT models 
shine on their ability to produce coherent long-form content and perform various tasks 
with minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, 
striving to mimic the unpredictability and richness of human conversations. 
Gopher
Gopher22 is a 280 billion parameter language model based on the decoder-only transformer 
architecture, developed by DeepMind in 2021.22 It can generate text, translate languages, 
write different kinds of creative content, and answer your questions in an informative way. 
Similar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:
• Dataset:  The researchers curated a high-quality text dataset called MassiveText, which 
contains over 10 terabytes of data and 2.45B documents from web pages, books, news 
articles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. 
Importantly, they improved the quality of the data by filtering it, such as by removing 
duplicate text and deduplicating similar documents. This significantly improved the 
model’s performance on downstream tasks.
• Optimization:  The researchers used a warmup learning rate for 1,500 steps and then 
decayed it using a cosine schedule. They also had an interesting rule that as they 
increased the model size, they decreased the learning rate and increased the number of 
tokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 
based on the global gradient norm helped stabilize the training.
Foundational Large Language Models & Text Generation26
September 2024Gopher was evaluated on a variety of tasks, including mathematics, common sense, logical 
reasoning, general knowledge, scientific understanding, ethics, and reading comprehension. 
Gopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, 
Gopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy 
tasks such as abstract algebra.
The authors also conducted a study on the effect of model size on different types of 
tasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that 
increasing the number of parameters had a significant impact on logical reasoning and 
reading comprehension, but it did not improve performance as much on tasks such as 
general knowledge, where performance eventually almost plateaued.
Figure 4. Ablation study22 on the effect of model size on the performance of Gopher on different types 
of tasks
GLaM
GLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts 
language model. Mixture-of-experts based models are much more computationally efficient 
given their parameter count. This is achieved by only activating a subset of their parameters 
Foundational Large Language Models & Text Generation27
September 2024(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ 
of the energy used to train GPT-3 and half of the FLOPs for inference while achieving better 
overall performance compared to GPT-3.
Chinchilla
Until 2022, LLMs were primarily scaled by increasing the model size and using datasets that 
are relatively small by current standards (up to 300 billion tokens for the largest models). 
This approach was informed by the Kaplan et al.24 study, which examined how performance 
of a language model, measured by cross-entropy loss, varies with changes in computational 
budget, model size, and dataset size. Specifically, given a 100-fold increase in computational 
resources ( C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times 
(Nopt∝ C0.73), while increasing dataset size by only 3.5 times ( Dopt∝ C0.27). 
The Chinchilla paper,25 revisited the compute optimal scaling laws and used three different 
approaches to find that near equal scaling in parameters and data is optimal with increasing 
compute. Thus, a 100-fold increase in compute should translate into a tenfold increase in 
both data size and model size. 
Figure 5. Overlaid predictions from three different approaches from Chinchilla paper,25 along with 
projections from Kaplan et al24 
Foundational Large Language Models & Text Generation28
September 2024To verify the updated scaling law, DeepMind trained a 70B parameter model (called 
Chinchilla) using the same compute budget as the previously trained Gopher model. 
Chinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and 
Megatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being 
4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are 
also smaller.
The findings of Chinchilla had significant ramifications for the development of future LLMs. 
Focus shifted into finding ways to scale dataset size (while maintaining quality) alongside 
increasing parameter count. Extrapolating this trend suggests that training dataset size 
may soon be limited by the amount of text data available. This has led to new research by 
Muennighoff et al.27 exploring scaling laws in data-constrained regimes.
PaLM
Pathways  language model (PaLM)28 is a 540-billion parameter transformer-based large 
language model developed by Google AI. It was trained on a massive dataset of text and 
code and is capable of performing a wide range of tasks, including common sense reasoning, 
arithmetic reasoning, joke explanation, code generation, and translation.
At the time of its release, PaLM was also able to achieve state-of-the-art performance on 
many language benchmarks, for example GLUE and SuperGLUE.29
One of the key features of PaLM is its ability to scale efficiently. This is thanks to the 
Pathways system, which Google developed to distribute the training of large language 
models across two TPU v4 Pods.
Foundational Large Language Models & Text Generation29
September 2024PaLM 2
PaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of 
architectural and training enhancements, PaLM 2 is even more capable than PaLM, with 
fewer total parameters. It excels at advanced reasoning tasks, including code generation, 
math, classification, question answering, and translation.
PaLM 2 has also been shown to be more efficient than PaLM and became the basis for a 
number of commercial models Google released as part of Google Cloud Generative AI.
Gemini
Figure 6. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all 
tokenized and fed into its transformer model. The transformer generates an output that can contain images 
and text 
Foundational Large Language Models & Text Generation30
September 2024Gemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can 
take interleaved sequences of text, image, audio, and video as input. It’s built on top of 
transformer decoders and has architectural improvements for scale as well as optimized 
inference on Google’s Tensor Processing Units (TPUs). In its current 1.5 version, these models 
are trained to support contexts of different sizes, up to 2M tokens in the Gemini 1.5 Pro 
version on Vertex AI and employ mechanisms such as multi-query attention for efficiency. 
Gemini models also employ a Mixture of Experts architecture to optimize efficiency and 
capabilities of the models. Multimodality allows the models to process text, images and video 
in input, with more modalities in input and output expected in the future.
The Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on 
size and configuration. The pre-training data consists of web documents, books, code, and 
image, audio, and video data. 
Larger models are trained for the compute-optimal number of tokens using the same 
approach as in Chinchilla paper,25 while small models are trained on significantly more tokens 
than compute optimal to improve performance for a given inference budget.
The Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini 
Nano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-
art results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and 
Gemini Nano is designed for on-device applications. The Gemini Nano models leverage 
advancements such as distillation to produce state-of-the-art performance for small 
language models on tasks such as summarization and reading comprehension. As the Gemini 
models are natively multi-modal, it can be seen that training across multiple modalities does 
indeed lead to a model that is capable of achieving strong capabilities in each domain. 
Foundational Large Language Models & Text Generation31
September 2024During the initial part of 2024, Google introduced the latest model of the Gemini family, 
Gemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This 
model  also dramatically increased the size of the context window to millions of tokens 
and is capable of recalling and reasoning over those millions of tokens, including multiple 
long documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable 
capabilities across different domains:
• Code understanding: It can process massive codebases and answer highly specific 
code-related questions.
• Language learning: The model can learn new languages never observed at training time 
solely based on reference materials provided within its input
• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene 
from the novel ‘Les Misérables’ based on a simple sketch.
• Video comprehension: It can analyze entire movies, answering detailed questions and 
pinpointing specific timestamps with remarkable accuracy.
Google’s Gemini 1.5 Pro model excels at retrieving information from even very long 
documents. In their study,32 it demonstrated 100% recall on documents up to 530,000 
tokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% 
accuracy when finding information in documents up to 10 million tokens.
Moreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex 
instructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed 
previous Gemini models. The model accurately followed almost 90% of instructions and fully 
completed 66% of the complex tasks. 
Foundational Large Language Models & Text Generation32
September 2024Gemini Flash is a new addition to the Gemini model family and the fastest Gemini model 
served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more 
cost-efficient to serve and features a breakthrough long context window of 1 million tokens. 
Although it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning 
across vast amounts of information and delivers impressive quality for its size.
Furthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open 
models built from the same research and technology used to create the Gemini models.33 The 
first model by Gemma boasts a large vocabulary of 256,000 words and has been trained on 
a massive 6 trillion token dataset. This makes it a valuable addition to the openly-available 
LLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on 
a single GPU.
Gemma 2,33 developed by Google AI, represents a significant advancement in the field of 
open large language models. Designed with a focus on efficiency, the 27-billion parameter 
model boasts performance comparable to much larger models like Llama 3 70B33 on standard 
benchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI 
developers. Its compatibility with diverse tuning toolchains, from cloud-based solutions 
to popular community tools, further enhances its versatility. With its strong performance, 
efficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation 
and democratizing AI capabilities.
Other open models
The landscape of open LLMs is rapidly evolving, with a growing number of models where 
both the code and pre-trained weights are publicly accessible. Below we highlight some of 
the known examples:
Foundational Large Language Models & Text Generation33
September 2024• LLaMA 234: Released by Meta AI, LLaMA 2 is a family of pretrained and fine-tuned 
LLMs ranging from 7B to 70B parameters. It shows significant improvements over its 
predecessor, LLaMA 1, including a 40% larger pre-training dataset (2 trillion tokens), 
doubled context length (4096 tokens), and the use of grouped-query attention. The 
fine-tuned version, LLaMA 2-Chat, is optimized for dialogue and shows competitive 
performance against closed-source models of the same size.
• LLaMA 3.221: Released by Meta AI, LLaMA 3.2 is the next generation of their open LLMs. 
Llama 3.2 includes multilingual text-only models (1B, 3B) and vision LLMs (11B, 90B), with 
quantized versions of 1B and 3B offering on average up to 56% smaller size and 2-3x 
speedup, ideal for on-device and edge deployments. LLaMA 3.2 utilizes grouped-query 
attention and a 128K token vocabulary for enhanced performance and efficiency.
• Mixtral35: Developed by Mistral AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) 
model. While its total parameter count is 47B, it utilizes only 13B active parameters per 
token during inference, leading to faster inference and higher throughput. This model 
excels in mathematics, code generation, and multilingual tasks, often outperforming 
LLaMA 2 70B in these domains. Mixtral also supports a 32k token context length, enabling 
it to handle significantly longer sequences. Its instruction-tuned version, Mixtral 8x7B-
Instruct, surpasses several closed-source models on human evaluation benchmarks.
• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 
72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show 
strong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms 
LLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in 
language understanding, reasoning, and math.
• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained 
on a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality 
through rigorous cleaning and filtering processes. The 34B model achieves performance 
Foundational Large Language Models & Text Generation34
September 2024comparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-
grade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a 
vision-language model (Yi-VL), and a depth-upscaled 9B model.
• Grok-138: Developed by xAI, Grok-1 is a 314B parameter Mixture-of-Experts model with 
25% of the weights active on a given token. It is the raw base model checkpoint from the 
pre-training phase and is not fine-tuned for specific tasks like dialogue. Grok-1 operates 
with a context length of 8k tokens.
The pace of innovation with LLMs has been rapid and shows no signs of slowing down. There 
have been many contributions to the field in both the academic and commercial settings. 
With over 20,000 papers published about LLMs in arxiv.org  it is impossible to name all 
of the models and teams that have contributed to the development of LLMs. However, an 
abbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, 
Stanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, 
NVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai, Llama from 
Meta mentioned above and many others. Some of notable companies developing commercial 
foundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI 
and many others in addition to Google and OpenAI mentioned in previous sections. It is 
important when using a model to confirm that the license is appropriate for your use case as 
many models are provided with very specific terms of use.
Comparison
In this section, we observed how transformer-based language models have evolved. They 
started as encoder-decoder architectures with hundreds of millions of parameters trained 
on hundreds of millions of tokens, and have grown to be massive decoder-only architectures 
with billions of parameters and trained on trillions of tokens. Table 1 shows how the 
important hyperparameters for all the models discussed in this whitepaper have evolved 
Foundational Large Language Models & Text Generation35
September 2024over time. The scaling of data and parameters has not only improved the performance of 
LLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-
shot generalizations to new tasks. However, even the best of these LLMs still have many 
limitations. For example, they are not good at engaging in human-like conversations, their 
math skills are limited, and they might not be aligned with human ethics (e.g., they might be 
biased or generate toxic responses). In the next section, we learn how a lot of these issues 
are being addressed.
Foundational Large Language Models & Text Generation36
September 2024Model
Attention
(2017)GPT  
(2018)GPT-2
(2019)GPT-3
(2020)LaMDA  
(2021)Gopher
(2021)Chinchilla
(2022)
Optimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W
# Parameters 213M 117M 1.5B 175B 137B 280B 70B
Vocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K
Embedding 
dimension 1024 768 1600 12288 8192 16384 8192
Key dimension 64 64 64 128 128 128 128
# heads (H) 16 12 25 96 128 128 64
# encoder 
layers 6 N/A N/A N/A N/A N/A N/A
# decoder 
layers 6 12 48 96 64 80 80
Feed forward 
dimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192
Context Token 
SizeN/A 512 1024 2048 N/A 2048 2048
Pre-Training 
tokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T
Table 1. Important hyperparameters for transformers-based large language models
A. This number is an estimate based on the reported size of the dataset.
Foundational Large Language Models & Text Generation37
September 2024Fine-tuning large language models
Large language models typically undergo multiple training stages. The first stage, often 
referred to as pre-training, is the foundational stage where an LLM is trained on large, 
diverse, and unlabelled text datasets where it’s tasked to predict the next token given the 
previous context. The goal of this stage is to leverage a large, general distribution of data 
and to create a model that is good at sampling from this general distribution. After language 
model pretraining, the resulting LLM usually demonstrates a reasonable level of language 
understanding and language generation skills across a variety of different tasks which 
are typically tested through zero-shot or few-shot prompting (augmenting the instruction 
with a few examples / demonstrations). Pretraining is the most expensive in terms of time 
(from weeks to months depending on the size of the model) and the amount of required 
computational resources, (GPU/TPU hours).
After training, the model can be further specialized via fine-tuning, typically called 
instruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a 
set of task-specific demonstration datasets where its performance is also measured across 
a set of domain-specific tasks. The following are some examples of behaviors that can be 
improved using fine-tuning:
• Instruction-tuning/instruction following: The LLM is provided as input an instruction to 
follow which might include summarizing a piece of text, writing a piece of code, or writing 
a poem in a certain style.17
• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned 
on conversational data in the form of questions and responses. This is often called 
multi-turn dialogue.39
Foundational Large Language Models & Text Generation38
September 2024• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and 
toxic outputs. It involves a multi-pronged approach encompassing careful data selection, 
human-in-the-loop validation, and incorporating safety guardrails. Techniques like 
reinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe 
and ethical responses.
Fine-tuning is considerably less costly and more data efficient compared to pre-training. 
Numerous techniques exist to optimize the costs further which are discussed later in 
this whitepaper.
Supervised fine-tuning 
As mentioned in the previous section, SFT is the process of improving an LLM’s performance 
on a specific task or set of tasks by further training it on domain-specific, labeled data. The 
dataset is typically significantly smaller than the pre-training datasets, and is usually human-
curated and of high quality. 
In this setting, each data point consists of an input (prompt) and a demonstration (target 
response). For example, questions (prompt) and answers (target response), translations from 
one language (prompt) to another language (target response), a document to summarize 
(prompt), and the corresponding summary (target response). 
It’s important to note that, while fine-tuning can be used to improve the performance on 
particular tasks as mentioned above, it can also serve the purpose of helping the LLM 
improve its behavior to be safer, less toxic, more conversational, and better at following 
instructions. 
Foundational Large Language Models & Text Generation39
September 2024Reinforcement learning from human feedback
Typically, after performing SFT, a second stage of fine-tuning occurs which is called 
reinforcement learning from human feedback  (RLHF). This is a very powerful fine-tuning 
technique that enables an LLM to better align with human-preferred responses (i.e. making 
its responses more helpful, truthful, safer, etc.). 
Figure 7. An example RLHF procedure 
In contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality 
demonstration data), RLHF makes it possible to also leverage negative outputs thus 
penalizing an LLM when it generates responses that exhibit undesired properties. Penalizing 
negative output makes it less likely to generate unhelpful or unsafe responses. 
To leverage RLHF, a reward model  (RM) typically needs to be trained with a procedure similar 
to that in Figure 7. An RM is usually initialized with a pretrained transformer model, often also 
one that is SFT. Then it is tuned on human preference data which is either single sided (with a 
prompt, response and a score) or composed of a prompt and a pair of responses along with 
Foundational Large Language Models & Text Generation40
September 2024a preference label indicating which of the two responses was preferred. For example, given 
two summaries, A and B, of the same article, a human rater selects a preferred summary 
(relying on the detailed guidance). We refer to the provided preference labels as human 
feedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, 
rank order when more than 2 candidates are evaluated, or a more detailed assessment of the 
summary quality. The preference signal can also incorporate many dimensions that capture 
various aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and 
truthfulness. 
Figure 7 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on 
preference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning 
(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to 
generate responses that are better aligned with human preferences. 
To better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human 
feedback to generate preference labels. It’s also possible to remove the need for training 
RLHF by leveraging approaches such as direct preference optimization  (DPO).45 Both RLHF 
and RLAIF can be used on Google Cloud.
Foundational Large Language Models & Text Generation41
September 2024Parameter Efficient Fine-Tuning
Both SFT and RLHF are still very costly in terms of compute time and accelerators required, 
especially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, 
there are some really useful and effective techniques that can make fine-tuning significantly 
cheaper and faster compared to pre-training and full fine-tuning. One such family of 
methods is parameter efficient fine-tuning  (PEFT) techniques. 
At a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the 
order of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. 
The perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. 
This has the benefit of training a significantly smaller set of weights, compared to traditional 
fine-tuning of the entire model. 
Some common PEFT techniques include the adapter, low-rank adaptation, and 
soft prompting:
• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-
trained model. Only the adapter parameters are trained, resulting in significantly fewer 
parameters than traditional SFT. 
• Low-Rank Adaptation  (LoRA)47 tackles efficiency differently. It uses two smaller matrices 
to approximate the original weight matrix update instead of fine-tuning the whole LLM. 
This technique freezes the original weights and trains these update matrices, significantly 
reducing resource requirements with minimum additional inference latency. Additionally, 
LoRA has improved variants such as QLoRA,48 which uses quantized weights for even 
greater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, 
meaning you can train a LoRA module that specializes in one task and easily replace it with 
another LoRA module trained on a different task. It also makes it easier to transfer the 
model since assuming the receiver has the original matrix, only the update matrices need 
to be provided.
Foundational Large Language Models & Text Generation42
September 2024• Soft prompting49 is a technique for conditioning frozen large language models with 
learnable vectors instead of hand-crafted text prompts. These vectors, called soft 
prompts, are optimized on the training data and can be as few as five tokens, making them 
parameter-efficient and enabling mixed-task inference. 
For most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft 
prompting, but the order is reversed when it comes to cost. All three approaches are more 
memory efficient than traditional fine-tuning and achieve comparable performance.
Foundational Large Language Models & Text Generation43
September 2024Python
# Before you start run this command:
# pip install --upgrade --user --quiet google-cloud-aiplatform
# after running pip install make sure you restart your kernel
import vertexai
from vertexai.generative_models import GenerativeModel
from vertexai.preview.tuning import sft
# TODO : Set values as per your requirements
# Project and Storage Constants
PROJECT_ID = ‘<project_id>’
REGION = ‘<region>’
vertexai.init(project=PROJECT_ID, location=REGION)
# define training & eval dataset.
TRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/
peft_train_sample.jsonl’
# set base model and specify a name for the tuned model
BASE_MODEL = ‘gemini-1.5-pro-002’
TUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’
# start the fine-tuning job
sft_tuning_job = sft.train(
   source_model=BASE_MODEL,
   train_dataset=TRAINING_DATASET,
   # # Optional:
   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,
)
# Get the tuning job info.
sft_tuning_job.to_dict()
# tuned model endpoint name
tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name
# use the tuned model
tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)
print(tuned_genai_model.generate_content(contents= ’What is a LLM?’ ))
Snippet 1. SFT fine tuning on Google cloud
Foundational Large Language Models & Text Generation44
September 2024Using large language models
Prompt engineering and sampling techniques have a strong influence on the performance of 
LLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) 
that you feed into an LLM to achieve desired and relevant outputs. Sampling techniques 
determine the way in which output tokens are chosen and influence the correctness, 
creativity and diversity of the resulting output. We next discuss different variants of prompt 
engineering and sampling techniques as well as touch on some important parameters that 
can have a significant impact on LLM performance.
Prompt engineering 
LLMs are very powerful, but they still need guidance to unleash their full potential. Prompt 
engineering is a critical component in guiding an LLM to yield desired outputs. This might 
include grounding the model to yield factual responses or unleashing the creativity of the 
model to tell a story or write a song. Examples of prompt engineering include providing 
clear instructions to the LLM, giving examples, using keywords, and formatting to emphasize 
important information, providing additional background details etc. 
You will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the 
context of prompt engineering. We define these terms below: 
• Few-shot prompting: This is when you provide the LLM with a task description, as well 
as a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s 
response. For example, you might provide the model with the name of a few countries 
and their capital cities, then ask it to generate the capital for a new country that isn’t in 
the examples.
Foundational Large Language Models & Text Generation45
September 2024• Zero-shot prompting: This is when you provide the LLM directly with a prompt with 
instructions. You usually give the LLM a task description and the LLM relies heavily on its 
existing knowledge to output the correct response. This requires no additional data or 
examples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.
• Chain-of-thought prompting: This technique aims to improve performance on complex 
reasoning tasks. Rather than simply asking the LLM a question, you provide a prompt 
that demonstrates how to solve similar problems using step-by-step reasoning. The 
LLM then generates its own chain of thought for the new problem, breaking it down into 
smaller steps and explaining its reasoning. Finally, it provides an answer based on its 
reasoning process.
Prompt engineering is an active area of research.
Sampling Techniques and Parameters
A variety of sampling techniques can be employed to determine how the model chooses 
the next token in a sequence. They are essential for controlling the quality, creativity, and 
diversity of the LLM’s output. The following is a breakdown of different sampling techniques 
and their important parameters:
• Greedy search50: Selects the token with the highest probability at each step. This is the 
simplest option but it can lead to repetitive and predictable outputs.
• Random sampling :50 Selects the next token according to the probability distribution, where 
each token is sampled proportionally to its predicted probability. This can produce more 
surprising and creative text, but also a higher chance of nonsensical output.
• Temperature sampling :50 Adjusts the probability distribution by a temperature parameter. 
Higher temperatures promote diversity, lower temperatures favor high-probability tokens.
Foundational Large Language Models & Text Generation46
September 2024• Top-K sampling:  Randomly samples from the top K most probable tokens. The value of K 
controls the degree of randomness.
• Top-P sampling  (nucleus sampling):51 Samples from a dynamic subset of tokens whose 
cumulative probability adds up to P. This allows the model to adapt the number of potential 
candidates depending on its confidence, favoring more diversity when uncertain and 
focusing on a smaller set of highly probable words when confident.
• Best-of-N sampling:  Generates N separate responses and selects the one deemed best 
according to a predetermined metric (e.g., a reward model or a logical consistency check). 
This is particularly useful for short snippets or situations where logic and reasoning 
are key.
By combining prompt engineering with sampling techniques and correctly calibrated 
hyperparameters, you can greatly influence the LLM’s response, making it more relevant, 
creative, and consistent for your specific needs.
Until now, we have seen the various types of LLM architectures, their underlying technology, 
as well as the approaches used to train, tune, and adapt these models for various tasks. Let’s 
now look at some key research about how the decoding process in LLMs can be sped up 
considerably to generate faster responses. 
Accelerating inference
The scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue 
to hold today. Language models have been consistently increasing in size and this has been 
a direct contributor to the vast improvement in these models’ quality and accuracy over the 
last few years. As increasing the number of parameters has improved the quality of LLMs it 
Foundational Large Language Models & Text Generation47
September 2024has also increased the computational resources needed to run them. Numerous approaches 
have been used to try and improve the efficiency of LLMs for different tasks as developers 
are incentivized to reduce cost and latency for model users. Balancing the expense of 
serving a model in terms of time, money, energy is known as the cost-performance tradeoff 
and often needs adjusting for particular use cases.
Two of the main resources used by LLMs are memory and computation. Techniques for 
improving the efficiency or speed of inference focus primarily on these resources. The 
speed of the connection between memory and compute is also critical, but usually hardware 
constrained.  As LLMs have grown in size 1000x from millions to billions of parameters. 
Additional parameters increase both the size of memory required to hold the model and 
computations needed to produce the model results.
With LLMs being increasingly adopted for large-scale and low-latency use cases, finding 
ways to optimize their inference performance has become a priority and an active research 
topic with significant advancements. We will explore a number of methods and a few 
tradeoffs for accelerating inference.
Trade offs
Many of the high yielding inference optimisation methods mandate trading off a number of 
factors, this can be tweaked on a case-by-case basis allowing for tailored approaches to 
different inference use cases and requirements. A number of the optimization methods we 
will discuss later fall somewhere on the spectrum of these tradeoffs. 
Foundational Large Language Models & Text Generation48
September 2024Trading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that 
we’re completely sacrificing that factor, it just means that we’re accepting what might be 
a marginal degradation in quality, latency or cost for the benefit of substantially improving 
another factor.
The Quality vs Latency/Cost Tradeoff
It is possible to improve the speed and cost of inference significantly through accepting 
what might be marginal to negligible drops in the model’s accuracy. One  example of this 
is using a smaller model to perform the task. Another example is quantisation where we 
decrease the precision of the model’s parameters thereby leading to faster and less memory 
intensive calculations.
One important distinction when approaching this trade-off is between the theoretical 
possibility of a quality loss versus the practical capability of the model to perform the desired 
task. This is use case specific and exploring it will often lead to significant speedups without 
sacrificing quality in a meaningful or noticeable way. For example, if the task we want the 
model to perform is simple, then a smaller model or a quantised one will likely be able to 
perform this task well. Reduction in parametric capacity or precision does not automatically 
mean that the model is less capable at that specific task.
The Latency vs Cost Tradeoff
Another name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers 
to the system’s ability at handling multiple requests efficiently. Better throughput on the same 
hardware means that our LLM inference cost is reduced, and vice versa.
Foundational Large Language Models & Text Generation49
September 2024Much like traditional software systems, there are often multiple opportunities to tradeoff 
latency against the cost of LLM inference. This is an important tradeoff since LLM inference 
tends to be the slowest and most expensive component in the entire stack; balancing latency 
and cost intentionally is key to making sure we tailor LLM performance to the product or use 
case it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) 
where cost can be a more important factor than the latency of any particular request. On the 
other hand, an LLM chatbot product will place much higher importance on request latency.
Now that we’ve covered some of the important tradeoffs to consider when optimizing 
inference, let’s examine some of the most effective inference acceleration techniques. As 
discussed in the tradeoffs section, some optimization techniques can have an impact on the 
model’s output. Therefore we will split the methods into two types: output-approximating 
and output-preserving.
Output-approximating methods
Quantization
LLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). 
During inference, matrix operations are then applied to these model weights to produce 
numerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical 
precision in which weights and activations are stored, transferred and operated upon. The 
default representation of weights and activations is usually 32 bits floating numbers, with 
quantization we can drop the precision to 8 or even 4 bit integers. 
Foundational Large Language Models & Text Generation50
September 2024Quantization has multiple performance benefits, it reduces the memory footprint of 
the model, allowing to fit larger models on the same hardware, it also reduces the 
communication overhead of weights and activations within one chip and across chips in 
a distributed inference setup- therefore speeding up inference as communication is a 
major contributor to latency. In addition, decreasing the precision of weights/activations 
can enable faster arithmetic operations on these models as some accelerator hardware 
(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower 
precision representations.
Quantization’s impact on quality can be very mild to non-existent depending on the use 
case and model.  Further, in cases where quantisation might introduce a quality regression, 
that regression can be small compared to the performance gain, therefore allowing for an 
effective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X 
speed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.
Quantization can be either applied as an inference-only operation, or it can be incorporated 
into the training (referred to as Quantisation Aware Training QAT). QAT is generally 
considered to be a more resilient approach as the model is able to recover some of the 
quantisation-related quality losses during training. To make sure we get the best cost/quality 
tradeoff, we tweak the quantization strategy (e.g. select different precisions for weights 
vs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel 
or group-wise58).
Distillation
Using a smaller model to perform a task is one of the most efficient inference optimization 
techniques, however, smaller models can demonstrate significant regressions on quality 
compared to their larger counterparts.
Foundational Large Language Models & Text Generation51
September 2024Distillation is a set of training techniques that targets improving the quality of a smaller model 
(the student) using a larger model (the teacher). This method can be effective because larger 
models outperform smaller ones even if both are trained on the same data, mainly due to 
parametric capacity and training dynamics. The gap in performance continues as the training 
dataset grows as illustrated by Figure 8.
It is worth noticing that even at low volumes of training data, large models can already 
demonstrate better performance than the correspondingly trained smaller models, this fact 
leads us to the first variant of distillation which is referred to as data distillation or model 
compression.56 We use a large model which was trained on the data we have to generate 
more synthetic data to train the smaller student model, the increase in data volume will help 
move the the student further along the quality line compared to only training on the original 
data. Synthetic data needs to be approached carefully as it needs to be of high quality and 
can lead to negative effects otherwise.
Figure 8. An illustration of the performance of models of various sizes as a function of the training 
dataset’s size
Foundational Large Language Models & Text Generation52
September 2024Other distillation techniques attempt to bring the student model closer to the teacher 
on a more granular level than just synthetic data generation. One prominent technique is 
knowledge distillation57, in this approach we attempt to align the output token distribution 
of the student model to that of the teacher’s, this can be much more sample efficient than 
data distillation. On-policy distillation59 is another technique that leverages feedback from 
the teacher model on each sequence generated by the student in a reinforcement learning 
setup. 
Output-preserving methods
These methods are guaranteed to be quality neutral, they cause no changes to the model 
output which often makes them obvious first steps to optimize inference before facing the 
more nuanced tradeoffs of the approximating methods
Flash Attention
Scaled Dot-product Attention, which is the predominant attention mechanism in the 
transformer architecture, is a quadratic operation on the input length. Optimizing the self-
attention calculation can bring significant latency and cost wins.
Flash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making 
the attention algorithm IO Aware, particularly trying to minimize the amount of data we move 
between the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in 
TPUs and GPUs. When calculating attention, the order of operations is changed and multiple 
layers are fused so we can utilize the faster memory tiers as efficiently as possible.
Foundational Large Language Models & Text Generation53
September 2024Flash Attention is an exact algorithm, it maintains the numerical output of the attention 
computation and can yield significant latency benefits due to reducing the IO overhead, Tri 
Dao et al.62 showed 2-4X latency improvements in the attention computation.
Prefix Caching
One of the most compute intensive, and thus slowest, operations in LLM inference is 
calculating the attention key and value scores (a.k.a KV) for the input we’re passing to the 
LLM, this operation is often referred to as prefill. The final output of prefill is what is termed 
KV Cache which includes the attention key and value scores for each layer of the transformer 
for the entire input. This cache is vital during the decoding phase which produces the output 
tokens, the KV cache allows us to avoid recalculating attention scores for the input on each 
autoregressive decode step.
Prefix Caching refers to the process of caching the KV Cache itself between subsequent 
inference requests in order to reduce the latency and cost of the prefill operation. The way 
the self-attention mechanism works makes reusing KV caches possible because tokens will 
only pay attention to tokens that came before them in the sequence. If there’s new input 
being appended to input that the model has seen before, then we can potentially avoid 
recalculating the prefill for the older input.
Foundational Large Language Models & Text Generation54
September 2024Figure 9. An illustration of Prefix Caching in a chat scenario
Figure 9 illustrates how prefix caching works in a multi-turn scenario with a document upload. 
On the first user turn, the prefill operation has to process the entire document therefor taking 
500ms, the resulting KV cache is then stored so that on the second user turn, we can retrieve 
the cache directly from storage and avoid recomputing it for the long doc, therefore saving 
substantial amounts of compute and latency.

Foundational Large Language Models & Text Generation55
September 2024Prefix caches can be stored either in memory or on disk and fetched on-demand. One 
important consideration is making sure that the input structure/schema remains prefix-
caching friendly, we should avoid changing the prefix in subsequent requests as that will 
invalidate the cache for all the tokens that follow For example, putting a fresh timestamp at 
the very beginning of each request will invalidate the cache completely as every subsequent 
request will have a new prefix.
Many LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots 
where users will have a multi-turn conversation that can span 10s of 1000s of tokens and 
we can avoid recalculating the KV cache for the previous parts of the conversation. Large 
document/code uploads is another use case where the artifact the user uploads will remain 
unchanged from one request to the next. All that’s changing are the questions the user is 
asking, so caching the KV cache for the document (especially for larger artifacts) can result 
in significant latency and cost savings.
Prefix caching is available as a service called Context Caching on Google AI studio52 and  
Vertex AI on Google Cloud53.
Speculative Decoding
The first phase of LLM inference, known as prefill, is compute bound due large matrix 
operations on many tokens occurring in parallel. The second phase, known as decode, is 
generally memory bound as tokens are auto-regressively decoded one at a time. 
Foundational Large Language Models & Text Generation56
September 2024It is not easy to naively use additional parallel compute capacity to speed up decode 
given the  need to wait for the current token to be produced before we can calculate what 
the next token should be (as per the self-attention mechanism), the decode process is 
inherently serial.
Speculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding 
a way to utilize the spare compute capacity to make each decode step faster. The main idea 
is to use a much smaller secondary model (often referred to as the drafter) to run ahead of 
the main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly 
as the drafter is much faster and smaller than the main model. We then use the main model to 
verify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the 
first two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted 
hypothesis with the maximum number of tokens. For example:
Figure 10. An illustration of speculative decoding over 3 tokens
Note that the 3 main model steps run in parallel. And because we are not compute bound in 
decode, we can use the spare capacity to get much better decode latencies. In the example 
above, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without 
speculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative 

Foundational Large Language Models & Text Generation57
September 2024decoding, there’s only one main model step on the critical path due to parallelization, so we 
need 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely 
quality neutral, the main model will reject any tokens that it wouldn’t have predicted itself 
in the first place, so the only thing speculative decoding does is run ahead and present 
hypotheses that the main model can accept or reject in parallel.
One important condition for speculative decoding to work effectively is that the drafter model 
has good levels of alignment with the main model, otherwise we won’t be able to accept any 
of the tokens. So investing in the training quality of the drafter model is worthwhile to get 
better latencies.
Now that we have seen some methods to make LLM generate their responses faster, let’s 
look at some examples of how these models can be applied to various tasks to get an idea 
how to use them.
Batching and Parallelization
Most of the optimization techniques we’ve discussed so far are specific to Machine Learning 
and Transformer architecture in particular. However, much like any software system, there 
are opportunities to improve throughput and latency through a combination of 1) batching 
less compute-intensive operations (i.e. we can run multiple requests on the same hardware 
simultaneously to better utilize the spare compute) and 2) parallelizing the more compute-
intensive parts of the computations (i.e. we can divide the computation and split it amongst 
more hardware instances to get more compute capacity and therefore better latencies
Batching in LLMs is most useful on the decode side - as we explained in the Speculative 
Decoding section, decode is not compute-bound and therefore there’s an opportunity 
to batch more requests. We need to be careful that we batch computations in a way that 
Foundational Large Language Models & Text Generation58
September 2024enables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs 
and GPUs). We also need to make sure we remain within the memory limits, as decode is a 
memory intensive operations, batching more requests will put more pressure on the free 
memory available. Batching has become an important component in most high-throughput 
LLM inference setups.
Parallelization is also a widely used technique given the variety of opportunities in 
transformers for horizontal scaling across more hardware instances. There are multiple 
parallelism techniques across the model input (Sequence parallelism) the model layers 
(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important 
considerations for parallelism is the cost of communication and synchronization between 
the different shards that we distribute to other machines. Communication is a significant 
overhead and can erode the benefits of adding more computational capacity if we’re not 
careful about which parallelization strategy to use. On the other hand, selecting the right 
strategy to balance the need for additional compute and the communication cost can yield 
significant latency wins.
Now that we have seen some methods to make LLM generate their responses faster, let’s 
look at some examples of how these models can be applied to various tasks to get an idea 
how to use them.
Applications
Large language models are revolutionizing the way we interact with and process information. 
With their unprecedented ability to understand context and generate content, they’re 
transforming numerous applications in the worlds of text, code, images, audio and video. 
Here we collected a few examples of application areas, but the reader should keep in mind 
that this is not a comprehensive list and that many new ideas are emerging continuously 
Foundational Large Language Models & Text Generation59
September 2024about how to best utilize the capabilities of these new tools. For more information about 
optimally building and deploying functioning applications based on the following mentioned 
use cases, refer to the subsequent whitepapers. 
It is also very simple to generate text-based responses for your use case using either 
the Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows 
code examples from these SDKs to generate responses to text prompts using the Gemini 
model. Note that the multimodal aspects of Gemini are covered in their respective 
dedicated whitepapers.
Foundational Large Language Models & Text Generation60
September 2024Python
# Before you start run this command:
# pip install --upgrade --user --quiet google-cloud-aiplatform
# after running pip install make sure you restart your kernel
import vertexai
from vertexai.language_models import TextGenerationModel
from vertexai.preview.generative_models import GenerationConfig,GenerativeModel
# Set values as per your requirements
PROJECT_ID = ‘<project_id>’  # set to your project_id
vertexai.init(project=PROJECT_ID, location= ’us-central1’ )
PROMPT= ‘What is a LLM?’  # set your prompt here
model = GenerativeModel( ‘gemini-1.5-pro-002’ )
# call the Gemini API
response = model.generate_content(
   PROMPT)
print(response.text)
# google AI Studio SDK
import google.generativeai as genai
import os
# update with your API key
genai.configure(api_key=os.environ[ “GOOGLE_API_KEY” ])
# choose the mode l
model = genai.GenerativeModel( ‘gemini-pro’ )
response = model.generate_content( ‘What is a LLM?’ ) # set your prompt here
print(response.text)
Snippet 3. Using Vertex AI and Google AI studio SDKs for unimodal text gene 
Foundational Large Language Models & Text Generation61
September 2024Code and mathematics
Generative models can comprehend and generate code and algorithms to supercharge 
developers by assisting them across many application areas. Some of the popular use cases 
for code include:
• Code generation: LLMs can be prompted in natural language to generate code in a 
specific programming language to perform certain operations. The output can be used as 
a draft.
• Code completion:  LLMS can proactively suggest useful code as the user types it. This 
can save developers time and improve code quality.
• Code refactoring and debugging:  LLMs can help reduce technical debt by refactoring 
and debugging code to improve quality, efficiency and correctness.
• Code translation:  LLMs can significantly help developer time and effort by helping to 
convert code from one programming language to another. For example, an LLM might 
convert Python code to Java.
• Test case generation: LLMs can be prompted to generate unit tests for a provided 
codebase which saves considerable time and reduces errors.
• Code documentation and understanding:  LLMs can be used in a conversational manner 
to engage in a natural language chat to help you understand a codebase. They can also 
generate appropriate comments, understand copyright status, and create release notes.
Recently, a number of exciting advancements have been made in the space of competitive 
coding and mathematics. AlphaCode 2 ,64 combines Gemini’s reasoning capabilities with 
search and the use of tools to solve competitive coding problems. It receives as input a 
description of a problem to solve, and outputs a code solution that solves the problem. It 
Foundational Large Language Models & Text Generation62
September 2024now ranks among the top 15% competitive coders on the popular Codeforces competitive 
coding platform. FunSearch65 uses an evolutionary procedure which is based on pairing 
a pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open 
problem in mathematics, and also discovered more efficient bin-packing algorithms which 
are used in many applications such as making data centers more efficient. Another recent 
approach called AlphaGeometry  tackles the problem of finding proofs for complex geometric 
theorems. It comprises a neuro-symbolic system made up of a neural language model and 
a symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad 
geometry problems, where the average human gold medalist scores on average 25.9. 67
Machine translation
LLMs are capable of generating fluid, high-quality and contextually accurate translations. 
This is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and 
context. The following are some possible real world use cases:
• Instant messaging apps:  In messaging platforms, LLMs can provide on-the-fly 
translations that feel natural. Unlike previous algorithms that might translate word-
for-word, LLMs understand slang, colloquialisms, and regional differences, enhancing 
cross-language communication.
• E-commerce: On global platforms like AliExpress, product descriptions are automatically 
translated. LLMs help with ensuring cultural nuances and idiomatic expressions in product 
details are appropriately translated, leading to fewer misunderstandings.
• Travel apps:  In apps like Google Translate, travelers get real-time spoken translations. 
With LLMs, the translated conversations are smoother, making interactions in foreign 
countries more effortless.
Foundational Large Language Models & Text Generation63
September 2024Text summarization
Text summarization is a core capability of many of the LLMs mentioned in this whitepaper. 
There are a number of natural potential use cases which include:
• News aggregators: LLMs could craft summaries that capture not only the main 
events but also the sentiment and tone of the article, providing readers with a more 
holistic understanding.
• Research databases: LLMs could help researchers generate abstracts that encapsulate 
the core findings and implications of scientific papers.
• Chat management:  In platforms like Google Chat, LLM-based systems could generate 
thread summaries that capture the urgency and tone, aiding users in prioritizing 
their responses.
Question-answering
The older generation of QA systems often worked by keyword matching, frequently missing 
out on the contextual depth of user queries. LLMs, however, dive deep into context. They can 
infer user intent, traverse vast information banks, and provide answers that are contextually 
rich and precise. Some of the examples where this could be used include:
• Virtual assistants:  LLMs can offer detailed explanations of a weather forecast 
considering the user’s location, time of year, and recent weather trends.
• Customer support:  In business platforms, LLM-based bots could provide answers that 
take into account the user’s purchase history, past queries, and potential issues, offering 
personalized assistance.
Foundational Large Language Models & Text Generation64
September 2024• Academic platforms:  On academic platforms like Wolfram Alpha, LLMs could cater to 
user queries by understanding the depth and context of academic questions, offering 
answers that suit everyone from a high school student to a postgraduate researcher.
The quality of the generated answers, as well as the corresponding citations and sources 
can be significantly improved by using advanced search systems (such as those based on 
Retrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant 
information, as well as post-hoc grounding after the response has been generated. Clear 
instructions, roles of what should and should not be used to answer the question, and 
advanced prompt engineering approaches (such as chain of thought and search/RAG 
architectures), combined with a lower temperature value amongst other things can also 
help greatly.
Chatbots
Earlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs 
transform this space by offering dynamic, human-like interactions. They can analyze 
sentiment, context, and even humor, making digital conversations feel more authentic. Some 
examples of where this can be used include:
• Customer service: A chatbot on retail platforms like Zara could not only answer product-
related queries but also offer fashion advice based on current trends.
• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, 
reacting to live events in the stream and moderating chats with contextual understanding.
Foundational Large Language Models & Text Generation65
September 2024Content generation
Text generation isn’t new, but what LLMs bring to the table is the unprecedented ability 
to generate human-like text that’s contextually relevant and rich in detail. Earlier models 
would often lose context or coherence over longer passages. LLMs, with their vast 
knowledge and nuanced understanding, can craft text spanning various styles, tones, and 
complexities, mixing factuality with creativity (depending on the context) effectively bridging 
the gap between machine-generated and human-written content. The following are some 
real-world examples:
• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. 
Instead of generic content, the LLMs could generate creative, targeted, and 
audience-specific messages.
• Scriptwriting:  LLMs could potentially assist with producing scripts for movies or TV 
shows. Writers could input themes or plot points, and the model can suggest dialogues or 
scene descriptions, enhancing the creative process.
Text generation is a wide task encompassing a variety of use cases that might range from 
those where correctness of the generated output is more or less important than its creativity/
diversity of the language. The sampling methods and parameters like temperature should be 
tuned accordingly. For more information, see the prompt engineering and architecting for 
LLM applications whitepapers.
Natural language inference
Natural language inference  (NLI) is the task of determining whether a given textual 
hypothesis can be logically inferred from a textual premise.
Foundational Large Language Models & Text Generation66
September 2024Traditional models struggled with nuanced relationships or those that require a deeper 
understanding of context. LLMs, with their intricate grasp of semantics and context, excel 
at tasks like these, bringing accuracy levels close to human performance. The following are 
some real-world examples:
• Sentiment analysis:  Businesses could utilize LLMs to infer customer sentiment from 
product reviews. Instead of just basic positive or negative tags, they could extract 
nuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.
• Legal document review:  Law firms could employ LLMs to infer implications 
and intentions in contracts, ensuring there are no contradictions or potentially 
problematic clauses.
• Medical diagnoses:  By analyzing patient descriptions and histories, LLMs could assist 
doctors in inferring potential diagnoses or health risks, ensuring early intervention.
The whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM 
applications give further insight into these use cases.
Text classification
Text classification involves categorizing text into predefined groups. While traditional 
algorithms were efficient, they often struggled with ambiguous or overlapping categories. 
LLMs, given their deep understanding of context, can classify text with higher precision, even 
when faced with subtle distinctions. Some examples of this include:
• Spam detection: Email services could utilize LLMs to classify emails as spam or 
legitimate. Instead of just keyword-based detection, the models understand the context 
and intent, potentially reducing false positives.
Foundational Large Language Models & Text Generation67
September 2024• News categorization: News platforms could employ LLMs to categorize articles into 
topics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries 
between categories.
• Customer feedback sorting:  Businesses could analyze customer feedback through 
LLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ 
ensuring targeted responses.
• Evaluating LLMs as autorater:  LLMs could be used to rate, compare and rank the 
generated outputs of other LLMs as well.
Text analysis
LLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning 
insights from vast textual datasets. Where traditional tools would scratch the surface, LLMs 
delve deep, offering rich and actionable insights. Some potential real-world examples are:
• Market research: Companies could leverage LLMs to analyze consumer conversations on 
social media, extracting trends, preferences, and emerging needs.
• Literary analysis: Academics could employ LLMs to understand themes, motifs, and 
character developments in literary works, offering fresh perspectives on classic and 
contemporary literature.
Foundational Large Language Models & Text Generation68
September 2024Multimodal applications
Multimodal LLMs, capable of processing and generating text, images, audio, and video, have 
opened up a new frontier in AI, offering a range of exciting and innovative applications across 
various sectors. The following are some examples: 
Creative content generation:
• Storytelling: An AI system could watch an image or video and spin a captivating narrative, 
integrating details from the visual with its knowledge base.
• Advertising and marketing: Generating targeted and emotionally resonant advertisements 
based on product photos or videos.
Education and accessibility:
• Personalized learning: Tailoring educational materials to individual learning styles by 
combining text with interactive visual and audio elements.
• Assistive technology: Multimodal LLMs could power tools that describe images, videos, 
and audio for visually or hearing-impaired individuals.
Business and industry:
• Document understanding and summarization: Automatically extracting key information 
from complex documents, combining text and visuals like invoices and contracts.
• Customer service: Multimodal chatbots can understand and respond to customer queries 
combining text and images, offering a richer and more personalized experience. Science 
and research:
Foundational Large Language Models & Text Generation69
September 2024• Medical diagnosis: Analyzing medical scans and reports together, identifying potential 
issues and providing insights for doctors.
• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources like 
medical images, protein structures, and research papers to accelerate research.
These examples are just the tip of the iceberg. As research progresses, the applications 
of multimodal LLMs are only expected to grow, transforming our daily lives in diverse and 
profound ways. Multimodal LLMs also benefit greatly from the existing methodologies of 
Unimodal LLMs ( i.e., text based LLMs).
LLMs, thanks to their ability to understand and process language, are reshaping how we 
interact with, generate, and analyze text across diverse sectors. As they continue to evolve, 
their applications will only grow, boosting the ability for machines and humans to have rich 
natural language interactions.
Summary
In this whitepaper we have discussed the basics of transformers, upon which all modern-day 
LLMs are based. We detailed the evolution of the various LLM model architectures and their 
components. We’ve also seen the various methodologies you can use to train and fine-tune 
models efficiently and effectively. We briefly discussed prompt engineering and sampling 
techniques that greatly influence the output of an LLM, and also touched on possible 
applications of this technology. There are a number of key takeaways to keep in mind:
Foundational Large Language Models & Text Generation70
September 2024• The transformer architecture is the basis for all modern-day LLMs. Across the various 
architectures mentioned in this whitepaper we see that it’s important not only to add more 
parameters to the model, but the composition of the dataset is equally important. 
• The order and strategies used for fine-tuning is important and may include multiple steps 
such as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important 
in capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the 
distribution from the pretraining distribution to a more desired one through the power of 
the reward function, that can reward desirable behaviors and penalize undesirable ones.
• Making inference from neural models efficient is an important problem and an active 
field of research. Many methods exist to reduce serving costs and latency with minimal 
impact to model performance, and some exact acceleration methods guarantee identical 
model outputs.
• Large language models can be used for a variety of tasks including summarization, 
translation, question answering, chat, code generation, and many more. You can 
create your own tasks using the Vertex and Makersuite text generation services which 
leverage Google’s latest language models. After the model has been trained and tuned, 
it is important to experiment with engineering prompts. You should use the technique 
most appropriate for the task-at-hand because LLMs can be sensitive to prompts k. 
Furthermore, it is also possible to enhance task specific performance or creativity and 
diversity by tweaking the parameters corresponding to sampling techniques such as 
Top-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, 
and creativity required for the task at hand.
Foundational Large Language Models & Text Generation71
September 2024Endnotes
1. 1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I., 2017, Attention is 
all you need. Advances in Neural Information Processing Systems , 30.
2. Wikipedia, 2024, Word n-gram language model. Available at:  
https://en.wikipedia.org/wiki/Word_n-gram_language_model .
3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in 
Neural Information Processing Systems,  27.
4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  
arXiv preprint arXiv:2111.00396.
5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  
https://jalammar.github.io/illustrated-transformer/ .
6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  
arXiv preprint arXiv:1607.06450.
7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition.
8. HuggingFace., 2024, Byte Pair Encoding. Available at:  
https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .
9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and 
detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.
10. HuggingFace, 2024, Unigram tokenization. Available at:  
https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .
11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .
12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.
13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information 
Processing Systems , 33, 1877-1901.
14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. 
arXiv preprint arXiv:1810.04805.
Foundational Large Language Models & Text Generation72
September 202415. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.
16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information 
Processing Systems.
17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances 
in Neural Information Processing Systems,  35, 27730-27744.-27744.
18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .
19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .
20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. 
arXiv preprint arXiv:2201.08239.
21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available 
at: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ .
22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language 
models: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf .
23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language 
models with mixture-of-experts. In International Conference on Machine Learning  (pp. 2790-2800). PMLR.
24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws 
for neural language models. arXiv preprint arXiv:2001.08361.
25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv 
preprint arXiv:2203.15556.
26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using 
model parallelism. arXiv preprint arXiv:1909.08053 .
27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.
28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine 
Learning Research , 24(240), 1-113.
29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding 
systems. Advances in Neural Information Processing Systems , 32.
30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403 .
Foundational Large Language Models & Text Generation73
September 202431. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  
https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .
32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 
Available at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .
33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. 
Available at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .
34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open 
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .
35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088 .
36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .
37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652 .
38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1 .
39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. 
arXiv preprint arXiv:2310.13650 .
40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback.  Available at:  
https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .
41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .
42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .
43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction.  MIT Press.
44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .
45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward 
model. arXiv preprint arXiv:2305.18290 .
46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In  International Conference on 
Machine Learning  (pp. 2790-2799). PMLR.
47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. 
arXiv preprint arXiv:2106.09685 .
48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314 .
Foundational Large Language Models & Text Generation74
September 202449. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv 
preprint arXiv:2104.08691 .
50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .
51. Google AI Studio Context caching. Available 
at: https://ai.google.dev/gemini-api/docs/caching?lang=python .
52. Vertex AI Context caching overview. Available 
at: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .
53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  
Available at: https://arxiv.org/abs/2111.00396 .
54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and 
activations. Available at: https://arxiv.org/abs/1609.07061 .
55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only 
inference. Available at: https://arxiv.org/abs/1712.05877 .
56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data 
Mining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .
57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  
Available at: https://arxiv.org/abs/1503.02531 .
58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained 
Quantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .
59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-
Policy Distillation of Language Models: Learning from Self-Generated Mistakes. Available 
at: https://arxiv.org/abs/2306.13649 .
60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural 
networks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .
61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  
https://arxiv.org/abs/2207.07061 .
62. Tri Dao et al. “FlashAttention. Available at:  
https://arxiv.org/abs/2205.14135 .
Foundational Large Language Models & Text Generation75
September 202463. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via 
speculative decoding. Available at: https://arxiv.org/abs/2211.17192 .
64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code 
generation with AlphaCode. Science, 378(1092-1097). DOI: 10.1126/science.abq1158.
65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical 
discoveries from program search with large language models. Nature. DOI: 10.1038/s41586-023-06924-6.
66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .
67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. 
Nature, 625, 476–482. DOI: 10.1038/s41586-023-06747-5.
Operationalizing 
Generative AI on 
Vertex AI using 
MLOps
Authors: Anant Nawalgaria,  
Gabriela Hernandez Larios, Elia Secchi,  
Mike Styer, Christos Aniftos  
and Onofrio Petragallo
Operationalizing Generative AI on Vertex AI using ML Ops2
September 2024
Acknowledgements
Reviewers and Contributors
Nenshad Bardoliwalla
Warren Barkley
Mikhail Chrestkha
Chase Lyall
Lakshmanan Sethu
Erwan Menard
Curators and Editors
Antonio Gulli
Anant Nawalgaria
Grace Mollison 
Technical Writer
Joey Haymaker
Designer
Michael Lanning 
Introduction  5
 What are DevOps and MLOps?  6
 Lifecycle of a gen AI system  7
Discover  9
Develop and experiment  10
 The foundational model paradigm  11
 The core component of LLM Systems: A prompted model component  13
 Chain & Augment  16
 Tuning & training  20
 Data Practices  23
 Evaluate  27
Deploy  30
 Deployment of gen AI systems  31
  Version control  31
  Continuous integration of gen AI systems  32
  Continuous delivery of gen AI systems  33
 Deployment of foundation models  34
  Infrastructure validation  34Table of contents

  Compression and optimization  35
  Deployment, packaging, and serving checklist  36
 Logging and monitoring  37
 Govern  41
The role of an AI platform for gen AI operations  42
 Key components of Vertex AI for gen AI  43
 Discover: Vertex Model Garden  44
 Prototype: Vertex AI Studio & Notebooks  47
 Customize: Vertex AI training & tuning  48
  Train  49
  Tune  49
  Orchestrate  51
 Chain & Augment: Vertex AI Grounding, Extensions, and RAG building blocks  52
 Evaluate: Vertex AI Experiments, Tensorboard, & evaluation pipelines  55
  Experiment  56
  Evaluation  57
 Predict: Vertex AI endpoints & monitoring  57
 Govern: Vertex AI Feature Store, Model Registry, and Dataplex  59
Conclusion  61
Endnotes  63
Operationalizing Generative AI on Vertex AI using ML Ops5
September 2024Introduction
The emergence of foundation models and generative AI (gen AI) has introduced a new era 
for building AI systems. Selecting the right model from a diverse range of architectures 
and sizes, curating data, engineering optimal prompts, tuning models for specific tasks, 
grounding model outputs in real-world data, optimizing hardware - these are just a few of the 
novel challenges that large models introduce.  
This whitepaper delves into the fundamental tenets of MLOps and the necessary adaptations 
required for the domain of gen AI and Foundation Models. We also examine the diverse range 
of Vertex AI products, specifically tailored to address the unique demands of foundation 
models and gen AI-based applications. Through this exploration we uncover how Vertex AI, 
with its solid foundations of AI infrastructure and MLOps tools, expands its capabilities to 
provide a comprehensive MLOps platform for gen AI.Emergence of foundation models 
and generative AI (gen AI) has 
introduced a new era for building 
AI systems.
Operationalizing Generative AI on Vertex AI using ML Ops6
September 2024What are DevOps and MLOps?
DevOps is a software engineering methodology that aims to bridge the gap between 
development (Dev) and operations (Ops). It promotes collaboration, automation, and 
continuous improvement to streamline the software development lifecycle, introducing 
practices such as continuous integration and continuous delivery. 
MLOps builds upon DevOps principles to address the unique challenges of operationalizing 
Machine Learning systems rapidly and reliably. In particular, MLOps tackles the experimental 
nature of ML through practices like:
• Data validation: Ensuring the quality and integrity of training data.
• Model evaluation: Rigorously assessing model performance with appropriate metrics.
• Model monitoring: Tracking model behavior in production to detect and mitigate drift.
• Tracking & reproducibility: Maintaining meticulous records for experiment tracking and 
result reproduction.
Figure 1. Machine learning workflow 
Operationalizing Generative AI on Vertex AI using ML Ops7
September 2024Lifecycle of a gen AI system
Imagine deploying your first chatbot after months of dedicated work, and it's now interacting 
with users and answering questions. Behind this seemingly simple interaction lies the 
complex and fascinating life cycle of a gen AI System, which can be broken down into five 
key moments.
First in the discovery phase , developers and AI engineers must navigate the expanding 
landscape of available models to identify the most suitable one for their specific gen AI 
application. They must consider each model's strengths, weaknesses, and costs to make an 
informed decision.
Next, development and experimentation  become paramount, with prompt engineering 
playing a crucial role in crafting and refining input prompts to elicit desired outputs based on 
an understanding of the model's intricacies. Few-shot learning, where examples are provided, 
can further guide model behavior, while additional customization may involve parameter-
efficient fine-tuning (PEFT). Most gen AI systems also involve model chaining, which refers to 
orchestrating calls to multiple models in a specific sequence to create a workflow.
Data engineering practices  have a critical role across all development stages, with factual 
grounding (ensuring the model's outputs are based on accurate, up-to-date information) and 
recent data from internal and enterprise systems being essential for reliable outputs. Tuning 
data is often needed to adapt models to specific tasks, styles, or to rectify persistent errors.
Deployment  needs to manage many new artifacts in the deployment process, including 
prompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned 
model adapters among others. These artifacts each have unique governance requirements, 
necessitating careful management throughout development and deployment. Gen AI system 
deployment also needs to account for the technical capabilities of the target infrastructure, 
ensuring that system hardware requirements are fulfilled.
Operationalizing Generative AI on Vertex AI using ML Ops8
September 2024Continuous monitoring  in production ensures improved application performance and 
maintains safety standards through responsible AI techniques, such as ensuring fairness, 
transparency, and accountability in the model's outputs.
Continuous Improvement   as a concept is still key for Gen AI-based applications, though 
with a twist. For most Gen AI applications, instead of training models from scratch, we’re 
taking foundation models (FMs) and then adapting them to our specific use case. This means 
constantly tweaking these FMs through prompting techniques, swapping them out for newer 
versions, or even combining multiple models for enhanced performance, cost efficiency, or 
reduced latency. Traditional continuous training  still holds relevance for scenarios when 
recurrent fine-tuning or incorporating human feedback loops are still needed.
Naturally, this lifecycle assumes that the foundational model powering the gen AI system is 
already operationalized. It's important to recognize that not all organizations will be directly 
involved in this part of the process. In particular, the operationalization of foundational 
models is a specialized set of tasks that is typically only relevant for a select few companies 
with the necessary resources and expertise.
Because of that, this whitepaper will focus on practices required to operationalize gen AI 
applications using and adapting existing foundation models, referring to other whitepapers in 
the book should you want to deepdive into how foundational models are operationalized. 
This includes active areas of research such as model pre-training, alignment (ensuring the 
model's outputs align with the desired goals and values), evaluation or serving.
Operationalizing Generative AI on Vertex AI using ML Ops9
September 2024
Figure 2. Lifecycle of a Foundational Model & gen AI system and relative operationalization practices 
Discover
As mentioned before, building foundational models from scratch is resource-intensive. 
Training costs and data requirements are substantial, pushing most practitioners towards 
adapting existing foundation models through techniques like fine-tuning and prompt 
engineering. This shift highlights a crucial need: efficiently discovering the optimal foundation 
model for a given use case.
These two characteristics of the gen AI landscape make model discovery an essential 
MLOps practice:
1. An abundance of models : The past year has witnessed an explosion of open-source 
and proprietary foundation models. Navigating this complex landscape, each with varying 
architectures, sizes, training datasets, and licenses, requires a systematic approach to 
identify suitable candidates for further evaluation.
Operationalizing Generative AI on Vertex AI using ML Ops10
September 20242. No one-size-fits-all solution : Each use case presents unique requirements, demanding a 
nuanced analysis of available models across multiple dimensions.
Here are some factors to consider when exploring models:
1. Quality: Early assessments can involve running test prompts or analyzing public 
benchmarks and metrics to gauge output quality.
2. Latency & throughput : These factors directly impact user experience. A chatbot 
demands lower latency than batch-processed summarization tasks.
3. Development & maintenance time : Consider the time investment for both initial 
development and ongoing maintenance. Managed models often require less effort than 
self-deployed open-source alternatives.
4. Usage cost : Factor in infrastructure and consumption costs associated with using the 
chosen model.
5. Compliance : Assess the model's ability to adhere to relevant regulations and 
licensing terms.
Because the activity of discovery has become so important for gen AI systems, many model 
discoverability platforms were created to support this need. An example of that is Vertex 
Model Garden,1 which is explored later in this whitepaper.
Develop and experiment
The process of development and experimentation remains iterative and orchestrated 
while building gen AI applications. Each experimental iteration involves a tripartite 
interplay between data refinement, foundation model(s) selection and adaptation, and 
Operationalizing Generative AI on Vertex AI using ML Ops11
September 2024rigorous evaluation. Evaluation provides crucial feedback, guiding subsequent iterations 
in a continuous feedback loop. Subpar performance might call for gathering more data, 
augmenting data, or further curating the data. Similarly, the adaptation of the foundation 
model itself might need tweaking - optimizing prompts, applying fine-tuning techniques, or 
even swapping it out for a different one altogether. This iterative refinement cycle, driven by 
evaluation insights, is just as critical for optimizing gen AI applications as it’s always been for 
traditional machine learning.
The foundational model paradigm
Foundation models differ from predictive models most importantly because they are multi-
purpose models. Instead of being trained for a single purpose, on data specific to that 
task, foundation models are trained on broad datasets, and therefore can be applied to 
many different use cases. This distinction brings with it several more important differences 
between foundation models and predictive models.
Foundation models also exhibit what are known as ‘emergent properties’,2 capabilities that 
emerge in response to specific input without additional training. Predictive models are 
only able to perform the single function they were trained for; a traditional French-English 
translation model, for instance, cannot also solve math problems.
Foundation models are also highly sensitive to changes in their input. The output of the 
model and the task it performs are strongly affected, indeed determined, by the input to the 
model. A foundation model can be made to perform translation, generation, or classification 
tasks simply by changing the input. Even insignificant changes to the input can affect its 
ability to correctly perform that task.
Operationalizing Generative AI on Vertex AI using ML Ops12
September 2024These new properties of foundation models have created a corresponding paradigm shift 
in the practices required to develop and operationalize Gen AI systems. While models in 
the predictive AI context are self-sufficient and task-specific, gen AI models are multi-
purpose and need an additional element beyond the user input to function as part of a 
gen AI Application: a prompt, and more specifically, a prompt template, defined as a set of 
instructions and examples along with placeholders to accommodate user input. A prompt 
template, along with dynamic data such as user input, can be combined to create a complete 
prompt, the text that is passed as input to the foundation model.
Figure 3.  How Prompt Template and User input can be combined to create a prompt
Operationalizing Generative AI on Vertex AI using ML Ops13
September 2024The core component of LLM Systems: A prompted 
model component
The presence of the prompt element is a distinguishing feature of gen AI applications. 
Neither the model nor the prompt is sufficient for the generation of content; gen AI needs the 
combination of both. We refer to the combination as a ‘prompted model component’. This 
is the smallest independent component sufficient to create an LLM application. The prompt 
does not need to be very complicated. It can be a simple instruction, such as “translate 
the following sentence from English to French“, followed by the sentence to be translated. 
Without that preliminary instruction, though, a foundation model would not perform the 
desired translation task. So a prompt, even just a basic instruction, is necessary along with 
the input to get the foundation model to do the task required by the application.
Figure 4.  Predictive AI unit compared with the gen AI unit
This introduces an important distinction when it comes to MLOps practices for gen AI. In 
the development of a gen AI System, experimentation and iteration need to be done in the 
context of a prompted model component , the combination of a model and a prompt. The Gen 
Operationalizing Generative AI on Vertex AI using ML Ops14
September 2024AI experimentation cycle typically begins with testing variations of the prompt – changing the 
wording of the instructions, providing additional context, or including relevant examples, etc., 
and evaluating the impact of those changes. This practice is commonly referred to as prompt 
engineering. 
Prompt engineering involves two iterative steps:
1. Prompting : Crafting and refining prompts to elicit desired behaviors from a foundational 
model for a specific use case.
2. Evaluation : Assessing the model's outputs, ideally programmatically, to gauge its 
understanding and success in fulfilling the prompt's instructions.
Figure 5.  The activity of prompt engineering
Results of an evaluation can be optionally registered as part of an experiment, to allow for 
result tracking. Since the prompt itself is a core element of the prompt engineering process, 
it becomes a first class citizen within the artifacts part of the experiment. 
However, we need to identify which type of artifacts they are. In the good old days of 
Predictive AI, we had clear lines - data was one thing, pipelines and code another. But with 
the “Prompt” paradigm in gen AI, those lines get blurry. Think about it: prompts can include 
anything from context, instructions, examples, guardrails to actual internal or external data 
pulled from somewhere else. So, are prompts data? Are they code?
Operationalizing Generative AI on Vertex AI using ML Ops15
September 2024To address these questions, a hybrid approach is needed, recognizing that a prompt has 
different components and requires different management strategies. Let’s break it down:
Prompt as Data: Some parts of the prompt will act just like data. Elements like few-shot 
examples, knowledge bases, and user queries are essentially data points. For these 
components, we need data-centric MLOps practices such as data validation, drift detection, 
and lifecycle management.
Prompt as Code: Other components such as context, prompt templates, guardrails are mode 
code-like. They define the structure and rules of the prompt itself. Here, we need code-
centric practices such as approval processes, code versioning, and testing.
As a result, when applying MLOps practices to gen AI, it becomes important to have in place 
processes that give developers easy storage, retrieval, tracking, and modification of prompts. 
This allows for fast iteration and principled experimentation. Often one version of a prompt 
will work well with a specific version of the model and less well with a different version. In 
tracking the results of an experiment, both the prompt and its components version, and the 
model version must be recorded and stored along with metrics and output data produced by 
the prompted model.
The fact that development and experimentation in gen AI requires working with the prompt 
and the model together introduces changes in some of the common MLOps practices, 
compared to the predictive AI case in which experimentation is done by changing the model 
alone. Specifically, several of the MLOps practices need to be expanded to consider the 
prompted model component together as a unit. This includes practices like evaluation, 
experiment tracking, model adaptation and deployment , and artifact managemen t, 
which will be discussed below in this whitepaper. 
Operationalizing Generative AI on Vertex AI using ML Ops16
September 2024Chain & Augment
Gen AI models, particularly large language models (LLMs), face inherent challenges in 
maintaining recency and avoiding hallucinations. Encoding new information into LLMs 
requires expensive and data-intensive pre-training, posing a significant hurdle. Additionally, 
LLMs might be unable to solve complex challenges, especially when step-by-step reasoning 
is required. Depending on the use case, leveraging only one prompted model to perform 
a particular generation might not be sufficient. To solve this issue, leveraging a divide and 
conquer approach, several prompted models can be connected together, along with calls 
to external APIs and logic expressed as code. A sequence of prompted model components 
connected together in this way is commonly known as a chain. 
Figure 6.  Components of a chain and relative development process
Operationalizing Generative AI on Vertex AI using ML Ops17
September 2024Two common chain-based patterns that have emerged to mitigate recency and 
hallucinations are retrieval augmented generation (RAG)3 and Agents. 
• RAG addresses these challenges by augmenting pre-trained models with 
“knowledge” retrieved from databases, bypassing the need for pre-training. This 
enables grounding and reduces hallucinations by incorporating up-to-date factual 
information directly into the generation process. 
• Agents, popularized by the ReAct prompting technique,4 leverage LLMs as mediators 
interacting with various tools, including RAG systems, internal or external APIs, 
custom extensions, or even with other agents. This enables complex queries and 
real-time actions by dynamically selecting and utilizing relevant information sources. 
The LLM, acting as an agent, interprets the user’s query, decides which tool to utilize, 
and how to formulate the response based on the retrieved information.
RAG and Agents approaches can be combined to create multi-agent systems connected 
to large information networks, enabling sophisticated query handling and real-time 
decision-making. 
The orchestration of different models, logic and APIs is not a novelty of gen AI 
Applications. For example, recommendation engines have long combined collaborative 
filtering models, content-based models, and business rules to generate personalized 
product recommendations for users. Similarly, in fraud detection, machine learning 
models are integrated with rule-based systems and external data sources to identify 
suspicious activities.
Operationalizing Generative AI on Vertex AI using ML Ops18
September 2024What makes these chains of gen AI components different, is that, we can't a priori 
characterize or cover the distribution  of component inputs, which makes the individual 
components much harder to evaluate and maintain in isolation.
This results in a paradigm shift in how AI applications are being developed for gen AI.
Unlike Predictive AI where it is often possible to iterate on the separate models and 
components in isolation to then chain in the AI application, in gen AI it’s often easier to 
develop a chain in integration, performing experimentation on the chain end-to-end, iterating 
over chaining strategies, prompts, the underlying foundational models and other APIs in 
a coordinated manner to achieve a specific goal. No feature engineering, data collection, 
or further model training cycles is often needed; just changes to the wording of the 
prompt template.
The shift towards MLOps for gen AI, in contrast to predictive AI, brings forth a new set of 
demands. Let's break down these key differences:
1. Evaluation : Because of their tight coupling, chains need end-to-end evaluation, not just 
on a per-component basis, to gauge their overall performance and the quality of their 
output. In terms of evaluation techniques and metrics, evaluating chains is not dissimilar 
to evaluating prompted models. Please refer to the below segment on evaluation for more 
details on these approaches.
2. Versioning : A chain needs to be managed as a complete artifact in its entirety. The chain 
configuration should be tracked with its own revision history for analysis, reproducibility, 
and understanding the impact of changes on output. Logging should also include the 
inputs, outputs, and intermediate states of the chain, and any chain configurations used 
during each execution.
Operationalizing Generative AI on Vertex AI using ML Ops19
September 20243. Continuous Monitoring : Establishing proactive monitoring systems is vital for detecting 
performance degradation, data drift, or unexpected behavior in the chain. This ensures 
early identification of potential issues to maintain the quality of the generated output. The 
activity of monitoring Chains is discussed in detail in the section ‘Logging and Monitoring’. 
4. Introspection : The ability to inspect the internal data flows of a chain (inputs and outputs 
from each component) as well as the inputs and outputs of the entire chain is paramount. 
By providing visibility into the data flowing through the chain and the resulting content, 
developers can pinpoint the sources of errors, biases, or undesirable behavior.
Figure 7.  Putting together chains, prompted models and model tuning
There are several products in Vertex AI that can support the need for chaining and 
augmentation, including Grounding as a service,5 Extensions,6 and Vector Search,7 Agent 
Builder.8 We discuss the products in the  section “Role of a AI Platform”. Langchain9 is also 
integrated with the Vertex SDK,10 and can be used alongside the core Vertex products to 
define and configure gen AI chained applications.
Operationalizing Generative AI on Vertex AI using ML Ops20
September 2024Tuning & training
When developing a gen AI use case and a specific task that involves LLMs, it can be difficult, 
especially for complex tasks, to rely on only prompt engineering and chaining to solve it.      
To improve task performance practitioners often also need to fine-tune the model directly. 
Fine-tuning lets you actively change the layers or a subset of layers of the LLM to optimize 
the capability of the model to perform a certain task. Two of the most common ways of 
tuning a model are:
1. Supervised fine-tuning: This is where we train the model in a supervised manner, teaching 
it to predict the right output sequence for a given input. 
2. Reinforcement Learning from Human Feedback (RLHF): In this approach, we first train 
a reward model to predict what humans would prefer as a response. Then, we use this 
reward model to nudge the LLM in the right direction during the tuning process. Like 
having a panel of human judges guiding the model's learning. 
Figure 8.  Putting together chains, prompted models and model tuning
Operationalizing Generative AI on Vertex AI using ML Ops21
September 2024When viewed through the MLOps lens, fine-tuning shares similar requirements with 
model training:
1. The capability to track artifacts being part of the tuning job. This includes for example the 
input data or the parameters being used to tune the model.
2. The capability to measure the impact of the tuning. This translates into the capability 
to perform evaluation of the tuned model for the specific tasks it was trained on and to 
compare results with previously tuned models or frozen models for the same task.
Platforms like Vertex AI11 (and the Google Cloud platform more broadly) provide a robust 
suite of services designed to address these MLOps requirements: Vertex Model Registry,12 
for instance, provides a centralized storage location for all the artifacts created during the 
tuning job, and Vertex Pipelines13 streamlines the development and management of these 
tuning jobs. Dataplex,14 meanwhile, provides an organization-wide data fabric for data lineage 
and governance and integrates well with both Vertex AI and BigQuery.15 What’s more, these 
products provide the same governance capability for both predictive and gen AI applications, 
meaning customers do not need separate products or configurations to manage generative 
versus AI development.
Continuous Training & Tuning
In machine learning operations (MLOps), continuous training is the practice of repeatedly 
retraining machine learning models in a production environment. This is done to ensure 
that the model remains up-to-date and performs well as real-world data patterns change 
over time. For gen AI models, continuous tuning of the models is often more practical than 
retraining from scratch due to the high data and computational costs involved. 
Operationalizing Generative AI on Vertex AI using ML Ops22
September 2024The approach to continuous tuning depends on the specific use case and goals. For relatively 
static tasks like text summarization, the continuous tuning requirements may be lower. But 
for dynamic applications like chatbots that need constant human alignment, more frequent 
tuning using techniques like RLHF based on human feedback is necessary. 
To determine the right continuous tuning strategy, AI practitioners must carefully evaluate 
the nature of their use case and how the input data evolves over time. Cost is also a major 
consideration, as the compute infrastructure greatly impacts the speed and expense of 
tuning.  We discuss in detail monitoring of GenAI systems in the Logging and Monitoring 
section of this whitepaper.
Graphics processing units (GPUs) and tensor processing units (TPUs) are key hardware for 
fine-tuning. GPUs, known for their parallel processing power, are highly effective in handling 
the computationally intensive workloads and often associated with training and running 
complex machine learning models. TPUs, on the other hand, are specifically designed 
by Google for accelerating machine learning tasks. TPUs excel in handling large matrix 
operations common in deep learning neural networks.
To manage costs, techniques like model quantization can be applied. This represents model 
weights and activations using lower-precision 8-bit integers rather than 32-bit floats, which 
reduces computational and memory requirements.
We discuss in detail the support for tuning in Vertex AI in the Customize: Vertex AI Training & 
Tuning section.
Operationalizing Generative AI on Vertex AI using ML Ops23
September 2024Data Practices
Traditionally, ML model behavior was dictated solely by its training data. While this still holds 
true for foundation models – trained on massive, multilingual, multimodal datasets – gen AI 
applications built on top of them introduce a new twist: model behavior is now determined by 
how you adapt the model using different types of input data (Figure. 9).
Figure 9.  Examples of data spectrum for foundation models – creation (left) vs. adaptation (right)
The key difference between traditional predictive ML and gen AI lies in where you start. In 
predictive ML, the data is paramount. You spend a lot of time on data engineering, and if you 
don’t have the right data, you cannot build an application. Gen AI takes a unique approach to 
this matter. You start with a foundation model, some instructions and maybe a few example 
inputs (in-context learning). You can prototype and launch an application with surprisingly 
little data. 
Operationalizing Generative AI on Vertex AI using ML Ops24
September 2024This ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on 
apriori well-defined dataset(s). In gen AI, a single application can leverage various data types, 
from completely different data sources, all working together (Figure 10). Let’s explore some 
of these data types:
• Conditioning prompts : These are essentially instructions given to the Foundation Model 
(FM) to guide its output, setting boundaries of what it can generate.
• Few-shot examples : A way to show the model what you want to achieve through input-
output pairs. This helps the model grasp the specific task(s) at hand, and in many cases, it 
boosts performances.
• Grounding/augmentation data : Data coming from either external APIs (like Google 
Search) or internal APIs and data sources. This data  permits the FM to produce answers 
for a specific context, keeping responses current, relevant without retraining the entire 
FM. This type of data also supports reducing hallucinations.
• Task-specific datasets : These are used to fine-tune an existing FM for a particular task, 
improving its performance in that specific area.
• Human preference datasets : These capture feedback on generated outputs, helping 
refine the model’s ability to produce outputs that align with human preferences. 
• Full pre training corpora : These are massive datasets used to initially train foundation 
models. While application builders may not have access to them nor the tokenizers, 
the information encoded in the model itself will influence the application’s output 
and performance.
This is not an exhaustive list. The variety of data used in gen AI applications is constantly 
growing and evolving.
Operationalizing Generative AI on Vertex AI using ML Ops25
September 2024
Figure 10.  Example of high-level data and adaptations landscape for developing gen AI applications using 
existing foundation models
This diverse range of data adds another complexity layer in terms of data organization, 
tracking and lifecycle management. Take a RAG-based application as an example: it might 
involve rewriting user queries, dynamically gathering relevant examples using a curated set 
of examples, querying a vector database, and combining it all with a prompt template. This 
involves managing multiple data types: user queries,  vector databases with curated few-shot 
examples and company information, and prompt templates.
Operationalizing Generative AI on Vertex AI using ML Ops26
September 2024Each data type needs careful organization and maintenance. For example, the vector 
database requires processing data into embeddings, optimizing chunking strategies, and 
ensuring only relevant information is available. The prompt template itself needs versioning 
and tracking, the user queries need rewriting, etc. This is where  traditional MLOps and 
DevOps best practices come into play, with a twist. We need to ensure reproducibility, 
adaptability, governance, and continuous improvement using all the data required in an 
application as a whole but also individually. Think of it this way: in predictive AI, the focus 
was on well-defined data pipelines for extraction, transformation, and loading. In gen AI, 
it's about building pipelines to manage, evolve, adapt and integrate different data types in a 
versionable, trackable, and reproducible way. 
As mentioned earlier, fine-tuning foundation models (FMs) can boost gen AI app 
performance, but it needs data. You can get this data by launching your app and gathering 
real-world data, generating synthetic data, or a mix of both. Using large models to generate 
synthetic data is becoming popular because it speeds things up, but it's still good to have a 
human check the results for quality assurance. Here are few ways to leverage large models 
for data engineering purposes:
1. Synthetic data generation : This process involves creating artificial data that closely 
resembles real-world data in terms of its characteristics and statistical properties, often 
being done with a large and capable model. This synthetic data serves as additional 
training data for gen AI, enabling it to learn patterns and relationships even when labeled 
real-world data is scarce.
2. Synthetic data correction : This technique focuses on identifying and correcting errors 
and inconsistencies within existing labeled datasets. By leveraging the power of larger 
models, gen AI can flag potential labeling mistakes and propose corrections, improving the 
quality and reliability of the training data.
Operationalizing Generative AI on Vertex AI using ML Ops27
September 20243. Synthetic data augmentation : This approach goes beyond simply generating new 
data. It involves intelligently manipulating existing data to create diverse variations while 
preserving essential features and relationships. Thus, gen AI can encounter a broader 
range of scenarios during training, leading to improved generalization and ability to 
generate nuanced and relevant outputs.
Evaluating gen AI, unlike predictive AI, is tricky. You don't usually know the training data 
distribution of the foundational models. Building a custom evaluation dataset reflecting your 
use case is essential. This dataset should cover essential, average, and edge cases. Similar 
to fine-tuning data, you can leverage powerful language models to generate, curate, and 
augment data for building robust evaluation datasets.
Evaluate
Even if only prompt engineering is performed, as any experimental process, it does require 
evaluation in order to iterate and improve. This makes the evaluation process a core activity 
of the development of any gen AI systems.
In the context of gen AI systems, evaluation might have different degrees of automation: from 
entirely driven by humans to entirely automated by a process. 
In the early days of a project, when you're still prototyping, evaluation is often a manual 
process. Developers eyeball the model's outputs, getting a qualitative sense of how it's 
performing. But as the project matures and the number of test cases balloons, manual 
evaluation becomes a bottleneck. That's when automation becomes key.
Operationalizing Generative AI on Vertex AI using ML Ops28
September 2024Automating evaluation has two big benefits. First, it lets you move faster. Instead of spending 
time manually checking each test case, you can let the machines do the heavy lifting. 
This means more iterations, more experiments, and ultimately, a better product. Second, 
automation makes evaluation more reliable. It takes human subjectivity out of the equation, 
ensuring that results are reproducible.
But automating evaluation for gen AI comes with its own set of challenges. 
For one, both the inputs (prompts) and outputs can be incredibly complex. A single prompt 
might include multiple instructions and constraints that the model needs to juggle. And the 
outputs themselves are often high-dimensional - think a generated image or a block of text. 
Capturing the quality of these outputs in a simple metric is tough.
There are some established metrics, like BLEU for translations and ROUGE for summaries, 
but they don't always tell the full story. That's where custom evaluation methods come in. 
One approach is to use another foundational model as a judge. For example, you could 
prompt a large language model to score the quality of generated texts across various 
dimensions. This is the idea behind techniques like AutoSxS.16
Another challenge is the subjective nature of many evaluation metrics for gen AI. What 
makes one output ‘better’ than another can often be a matter of opinion. The key here is to 
make sure your automated evaluation aligns with human judgment. You want your metrics 
to be a reliable proxy for what people would think. And to ensure comparability between 
experiments, it's crucial to lock down your evaluation approach and metrics early in the 
development process.
Lack of ground truth data is another common hurdle, especially in the early stages of a 
project. One workaround is to generate synthetic data to serve as a temporary ground truth, 
which can be refined over time with human feedback.
Operationalizing Generative AI on Vertex AI using ML Ops29
September 2024Finally, comprehensive evaluation is essential for safeguarding gen AI applications against 
adversarial attacks. Malicious actors can craft prompts to try to extract sensitive information 
or manipulate the model's outputs. Evaluation sets need to specifically address these attack 
vectors, through techniques like prompt fuzzing (feeding the model random variations on 
prompts) and testing for information leakage.
Automating the evaluation process ensures speed, scalability and reproducibility
An automation of the evaluation process can be considered a proxy for the 
human judgmen
Depending on the use case, the evaluation process will require a high degree 
of customization.
To ensure comparability it is essential to stabilize the evaluation approach, metrics, 
and ground truth data as early as possible in the development phase.
It is possible to generate synthetic ground truth data to accommodate for the lack of 
real ground truth data.
It is important to include test cases of adversarial prompting as part of the evaluation 
set to test the reliability of the system itself for these attacks.
Table 1.  Key suggestions to approach evaluation of gen AI systems
Operationalizing Generative AI on Vertex AI using ML Ops30
September 2024Deploy
It should be clear by this point that production gen AI applications are complex systems with 
many interacting components. Some of the common components discussed include multiple 
prompts, models, adapter layers and external data sources. In deploying a gen AI system to 
production, all these components need to be managed and coordinated with the previous 
stages of gen AI system development. Given the novelty of these systems, best practices 
for deployment and management are still evolving, but we can discuss observations and 
recommendations for these components and indicate how to address the major concerns.
Deploying gen AI solutions necessarily involves multiple steps. For example, a single 
application might utilize several large language models (LLMs) alongside a database, all 
fed by a dynamic data pipeline. Each of these components potentially requires its own 
deployment process.
For clarity, we distinguish between two main types of deployment:
1. Deployment of gen AI systems : This focuses on operationalizing a complete system 
tailored for a specific use case. It encompasses deploying all the necessary elements 
-  the application, chosen LLMs, database, data pipelines, and any other relevant 
components - to create a functioning end-user solution.
2. Deployment of foundational models : This applies to open-weight models, where the 
model weights are publicly available on platforms like Vertex Model Garden or Hugging 
Face, or privately trained models. Deployment in this scenario centers around making 
the foundational model itself accessible to users. Given their multipurpose nature, these 
deployments often aim to support various potential use cases.
Operationalizing Generative AI on Vertex AI using ML Ops31
September 2024Deployment of gen AI systems
Deployment of gen AI systems is broadly similar to deployment of any other complex 
software system. Most of the system components – databases, Python applications, etc. – 
are also found in other non-gen AI applications. As a result, our general recommendation is 
to manage these components using standard software engineering practices such as version 
control17 and Continuous Integration / Continuous Delivery (CI/CD).18
Version control
Gen AI experimentation is an iterative process involving repeated cycles of development, 
evaluation, and modification. To ensure a structured and manageable approach, it's crucial to 
implement strict versioning for all modifiable components. These components include:
• Prompt templates : Unless leveraging specific prompt management solutions, version 
them through standard version control tools like Git.
• Chain definitions : The code defining the chain (including API integrations, database calls, 
functions, etc.) should also be versioned using tools like Git. This provides a clear history 
and enables easy rollback if needed.
• External datasets : In retrieval augmented generation (RAG) systems, external datasets 
play a key role. It’s important to track these changes and versions of these datasets for 
reproducibility. You can do that by leveraging existing data analytics solutions such as 
BigQuery, AlloyDB, Vertex Feature Store. 
• Adapter models : The landscape of techniques like LoRA tuning for adapter models is 
constantly evolving. . You can leverage established data storage solutions (e.g. cloud 
storage) to manage and version these assets effectively.
Operationalizing Generative AI on Vertex AI using ML Ops32
September 2024Continuous integration of gen AI systems
In a continuous integration framework, every code change goes through automatic testing 
before merging to catch issues early. Here, unit and integration testing are key for quality 
and reliability. Unit tests act like a microscope, zooming in on individual code pieces, while 
integration testing verifies that different components work together.
The benefits of continuous integration in traditional software development are well-
understood. Implementing a CI system helps to do the following:
1. Ensure reliable, high-quality outputs : Rigorous testing increases confidence in the 
system's performance and consistency.  
2. Catch bugs early : Identifying issues through testing prevents them from causing bigger 
problems downstream. It also makes the system more robust and resilient to edge cases 
and unexpected inputs.
3. Lower maintenance costs : Well-documented test cases simplify troubleshooting and 
enable smoother modifications in the future, reducing overall maintenance efforts
These benefits are applicable to gen AI Systems as much as any software product. 
Continuous Integration should be applied to all elements of the system, including the prompt 
templates, the chain and chaining logic, and any embedding models and retrieval systems.
However, applying CI to gen AI comes with challenges:
1. Difficult to generate comprehensive test cases: The complex and open-ended nature of 
gen AI outputs makes it hard to define and create an exhaustive set of test cases that 
cover all possibilities.
Operationalizing Generative AI on Vertex AI using ML Ops33
September 20242. Reproducibility issues: Achieving deterministic, reproducible results is tricky since 
generative models often have intrinsic randomness and variability in their outputs, even for 
identical inputs. This makes it harder to consistently test for specific expected behaviors.
These challenges are closely related to the broader question of how to evaluate gen AI 
systems. Many of the same techniques discussed in the Evaluation section above can also 
be applied to the development of CI systems for gen AI. This is an ongoing area of research, 
however, and more techniques will undoubtedly emerge in the near future.
Continuous delivery of gen AI systems
Once the code is merged, a continuous delivery process begins to move the built and tested 
code through environments that closely resemble production for further testing before the 
final deployment. 
As mentioned in the "'"Develop and Experiment"'" segment, chain elements become one 
of the main components to deploy, as they fundamentally constitute the gen AI application 
serving users.
The delivery process of the gen AI application containing the chain may vary depending on 
the latency requirements and whether the use case is batch or online:
1. Batch use cases require deploying a batch process executed on a schedule in production. 
The delivery process should focus on testing the entire pipeline in integration in an 
environment close to production before deployment. As part of the testing process, 
developers can assert specific requirements around the throughput of the batch process 
itself and checking that all components of the application are functioning correctly (e.g., 
permissioning, infrastructure, code dependencies).
Operationalizing Generative AI on Vertex AI using ML Ops34
September 20242. Online use cases require deploying an API, in this case, the application containing the 
chain, capable of responding to users at low latency. The delivery process should involve 
testing the API in integration in an environment close to production, with tests to assert 
that all components of the application are functioning correctly (e.g., permissioning, 
infrastructure, code dependencies). Non-functional requirements (e.g., scalability, 
reliability, performance) can be verified through a series of tests, including load tests.
Deployment of foundation models
Because foundation models are so large and complex, deployment and serving of these 
models raises a number of issues – most obviously, the compute and storage resources 
needed to run these massive models successfully. At a minimum, a foundation model 
deployment needs to include several key considerations: selecting and securing necessary 
compute resources, such as GPUs or TPUS; choosing appropriate data storage services 
like BigQuery or Google Cloud Storage that can scale to deal with the large datasets; and 
implementing model optimization or compression techniques.
Infrastructure validation
One technique that can be applied to address the resource requirements of gen AI systems is 
infrastructure validation. This refers to the introduction of an additional verification step, prior 
to deploying the training and serving systems, to check both the compatibility of the model 
with the defined serving configuration and the availability of the required hardware. There 
are a number of optional infrastructure validation layers that can perform some of these 
checks automatically. For instance, TFX19 has an infrastructure validation layer that checks 
Operationalizing Generative AI on Vertex AI using ML Ops35
September 2024whether the model will run correctly on a specified hardware configuration, which can help 
catch configuration issues before deployment. Nevertheless, the availability of the required 
hardware still needs to be verified by hand by the engineer or the system administrator.
Compression and optimization
Another way of addressing infrastructure challenges is to optimize the model itself. 
Compressing and/or optimizing the model can often significantly reduce the storage and 
compute resources needed for training and serving, and in many cases can also decrease 
the serving latency.
Some techniques for model compression and optimization include quantization, distillation 
and model pruning. Quantization reduces the size and computational requirements of the 
model by converting its weights and activations from higher-precision floating-point numbers 
to lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. 
This can significantly reduce the memory footprint and computational overhead of the model. 
Model Pruning is a technique for eliminating unnecessary weight parameters or by selecting 
only important subnetworks within the model. This reduces model size while maintaining 
accuracy as high as possible. Finally, distillation trains a smaller model, using the responses 
generated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. 
This can significantly reduce the amount of training data, compute, and storage resources 
needed for the application.
In certain situations, model distillation can also improve the performance of the model itself 
in addition to reducing resource requirements. This happens because the smaller model can 
combine the knowledge of the larger model with labeled data, which can help it to generalize 
better to new data on a limited use case.The process of distillation usually involves training 
a large foundational LLM (teacher model) and having it generate responses to certain tasks, 
Operationalizing Generative AI on Vertex AI using ML Ops36
September 2024and then having the smaller LLM (student model) use a combination of the LLMs knowledge 
as well as task specific supervised dataset to learn. The size and complexity of the smaller 
LLM can be adjusted to achieve the desired trade-off between performance and resource 
requirements. A technique known as step-by-step distillation20 has proven to achieve 
great results.
Deployment, packaging, and serving checklist
Following are the important steps to take when deploying a model on Vertex AI.
 □Configure version control : Implement version control practices for LLM deployments. 
This allows you to roll back to previous versions if necessary and track changes made to 
the model or deployment configuration.
 □Optimize the model : Perform any model optimization (distillation, quantization, pruning, 
etc.) before packaging or deploying the model.
 □Containerize the model : Package the trained LLM model into a container. 
 □Define target hardware requirements : Ensure the target deployment environment 
meets the requirements for optimal performance of the LLM model, such as GPUs, as well 
as TPUs and other specialized hardware accelerators.
 □Define model endpoint : Define the endpoint configuration using Vertex AI's endpoint 
creation interface or the Vertex AI SDK. Specify the model container, input and output 
formats, and any additional configuration parameters.
 □Allocate resources : Allocate the appropriate compute resources for the endpoint based 
on the expected traffic and performance requirements. 
Operationalizing Generative AI on Vertex AI using ML Ops37
September 2024 □Configure access control : Set up access control mechanisms to restrict access to 
the endpoint based on authentication and authorization policies. This ensures that only 
authorized users or services can interact with the deployed LLM.
 □Create model endpoint : Create a Vertex AI endpoint to deploy21 the LLM as a REST API 
service. This allows clients to send requests to the endpoint and receive responses from 
the LLM..
 □Configure monitoring and logging : Establish monitoring and logging systems to track 
the endpoint's performance, resource utilization, and error logs.
 □Deploy custom integrations : Integrate the LLM into custom applications or services 
using the model's SDK or APIs. This provides more flexibility for integrating the LLM into 
specific workflows or frameworks.
 □Deploy Real-time Applications : For real-time applications, consider using Cloud 
Functions and Cloud Run in combination with LLMs hosted in Vertex AI to create a 
streaming pipeline that processes data and generates responses in real time.
Logging and monitoring
Monitoring gen AI applications and, as a result, their components, presents unique 
challenges that require additional techniques and approaches on top of those in traditional 
MLOps. The use of gen AI requires the chaining of components in order to produce results 
for practical applications. Additionally, to your application user, all the components are 
hidden. Therefore, the interface they have to your application is their input and the final 
output. This creates the need to log and monitor your application end-to-end: that is, logging 
and monitoring the input and output of your application overall as well as the input and 
output of every single component.
Operationalizing Generative AI on Vertex AI using ML Ops38
September 2024Logging is necessary for applying monitoring and debugging on your gen AI system in 
production. An input to the application triggers multiple components. Imagine the output 
to a given input is factually inaccurate. How can you find out which of the components are 
the ones that didn’t perform well? To answer this question it is necessary to apply logging 
on the application level and at the component level. We need lineage in our logging for all 
components executed. For every component we need to log their inputs and outputs. We 
also need to be able to map those with any additional artifacts and parameters they depend 
on so we can easily analyze those inputs and outputs.
Monitoring can be applied to the overall gen AI application and to individual components. We 
prioritize monitoring at the application level. This is because if the application is performant 
and monitoring proves that, it implies that all components are also performant. You can also 
apply the same practices to each of the prompted model components to get more granular 
results and understanding of your application.
Skew detection in traditional ML systems refers to training-serving skew that occurs when 
the feature data distribution in production deviates from the feature data distribution 
observed during model training. In the case of Gen AI systems using pretrained models in 
components chained together to produce the output, we need to modify our approach. We 
can measure skew by comparing the distribution of the input data we used to evaluate our 
application (the test set as described under the Data Curation and Principles section above) 
and the distribution of the inputs to our application in production. Once the two distributions 
drift apart,further investigation is needed. The same process can be applied to the output 
data as well.
Operationalizing Generative AI on Vertex AI using ML Ops39
September 2024
Figure 11.  Drift/skew detection process overview
Like skew detection, the drift detection process checks for statistical differences between 
two datasets. However, instead of comparing evaluations and serving inputs, drift looks for 
changes in input data. This allows you to check how the inputs and therefore the behavior of 
your users changed over time. This is the same as traditional MLOps. 
Given that the input to the application is typically text, there are a few approaches to 
measuring skew and drift. In general all the methods are trying to identify significant 
changes in production data, both textual (size of input) and conceptual (topics in input), 
when compared to the evaluation dataset. All these methods are looking for changes that 
could potentially indicate the application might not be prepared to successfully handle the 
nature of the new data that are now coming in. Some common approaches are calculating 
embeddings and distances, counting text length and number of tokens, and tracking 
vocabulary changes, new concepts and intents, prompts and topics in datasets, as well 
as statistical approaches such as least-squares density difference,22 maximum mean 
discrepancy (MMD),23 learned kernel MMD,24 or context-aware MMD.25 As gen AI use cases 
are so diverse, it is often necessary to create additional custom metrics that better capture 
abnormal changes in your data.
Operationalizing Generative AI on Vertex AI using ML Ops40
September 2024Continuous evaluation is another common approach to GenAI application monitoring. In 
a continuous evaluation system, you capture the model's production output and run an 
evaluation task using that output, to keep track of the model's performance over time. One 
approach is collecting direct user feedback, such as ratings (for example thumbs up/down), 
which provides immediate insight into the perceived quality of outputs. In parallel, comparing 
model-generated responses against established ground truth, often collected through 
human assessment or as a result of an ensemble AI Model approach, allows for deeper 
analysis of performance. Ground truth metrics can be used to generate evaluation metrics 
as described in the Evaluation section. This process provides a view on how your evaluation 
metrics changed from when you developed your model to what you have in production today.
As with traditional monitoring in MLOps an alerting process should be deployed for notifying 
application owners when a drift, skew or performance decay from evaluation tasks is 
detected. This can help you promptly intervene and resolve issues. This is achieved by 
integrating alerting and notification tools into your monitoring process.
Monitoring expands beyond drift, skew and evaluation tasks. Monitoring in MLOps includes 
efficiency metrics like resources utilization and latency. Efficiency metrics are as relevant and 
important in gen AI as they are in any other AI application.
Vertex AI provides a set of tools that can help with monitoring. Model Evaluation for gen AI26 
tasks can be used for classification, summarization, question answering, and text generation 
tasks. Vertex Pipelines can be used to allow the recurrent execution of evaluation jobs in 
production as well as running pipelines for skew and drift detection processes.
Operationalizing Generative AI on Vertex AI using ML Ops41
September 2024Govern
In the context of MLOps governance encompasses all the practices, and policies that 
establish control, accountability, and transparency over the development, deployment, and 
ongoing management of machine learning (ML) models, including all the activities related to 
the code, data and models lifecycle.
As mentioned in the Develop & Experiment section the chain element and the relative 
components become a new type of assets that need to be governed over the full lifecycle 
from development to deployment, to monitoring. 
The governance of the chain element lifecycle extends to lineage tracking practices as well.  
While for predictive AI systems lineage focuses on tracking and understanding the complete 
journey of a machine learning model, in gen AI, lineage goes beyond the model artifact 
extending to all the components in the chain. This includes the data and models used and 
their lineage, the code involved and the relative evaluation data and metrics. This can help 
auditing, debugging and improvements of the models
Along with these new practices, existing MLOps and DevOps practices still apply to MLOps 
for gen AI:
1. The need to govern the data lifecycle; see “ Data Practices ”.
2. The need to govern the tuned model lifecycle; see “ Tuning and Training ”.
3. The need to govern the code lifecycle; see “ Deployment of GenAI 
System components ”.
Operationalizing Generative AI on Vertex AI using ML Ops42
September 2024The next segment will introduce a set of products that allow developers to perform 
governance of the data, model and code assets. We will discuss products like Google 
Cloud Dataplex, which centralizes the governance of model and data, Vertex ML Metadata 
and Vertex Experiment, which allows developers to register experiments, their metrics 
and artifacts.
The role of an AI platform for gen 
AI operations
Alongside the explosion of both predictive and gen AI applications, AI platforms, like Vertex 
AI,11 have emerged as indispensable tools for organizations seeking to leverage the power of 
Artificial Intelligence (AI). These comprehensive platforms provide a unified environment that 
streamlines the entire AI lifecycle, from data preparation and model training to deployment, 
automation, continuous integration/continuous delivery (CI/CD), governance, and monitoring.
At the heart of an AI platform lies its ability to support diverse AI development needs. 
Whether you seek to utilize pre-trained AI solutions, adapt existing models through tuning 
or transfer learning, or embark on training your own large models, AI platforms provide the 
infrastructure and tools necessary to support these journeys. The advent of these platforms 
has revolutionized the way organizations approach AI, enabling them to productionize AI 
applications in a secure, enterprise-ready, responsible, controlled and scalable manner. 
These platforms accelerate innovation as well as foster reproducibility and collaboration 
while reducing costs and maximizing Return on Investment (ROI).
Operationalizing Generative AI on Vertex AI using ML Ops43
September 2024The new gen AI paradigm discussed in prior sections demands a robust and reliable AI 
platform that can seamlessly integrate and orchestrate a wide range of functionalities.   
These functionalities include model tuning for specific tasks; leveraging paradigms like 
retrieval augmented generation3 (RAG) to connect to internal and external data sources; 
and pre-training or instruction fine-tuning large models from scratch. Complex applications 
also often require chaining with other models, such as classifiers to route inputs to the 
appropriate LLM/ML model, extraction of customer information from a knowledge base, 
inclusion of safety checks, or even creation of caching systems for cost optimization.  
Figure 12.  Key components of Vertex AI for gen AI
Key components of Vertex AI for gen AI
Vertex AI eliminates the complexities of managing the entire infrastructure required for AI 
development and deployment. Instead, Vertex AI offers a user-centric approach, providing 
on-demand access to the needed resources. This flexibility empowers organizations to 
focus on innovation and collaboration, rather than infrastructure management, and up-
front hardware purchase. The features of Vertex AI that support gen AI development can be 
grouped into eight areas.
Operationalizing Generative AI on Vertex AI using ML Ops44
September 2024Discover: Vertex Model Garden
As discussed before, there is already a wide variety of available foundation models, trained 
on a broad range of datasets, and the cost of training a new foundation model can be 
prohibitive. Thus it often makes sense for companies to adapt existing foundation models 
rather than creating their own from scratch. As a result, a platform facilitating seamless 
discovery and integration of diverse model types is critical.
Vertex AI Model Garden1 supports these needs, offering a curated collection of over 
150 Machine Learning and gen AI models from Google, Google partners, and the open-
source community. It simplifies the discovery, customization, and deployment of both 
Google’s proprietary foundational models and diverse open-source models across a 
vast spectrum of modalities, tasks, and features. This comprehensive repository permits 
developers to leverage the collective research on artificial intelligence models within a single 
streamlined environment.
Model Garden encompasses a diverse range of modalities such as Language, Vision, Tabular, 
Document, Speech, Video, and Multimodal data. This broad coverage enables developers 
to tackle a multitude of tasks, including generation, classification, regression, extraction, 
recognition, segmentation, tracking, translation, and embedding. Model Garden houses 
Google’s proprietary and foundational models (like Gemini,27 PaLM 2,28 Imagen29) alongside 
numerous popular open source and third-party partner models like like Llama 3,30 T5 Flan,31 
BERT,32 Stable Diffusion,33 Claude 3 (Anthropic),34 and Mistral AI.35 Additionally, it offers task-
specific models for occupancy analysis, watermark detection, text-moderation, text-to-video, 
hand-gesture recognition, product identification, and tag recognition, among others. Every 
model36 in Vertex Model Garden has a model card which includes a description of the model, 
the main use cases that can cover, and the option (if available) to tune the model or deploy 
it directly.
Operationalizing Generative AI on Vertex AI using ML Ops45
September 2024Model Garden fosters experimentation by facilitating access to Google’s proprietary 
foundational models through the Vertex AI Studio UI,37 a playground where you can play 
around with prompts, models, and open-source models using provided Colab notebooks. 
One-click deployment is available for some external models, and there are more than 40 
models available for fine-tuning for specific needs. Furthermore, the platform allows users to 
leverage technologies like vLLM38 and quantization techniques for optimizing deployments for 
efficiency and reduced costs. We present below an overview of some of the models in Model 
Garden. For an up-to-date list, please visit.36
Operationalizing Generative AI on Vertex AI using ML Ops46
September 2024Model Type Description Details
First-party models Foundation models
Leverage multimodal models 
from Google across vision, 
dialog, code generation, and 
code completion.Gemini39 and Palm240
Imagen for text-to-image41
Codey for code generation 
and completion42
Chirp for speech-to-text43
First-party models Pre-trained APIs
Build and deploy AI 
applications faster with our 
pre-trained APIs powered by 
the best Google AI research 
and technology.Text-to-Speech44
Natural Language processing45
Translation46
Vision47
Open  models Open source models
Access a wide variety of 
enterprise-ready open 
source modelsGoogle’s Gemma,48 PaliGemma,16 
CodeGemma49 
Meta's Llama30
TII's Falcon50
Mistral AI51
BERT,32 T-5 FLAN,31 
ViT,52 EfficientNet53
Third-party models Third-party models
Model Garden will support 
third-party models 
from partners with 
foundation models.Anthropic’s Claude 3 Haiku,  
Sonnet and Opus54,55
Table 2.  An overview of some of the models in Model Garden [Last Updated: March 18th, 2024]
Operationalizing Generative AI on Vertex AI using ML Ops47
September 2024Prototype: Vertex AI Studio & Notebooks
Rapid development and prototyping capabilities are also essential for developing gen AI 
applications. Vertex AI prioritizes inclusivity and flexibility in its development environments, 
catering to a wide range of developer preferences and proficiency levels. This platform 
provides options for both console-driven and programmatic development workflows. Users 
can leverage the intuitive web interface for end-to-end application creation or utilize various 
APIs for deeper customization and control. These include the REST API56 and dedicated 
SDKs for Python,57 NodeJS58 and Java,59 ensuring compatibility with diverse programming 
languages and ecosystems. Developers can choose to use the tools and IDEs of their 
choice for interacting with the platform, or take advantage of Vertex-native tools like Vertex 
Colab Enterprise or Vertex Workbench to explore and experiment with code within familiar 
notebook environments.
Vertex AI Studio60 provides a unified console-driven entry point to access and leverage the 
full spectrum of Vertex AI's gen AI services. It facilitates exploration and experimentation with 
various Google first party foundation models (for example, PaLM 2, Gemini, Codey, Imagen, 
and Universal Speech Model). Additionally, it offers prompt examples and functionalities 
for testing distinct prompts and models with diverse parameters. It’s also possible to adapt 
existing models through various techniques like supervised fine-tuning (SFT), reinforcement 
learning tuning techniques, and Distillation, and deploy gen AI applications in just a few 
clicks. Vertex AI Studio considerably simplifies and democratizes gen AI adoption, catering 
to a variety of users, from business analysts to machine learning engineers. You can see the 
homepage of Vertex AI Studio in Figure 13.
Operationalizing Generative AI on Vertex AI using ML Ops48
September 2024
Figure 13.  Vertex AI Studio - Homepage
Customize: Vertex AI training & tuning 
While prompt engineering and augmentation are sufficient for some gen AI use cases, other 
cases require training, tuning and adapting the models to get the best results. Vertex AI 
provides a comprehensive platform for training and adapting LLMs, supporting a range of 
techniques and approaches from prompt engineering to training models from scratch.
Operationalizing Generative AI on Vertex AI using ML Ops49
September 2024Train 
For full-scale LLM training, TPUs and GPUs are vital because of their superior processing 
power and memory capacity compared to CPUs. GPUs excel at parallel processing, enabling 
faster model training. TPUs, specifically designed for machine learning tasks, offer even 
faster processing and higher energy efficiency. This makes them ideal for large-scale, 
complex models. Google Cloud provides a range of offerings to support LLM training, 
including TPU VMs with various configurations, pre-configured AI platforms like Vertex AI, 
and dedicated resources like Cloud TPU Pods for scaling up training. These offerings allow 
users to choose the right infrastructure for their needs, accelerating LLM development and 
enabling cutting-edge research and applications.
Tune 
Vertex AI also provides a comprehensive solution for adapting pre-trained LLMs. It supports 
a spectrum of techniques from a non-technical prompt engineering playground at inference 
time, to data-driven approaches involving tuning, reinforcement learning and distillation 
methods during the development or adaptation phase. The following five techniques – many 
of which are unique to Vertex AI – enable users to explore and implement them effectively. 
This applies to both proprietary and open-source LLMs, allowing you to achieve superior 
results while optimizing for costs and latency requirements.
• Prompt engineering61 leverages carefully crafted natural language prompts, potentially 
chained and enriched with external knowledge and examples, to nudge the LLM towards 
desired outputs without necessitating further training. Vertex AI through Vertex AI Studio 
offers a dedicated playground for crafting, testing, comparing and managing diverse 
prompts and techniques. Users can access various pre-built prompt templates within the 
platform and leverage public prompting guidelines62 for Google’s proprietary large models. 
Operationalizing Generative AI on Vertex AI using ML Ops50
September 2024• Supervised fine-tuning (SFT)63 on Vertex AI facilitates model adaptation by leveraging a 
set of labeled examples (even a few hundred is enough) to tune a model on specific tasks 
and contexts within domain-specific datasets. The required examples resemble the one-
shot example structure employed in the construction of a prompt. This effectively extends 
the few-shot learning approach for enhanced optimization. This focused tuning enables 
the model to encode additional parameters in the model necessary for mimicking desired 
behaviors such as improved complex prompt comprehension, adaptation to specific 
output formats, correcting errors, and learning new tasks. The SFT tuning approach on 
Vertex AI, minimizes computational overhead and time while yielding an updated model 
that integrates the newly acquired parameters with the original model’s core parameters.
• Reinforcement learning with human feedback (RLHF) ,64 available on Vertex AI for 
foundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2, 
leverages human feedback to train large models to align with human preferences. This 
technique is well-suited in complex tasks involving preference modeling and optimizes 
LLMs on intricate, sequence-level objectives not easily addressed by traditional 
supervised fine-tuning. The process involves first training a reward model using a human 
preference dataset, then utilizing it to score the output from the LLM, and finally applying 
reinforcement learning to optimize the LLM. This approach is recognized as a key driver of 
success in conversational large language models.
• Distillation step-by-step20 is an advanced distillation technique transferring knowledge 
from a significantly larger model (known as teacher model) to a smaller task-specific 
model (known as student model), preserving important information while reducing model 
size. Step-by-Step Distillation20 surpasses common techniques by requiring significantly 
less data. This method, accessible on Vertex AI,65 significantly reduces inference costs and 
latencies while minimizing performance impact in the resulting smaller LLM.66
Operationalizing Generative AI on Vertex AI using ML Ops51
September 2024Orchestrate
Any training or tuning job you run can be orchestrated and then operationalized using Vertex 
Pipelines,13 a service that aims to simplify and automate the deployment, management, and 
scaling of your ML workflows. 
It provides a platform for building, orchestrating, scheduling and monitoring complex and 
custom ML pipelines, enabling you to efficiently translate your models from prototypes 
to production.
Vertex Pipelines is also the platform behind all the managed tuning and evaluation services 
for the Google Foundation Models on Vertex AI. This ensures consistency as you can 
consume and extend those pipelines easily, without having to familiarize yourself with 
many services.
Getting started with Vertex Pipelines is simple: you define the pipeline’s step sequence in 
a Python file utilizing Kubeflow SDK.67 For further details and comprehensive onboarding, 
consult the official documentation.68
Operationalizing Generative AI on Vertex AI using ML Ops52
September 2024Chain & Augment: Vertex AI Grounding, Extensions, and RAG 
building blocks
Beyond training, tuning and adapting models and prompts directly, Vertex AI offers a 
comprehensive ecosystem for augmenting LLMs, to address the challenges of factual 
grounding and hallucination. The platform incorporates emerging techniques like RAG and 
agent-based approaches.
RAG overcomes limitations by enriching prompts with data retrieved from vector databases, 
circumventing pre-training requirements and ensuring the integration of up-to-date 
information. Agent-based approaches, popularized by ReAct prompting, leverage LLMs as 
mediators interacting with tools like RAG systems, APIs, and custom extensions. Vertex AI 
facilitates this dynamic information source selection, enabling complex queries, real-time 
actions, and the creation of multi-agent systems connected to vast information networks for 
sophisticated query processing and real-time decision-making.
Vertex AI function calling69 empowers users by enhancing the capabilities of language 
models (LLMs). It enables LLMs to access real-time data and interact with external systems, 
providing users with more accurate and up-to-date information. To do that, users need to 
provide function definitions such as description, inputs, outputs to the gen AI model. Instead 
of directly executing functions, the LLM intelligently analyzes user requests and generates 
structured data outputs. These outputs propose which function to call and what arguments 
to use.
Vertex AI Grounding5 helps users connect large models with verifiable information by 
grounding them to internal data corpora on Vertex AI Agent Builder70 or external sources 
using Google Search. This enables two key functionalities: verifying model-generated outputs 
against internal or external sources and creating RAG systems using Google’s advanced 
search capabilities that produce quality content grounded in your own or web search data. 
Operationalizing Generative AI on Vertex AI using ML Ops53
September 2024Vertex AI extensions6 let developers integrate Vertex Foundation Models with real-time 
data and real-world actions through APIs and functions, enabling task execution and allowing 
enhanced capabilities. This extends to leveraging 1st party extensions like Vertex AI Search7 
and Code Interpreter,71 or 3rd party extensions for triggering and completing transactions. 
Imagine building an application that leverages the LLM's knowledge to plan a trip and 
seamlessly utilizes internal APIs to book hotels and flights, all within a single interface. 
Additionally, Vertex Extensions facilitate function calling with the gemini-pro model, enabling 
you to generate descriptions, pass them to the large model, receive JSON with function 
arguments, and automatically call the function.
Vertex AI Agent Builder70 is an out-of-the-box solution that allows you to quickly build gen 
AI agents, to be used as conversational chatbots or as part of a search engine. With Vertex 
AI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range 
of data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL, 
unstructured sources like website content crawling and cloud storage as well as connectors 
to Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search 
technologies, encompassing semantic search, content chunking, ranking, algorithms, 
and user intent understanding. Under the hood it optimizes document loading, chunking, 
embedding models, and ranking strategies. It abstracts away these complexities and allows 
users to simply specify their data source to initiate the gen AI-powered agent.This approach 
is ideal for organizations seeking to build robust search experiences for standard use cases 
without extensive technical expertise. 
Vector databases are specialized systems for managing multi-dimensional data. This data, 
encompassing images, text, audio, video, and other structured or unstructured formats, 
is represented as vectors capturing its semantic meaning. Vector databases accelerate 
searching and retrieval within these high-dimensional spaces, enabling efficient tasks like 
finding similar images from billions or extracting relevant text snippets based on various 
Operationalizing Generative AI on Vertex AI using ML Ops54
September 2024inputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible 
solutions for storing and serving embeddings at scale, catering to diverse use cases and 
user profiles.
Vertex AI Vector Search7 is a highly scalable low-latency similarity search and fully 
managed vector database scaling to billions of vector embeddings with auto-scaling. This 
technology, built upon ScaNN72 (a Google-developed technology used in products like 
Search, YouTube, and Play), allows you to search from billions of semantically similar or 
related items within your stored data. In the context of gen AI, the most common use cases 
where Vertex Vector Search can be used are:
1. Finding similar items (either text or image) based solely on their semantic meaning, in 
conjunction with an embedding model.
2. Creating a hybrid search approach that combines semantic and keyword or metadata 
search to refine the results.
3. Extracting relevant information from the database to feed into LLMs, enabling them to 
generate more accurate and informed responses.
Vertex AI Vector Search primarily functions as a vector database for storing pre-generated 
embeddings. These embeddings must be created beforehand using separate models like 
Vertex Embedding models73 (namely textembedding-gecko, text-embedding-gecko-
multilingual, or multimodalembedding ). Choosing Vertex Vector Search is optimal 
when you require control over aspects like the chunk, retrieval, query and models strategy. 
This includes fine-tuning an embedding model for your specific data. However, if your use 
case is a standard one requiring little customization, a readily available solution like Vertex 
Search might be a better choice.
Operationalizing Generative AI on Vertex AI using ML Ops55
September 2024Vertex AI Feature Store74 is a centralized and fully managed repository for ML features 
and embedding. It enables teams to share, serve, and reuse machine learning features and 
embeddings effortlessly alongside other data. Its native BigQuery23 integration eliminates 
duplication, simplifies lineage tracking and preserves data governance. Vertex AI Feature 
Store supports offline retrieval and an easy and fast online serving for machine learning 
features and embeddings. Vertex AI Feature Store is a good choice when you want to iterate 
and maintain different embedding versions alongside other machine learning features in a 
single place.
Vertex AI offers the flexibility to seamlessly create and connect various products to build 
your own custom grounding, RAG, and Agent systems. This includes utilizing diverse 
embedding models (multimodal, multilingual), various vector stores (Vector Search, Feature 
Store) and search engines like Vertex AI Agent Builder, extensions, grounding, and even SQL 
query generation for complex natural language queries. Moreover, Vertex AI provides SDK 
integration with LangChain9 to easily build and prototype applications using the umbrella 
of Vertex AI products. For further details and integration information, consult the official 
documentation75 and official examples.76
Evaluate: Vertex AI Experiments, Tensorboard, & 
evaluation pipelines
In the dynamic world of gen AI, experimentation and evaluation are the cornerstones of 
iterative development and continuous improvement. With a multitude of variables influencing 
Gen AI models (prompt engineering, model selection, data interaction, pretraining, 
and tuning), evaluation goes hand-in-hand with experimentation. The more seamlessly 
experiments and evaluations can be integrated into the development process, the 
Operationalizing Generative AI on Vertex AI using ML Ops56
September 2024smoother and more efficient the overall development becomes. Vertex AI provides cohesive 
experimentation and evaluation products permitting connected iterations over applications 
and models alongside their evaluations.
Experiment
The process of selecting, creating, and customizing machine learning (including large 
models) and its applications involves significant experimentation, collaboration, and iteration. 
Vertex AI seamlessly integrates experimentation and collaboration into the development 
lifecycle of AI/ML and gen AI models and applications. Its Workbench Instances77 provide 
Jupyter-based development environments for the entire data science workflow, connected 
to other Google Cloud services and with GitHub synchronization capabilities. Vertex Colab 
Enterprise78 accelerates the AI workflow by enabling collaborative coding and leveraging 
code completion and generation features.
Vertex AI also provides two tools for tracking and visualizing the output of many experiment 
cycles and training runs. Vertex AI Experiments79 facilitates meticulous tracking and 
analysis of model architectures, hyperparameters, and training environments. It logs 
experiments, artifacts, and metrics, enabling comparison and reproducibility across multiple 
runs. This comprehensive tracking permits data scientists to select the optimal model 
and architecture for their specific use case. Vertex AI TensorBoard80 complements the 
experimentation process by providing detailed visualizations for tracking, visualizing, and 
sharing ML experiments. It offers a range of visualizations, including loss and accuracy 
metrics tracking, model computational graph visualization, and weight and bias histograms, 
which - for example - can be used for tracking various metrics pertaining to training and 
evaluation of gen AI models with different prompting and tuning strategies. It also projects 
embeddings to lower-dimensional space, and displays image, text, and audio samples.
Operationalizing Generative AI on Vertex AI using ML Ops57
September 2024Evaluation
Vertex AI also provides a comprehensive set of evaluation tools for gen AI, from ground truth 
metrics to using LLMs as raters. 
For Ground Truth-based metrics, Automatic Metrics in Vertex AI81 lets you evaluate a model 
based on a defined task and “ground truth” dataset. For LLM-based evaluation, Automatic 
Side by Side (Auto SxS)  in Vertex AI82 uses a large model to evaluate the output of multiple 
models or configurations being tested, helping to augment human evaluation at scale. 
In addition to that, users can also leverage Rapid Evaluation API, which offers a set of pre-
built metrics for evaluating gen AI applications and relative SDK, integrated into the Vertex 
AI Python SDK for rapid and flexible, notebook-based, prototyping. To get started with Rapid 
Evaluation Vertex AI SDK see example in the official documentation.83
Predict: Vertex AI endpoints & monitoring
Once developed, a production gen AI application must be deployed, including all its model 
components. If the application uses any models that have been trained or adapted, those 
models need to be deployed to their own serving endpoints. You can serve any model in the 
Model Garden through Vertex AI Endpoints21,which acts as the gateway for deploying your 
trained machine learning models. They allow you to serve online predictions with low latency, 
manage access controls, and monitor model performance easily through Model Monitoring. 
Endpoints also offer scaling options to handle varying traffic demands, ensuring optimal user 
experience and reliability.
Operationalizing Generative AI on Vertex AI using ML Ops58
September 2024Along with the prediction service, Vertex AI offers the following features for all Google 
managed models:
• Citation checkers : Gen AI on Vertex performs Citation checks71. Citations are important 
for LLMs and gen AI for several reasons. Citing sources ensures proper acknowledgment 
of sources and prevents plagiarism and demonstrates transparency and accountability. 
Citing sources is essential for LLMs and gen AI also because they help identify, 
understand potential biases, and enable reproducibility and verification. For example in 
Google Cloud,84 the gen AI models are designed to produce original content, limiting the 
possibility of copying existing contents. If this happens, Google Cloud provides quotes for 
websites and code repositories. 
• Safety scores : Safety attributes are crucial for LLMs and gen AI to mitigate potential 
risks like bias, lack of explainability, and misuse. These attributes help detect and mitigate 
biased outputs and mitigate misuse, enabling these tools to be used responsibly. As 
LLMs and gen AI evolve, incorporating safety attributes will be increasingly essential for 
responsible and ethical use. For example, Google Cloud added safety scores in Vertex 
AI PaLM API and Vertex AI Gemini API85: content processed through the API is checked 
against a list of safety attributes, including "harmful categories" and sensitive topics. Each 
attribute has a confidence score between 0.0 and 1.0, indicating the likelihood of the 
input belonging to that category. These safety filters can be used in conjunction with all 
models: be it proprietary ones like Palm2 and Gemini or OSS ones like the ones available in 
Model garden.
• Watermarking : With AI-based tools becoming increasingly popular for creation of 
content, it’s very important to identify if an image has been created using AI. Vertex AI 
offers digital watermarking and verification for AI-generated images86 using the algorithm 
SynthID87 developed by Google DeepMind.
Operationalizing Generative AI on Vertex AI using ML Ops59
September 2024• Content moderation and bias detection : By using the Content moderation88 and Bias89 
detection tools on Vertex AI, you can add an extra layer of security on the responses 
of the LLMs to mitigate the risk that the model training and tuning may sway a model to 
generate outputs that aren’t fair or appropriate for the task.
Govern: Vertex AI Feature Store, Model Registry, 
and Dataplex
Addressing the multifaceted requirements of data and model lineage and governance in 
gen AI requires a comprehensive strategy that tackles both conventional challenges and 
novel regulatory or technical complexities associated with large models. By adopting robust 
governance, observability, and lineage practices in the development of gen AI solutions, 
organizations can ensure comprehensive tracking, iteration, and evolution of data. They 
can also track the large models used, prompt adaptations, tuning, and other artifacts. This 
facilitates reproducibility of results, transparency and understanding of generated content 
sources, troubleshooting, compliance enforcement, and enhanced reliability and security. 
These practices collectively enable the ethical and responsible development and deployment 
of gen AI solutions. This fosters internal and external trust and fairness in gen AI models and 
practices. Vertex AI and Google Cloud offer the following comprehensive suite of tools for 
unified lineage, governance and monitoring, effectively addressing these critical concerns.
In the context of governance and lineage, Vertex AI Feature Store74 offers:
• Track feature and embeddings versions and lineage, ensuring transparency
• Monitor feature (prompt) and embedding, response drift, and identify potential 
issues proactively
• Store feature formulas and discover relevant features or embeddings for different 
use cases
Operationalizing Generative AI on Vertex AI using ML Ops60
September 2024• Utilize feature selection algorithms to optimize model performance
• Consolidate and unify all machine learning data within a singular repository encompassing 
numerical data, categorical data, textual data, and embeddings representations
Vertex AI Model Registry12 serves as a centralized repository for comprehensive lifecycle 
management of both Google proprietary foundational and open-source Machine Learning 
models. This includes gen AI models in addition to predictive models. This unified platform 
enables registration, storage, and version control of diverse model types, including various 
iterations of tuning for large models. Vertex AI Model Registry seamlessly integrates with 
Vertex Pipelines,13 facilitating orchestration and management of training and tuning jobs 
while leveraging lineage capabilities for recording and documenting the lineage from 
datasets to models and associated artifacts. It also couples with Vertex AI Experiments79 
and Vertex AI Model Evaluation ,90 enabling performance monitoring and comparison of 
different model versions alongside their artifacts – all within a single interface. Furthermore, 
Vertex AI Model Registry bolsters observability by providing integrated configuration and 
access to Vertex AI Model Monitoring91 and logging functionalities. This enables proactive 
identification and mitigation of both training-serving skew and prediction drift, ensuring 
reliability and accuracy of deployed models. Users can directly assign desired model versions 
to endpoints for one-click deployment from Vertex Model Registry or leverage aliases for 
simplified deployment.
Google Cloud Dataplex14 provides an organization-wide lineage across product boundaries 
in Google Cloud. Within the domains of AI and gen AI (and more broadly across data analytics 
and AI/ML) Dataplex seamlessly integrates with BigQuery and Vertex AI. Dataplex facilitates 
the unification, management, discovery, and governance of both data and models. Through 
comprehensive data lineage, quality, and metadata management capabilities it provides 
actionable insights for comprehensive data and model understanding. This promotes 
compliance, facilitates data analysis, and guarantees the training of machine learning 
models on trusted data sources. This in turn leads to enhanced accuracy and reliability. This 
Operationalizing Generative AI on Vertex AI using ML Ops61
September 2024integration permits users across an organization to identify ‘champion models’ and ‘golden 
datasets and features’ across projects and regions in a secure way by adhering to identity 
access management (IAM)92 boundaries. In short, Dataplex encapsulates a framework within 
an organization that governs the interaction between people, processes and technology 
across all the products in Google Cloud.
Conclusion
The explosion of gen AI in the last several years introduced fundamental changes in the way 
AI applications are developed – but far from upending the MLOps discipline, these changes 
have only reinforced its basic principles and processes. As we have seen, the principles of 
MLOps that emphasize reliability, repeatability, and dependability in ML systems development 
are comfortably extended to include the innovations of gen AI. Some of the necessary 
changes are deeper and more far-reaching than others, but nowhere do we find any change 
that MLOps cannot accommodate.
As a result, many tools and processes built to support traditional MLOps can also support 
the requirements of gen AI. Vertex AI, for instance, is a powerful platform that can be used to 
build and deploy machine learning models and AI applications. It provides a comprehensive 
suite of functions for developing both Predictive and gen AI systems, encompassing data 
preparation, pre-trained APIs, AutoML capabilities, training and serving hardware,  advanced 
fine-tuning techniques and deployment tools, and a diverse selection of proprietary and 
open-source foundation models. It also offers evaluation methods, monitoring capabilities, 
and governance tools, all unified within a single platform to streamline the AI development 
lifecycle. It’s built on Google Cloud Platform, which provides a scalable, reliable, secure and 
compliant infrastructure for machine learning. It’s a good choice for organizations that want 
to build and deploy machine learning models and AI applications. 
Operationalizing Generative AI on Vertex AI using ML Ops62
September 2024The next few years will undoubtedly see gen AI extended in directions that today are 
unimaginable. Regardless of the direction these developments take, it will continue to 
be important to build on solid engineering processes that embody the basic principles 
of MLOps. These principles support the development of scalable, robust production AI 
applications today, and no doubt will continue to do so into the future.
Operationalizing Generative AI on Vertex AI using ML Ops63
September 2024Endnotes
1. Model Garden on Vertex AI. Available at: https://cloud.google.com/model-garden
2. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, 
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, 
Jeff Dean, William Fedus. 2022. Emergent Abilities of Large Language Models. Available at: https://arxiv.org/
pdf/2206.07682.pdf
3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich 
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2022. Retrieval-Augmented 
Generation for Knowledge-Intensive NLP Tasks. Available at: https://arxiv.org/pdf/2005.11401.pdf
4. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Department of 
Computer Science, Princeton University, Google Research, Brain team, REACT: SYNERGIZING REASONING AND 
ACTING IN LANGUAGE MODELS. Available at: https://arxiv.org/pdf/2210.03629.pdf
5. Grounding in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/
ground-language-models
6. Vertex Extensions. Connect models to APIs by using extensions. Available at:  https://cloud.google.com/
vertex-ai/docs/generative-ai/extensions/overview
7. Overview of Vertex AI Vector Search. Available at: https://cloud.google.com/vertex-ai/docs/vector-search/
overview
8. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/
introduction
9. LangChain. Get your LLM application from prototype to production. Available at: https://www.langchain.
com/
10. Introduction to the Vertex AI SDK for Python. Available at: https://cloud.google.com/vertex-ai/docs/python-
sdk/use-vertex-ai-python-sdk
11. Introduction to Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/start/introduction-unified-
platform
12. Introduction to Vertex AI Model Registry. Available at: https://cloud.google.com/vertex-ai/docs/model-
registry/introduction
Operationalizing Generative AI on Vertex AI using ML Ops64
September 202413. Introduction to Vertex AI Pipelines. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/
introduction
14. Dataplex. Available at: https://cloud.google.com/dataplex
15. BigQuery. Available at: https://cloud.google.com/bigquery?hl=en
16. PaLi-Gemma model card. Available at: https://ai.google.dev/gemma/docs/paligemma/model-card
17. Version Control. Available at: https://en.wikipedia.org/wiki/Version_control
18. Continuous integration. Available at: https://wikipedia.org/wiki/Continuous_integration
19. TFX is an end-to-end platform for deploying production ML pipelines. Available at: https://www.tensorflow.
org/tfx
20. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay 
Krishna, Chen-Yu Lee, Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with 
Less Training Data and Smaller Model Sizes. Available at: https://arxiv.org/pdf/2305.02301.pdf
21. Vertex Endpoints. Use private endpoints for online prediction. Available at: https://cloud.google.com/vertex-
ai/docs/predictions/using-private-endpoints
22. Tuan Duong Nguyen, Marthinus Christoffel du Plessis, Takafumi Kanamori, Masashi Sugiyama, 2014. 
Constrained Least-Squares Density-Difference Estimation. Available at: https://www.ms.k.u-tokyo.ac.jp/
sugi/2014/CLSDD.pdf
23. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola, 2012. A Kernel 
Two-Sample Test. Available at:  https://jmlr.csail.mit.edu/papers/v13/gretton12a.html
24. Oliver Cobb, Arnaud Van Looveren, 2022. Context-Aware Drift Detection. Available at:  https://arxiv.org/
pdf/2203.08644.pdf
25. Google Gemma Model. Available at: https://gemini.google.com/
26. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/
models/evaluate-models
27. Gemini Team, Google, 2023. Gemini: A Family of Highly Capable Multimodal Models. Available at: https://
storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
28. Anil, Dai et al., 2023. PaLM 2 Technical Report. Available at: https://arxiv.org/abs/2305.10403
Operationalizing Generative AI on Vertex AI using ML Ops65
September 202429. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed 
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David 
J Fleet, Mohammad Norouzi, 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language 
Understanding. Available at: https://arxiv.org/abs/2205.11487
30. Build the future of AI with Meta Llama 3. Available at: https://llama.meta.com/llama3
31. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, 
Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun 
Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, 
Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff 
Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei. 2022. Scaling Instruction-Finetuned 
Language Models. Available at: https://arxiv.org/abs/2210.11416
32. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018. BERT: Pre-training of Deep 
Bidirectional Transformers for Language Understanding. Available at: https://arxiv.org/abs/1810.04805
33. Stable Diffusion. Available at: https://github.com/CompVis/stable-diffusion
34. Vertex AI Function Calling. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/
function-calling
35. Mistral AI. Available at: https://mistral.ai/
36. Models available in Model Garden. Available at: https://cloud.google.com/vertex-ai/docs/start/explore-
models#available-models
37. Vertex AI Studio. Customize and deploy generative models. Available at: https://cloud.google.com/
generative-ai-studio
38. vLLM. Easy, fast, and cheap LLM serving for everyone. Available at: https://github.com/vllm-project/vllm
39. Overview of multimodal models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/
multimodal/overview
40. Text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text
41. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/
generative-ai/image/overview
42. Code models overview. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-
models-overview
Operationalizing Generative AI on Vertex AI using ML Ops66
September 202443. Convert speech to text. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/speech/
speech-to-text
44. Text-to-Speech AI. Available at: https://cloud.google.com/text-to-speech
45. Natural Language AI. Available at: https://cloud.google.com/natural-language
46. Translate docs, audio, and videos in real time with Google AI. Available at: https://cloud.google.com/
translate
47. Vision AI. Available at: https://cloud.google.com/vision
48. Git. Available at: https://git-scm.com/
49. CodeGemma model card. Available at: https://ai.google.dev/gemma/docs/codegemma/model_card
50. TII’s Falcon. Available at: https://falconllm.tii.ae/
51. Mistral AI. Available at:  https://mistral.ai/
52. Hugging Face, 2024. Vision Transformer (ViT) Documentation. Hugging Face, [online] Available at:   
https://huggingface.co/docs/transformers/en/model_doc/vit
53. Mingxing Tan, Quoc V. Le, 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. 
Available at: https://arxiv.org/abs/1905.11946
54. Anthropic Claude 3. Available at: https://www.anthropic.com/news/claude-3-haiku
55. Anthropic Claude 3 on Google Cloud Model Garden. Available at: https://cloud.google.com/blog/products/
ai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai
56. Vertex AI API. Available at: https://cloud.google.com/vertex-ai/docs/reference/rest
57. Vertex AI: Python SDK. Available at: https://cloud.google.com/python/docs/reference/aiplatform/latest/
vertexa i
58. Vertex AI: Node.js Client. Available at: https://cloud.google.com/nodejs/docs/reference/aiplatform/latest/
overview
59. Vertex AI for Java. Available at: https://cloud.google.com/java/docs/reference/google-cloud-aiplatform/
latest/overview
60. Customize and deploy generative models. Available at: https://cloud.google.com/generative-ai-studio
Operationalizing Generative AI on Vertex AI using ML Ops67
September 202461. Design text prompts. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-
prompts
62. Introduction to prompt design. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/
introduction-prompt-design
63. Supervised tuning. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-
models#supervised-tuning
64. RLHF model tuning. Available at:  https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-
models-rlhf
65. Vertex AI Distilation. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-
text-models
66. Create distilled text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/
distill-text-models
67. Pipeline Basics. Available at: https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/pipeline-
basics/
68. Build a pipeline. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline
69. Vertex AI Search extension. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/
extensions/vertex-ai-search
70. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/
introduction
71. Generative AI on Vertex AI, Citation Check. Available at: https://cloud.google.com/vertex-ai/generative-ai/
docs/learn/overview#citation_check
72. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar, 2020. 
Accelerating Large-Scale Inference with Anisotropic Vector Quantization. Available at: https://arxiv.org/
pdf/1908.10396.pdf
73. Get text embeddings. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/
get-text-embeddings
74. About Vertex AI Feature Store. Available at: https://cloud.google.com/vertex-ai/docs/featurestore/latest/
overview
Operationalizing Generative AI on Vertex AI using ML Ops68
September 202475. Google Cloud Vertex AI. Available at: https://python.langchain.com/docs/integrations/llms/google_vertex_
ai_palm
76. Generative AI - Language - LangChain. Available at: https://github.com/GoogleCloudPlatform/generative-
ai/tree/main/language/orchestration/langchain
77. Introduction to Vertex AI Workbench, Workbench Instances. Available at: https://cloud.google.com/vertex-
ai/docs/workbench/introduction
78. Introduction to Colab Enterprise. Available at: https://cloud.google.com/colab/docs/introduction
79. Introduction to Vertex AI Experiments. Available at: https://cloud.google.com/vertex-ai/docs/experiments/
intro-vertex-ai-experiments
80. Vertex AI TensorBoard Introduction to Vertex AI TensorBoard. Available at https://cloud.google.com/vertex-
ai/docs/experiments/tensorboard-introduction
81. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/
models/evaluate-models
82. Perform automatic side-by-side evaluation. Available at: https://cloud.google.com/vertex-ai/docs/
generative-ai/models/side-by-side-eval
83. Rapid Evaluation Vertex AI. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/
rapid-evaluation
84. Citation metadata. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-
ai#citation_metadata
85. Responsible AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-
ai#filters-palm-api
86. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/
generative-ai/image/overview
87. SynthID. Identifying AI-generated content with SynthID. Available at: https://deepmind.google/technologies/
synthid/
88. Moderate text. Available at: https://cloud.google.com/natural-language/docs/moderating-text
89. Model bias metrics for Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/model-
bias-metrics
Operationalizing Generative AI on Vertex AI using ML Ops69
September 202490. Model evaluation in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/
introduction
91. Introduction to Vertex AI Model Monitoring. Available at: https://cloud.google.com/vertex-ai/docs/model-
monitoring/overview
92. Identity and Access Management (IAM). Available at: https://cloud.google.com/iam/docs
Agents
Authors: Julia Wiesinger, Patrick Marlow  
and Vladimir Vuskovic

Agents2
September 2024
Acknowledgements
Reviewers and Contributors
Evan Huang
Emily Xue
Olcan Sercinoglu
Sebastian Riedel
Satinder Baveja
Antonio Gulli
Anant Nawalgaria
Curators and Editors
Antonio Gulli
Anant Nawalgaria
Grace Mollison 
Technical Writer
Joey Haymaker
Designer
Michael Lanning 
Introduction  4
What is an agent?  5
 The model  6
 The tools  7
 The orchestration layer  7
 Agents vs. models  8
 Cognitive architectures: How agents operate  8
Tools: Our keys to the outside world  12
 Extensions  13
  Sample Extensions  15
 Functions  18
  Use cases  21
  Function sample code  24
 Data stores  27
  Implementation and application  28
 Tools recap  32
Enhancing model performance with targeted learning  33
Agent quick start with LangChain  35
Production applications with Vertex AI agents  38
Summary  40
Endnotes  42Table of contents
Agents4
September 2024Introduction
Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools 
- like books, Google Search, or a calculator - to supplement their prior knowledge before 
arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools 
to access real-time information or suggest a real-world action. For example, a model can 
leverage a database retrieval tool to access specific information, like a customer's purchase 
history, so it can generate tailored shopping recommendations. Alternatively, based on a 
user's query, a model can make various API calls to send an email response to a colleague 
or complete a financial transaction on your behalf. To do so, the model must not only have 
access to a set of external tools, it needs the ability to plan and execute any task in a self-
directed fashion. This combination of reasoning, logic, and access to external information 
that are all connected to a Generative AI model invokes the concept of an agent, or a 
program that extends beyond the standalone capabilities of a Generative AI model. This 
whitepaper dives into all these and associated aspects in more detail.This combination of reasoning, 
logic, and access to external 
information that are all connected 
to a Generative AI model invokes 
the concept of an agent.
Agents5
September 2024What is an agent?
In its most fundamental form, a Generative AI agent can be defined as an application that 
attempts to achieve a goal by observing the world and acting upon it using the tools that it 
has at its disposal. Agents are autonomous and can act independently of human intervention, 
especially when provided with proper goals or objectives they are meant to achieve. Agents 
can also be proactive in their approach to reaching their goals. Even in the absence of 
explicit instruction sets from a human, an agent can reason about what it should do next to 
achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this 
whitepaper focuses on the specific types of agents that Generative AI models are capable of 
building at the time of publication.
In order to understand the inner workings of an agent, let’s first introduce the foundational 
components that drive the agent’s behavior, actions, and decision making. The combination 
of these components can be described as a cognitive architecture, and there are many 
such architectures that can be achieved by the mixing and matching of these components. 
Focusing on the core functionalities, there are three essential components in an agent’s 
cognitive architecture as shown in Figure 1.
Agents6
September 2024
Figure 1. General agent architecture and components
The model
In the scope of an agent, a model refers to the language model (LM) that will be utilized as 
the centralized decision maker for agent processes. The model used by an agent can be one 
or multiple LM’s of any size (small / large) that are capable of following instruction based 
reasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models 
can be general purpose, multimodal or fine-tuned based on the needs of your specific agent 
architecture. For best production results, you should leverage a model that best fits your 
desired end application and, ideally, has been trained on data signatures associated with the 
tools that you plan to use in the cognitive architecture. It’s important to note that the model is 
typically not trained with the specific configuration settings (i.e. tool choices, orchestration/
reasoning setup) of the agent. However, it’s possible to further refine the model for the 
agent’s tasks by providing it with examples that showcase the agent’s capabilities, including 
instances of the agent using specific tools or reasoning steps in various contexts.
Agents7
September 2024The tools
Foundational models, despite their impressive text and image generation, remain constrained 
by their inability to interact with the outside world. Tools bridge this gap, empowering agents 
to interact with external data and services while unlocking a wider range of actions beyond 
that of the underlying model alone. Tools can take a variety of forms and have varying 
depths of complexity, but typically align with common web API methods like GET, POST, 
PATCH, and DELETE. For example, a tool could update customer information in a database 
or fetch weather data to influence a travel recommendation that the agent is providing to 
the user. With tools, agents can access and process real-world information. This empowers 
them to support more specialized systems like retrieval augmented generation (RAG), 
which significantly extends an agent’s capabilities beyond what the foundational model can 
achieve on its own. We’ll discuss tools in more detail below, but the most important thing 
to understand is that tools bridge the gap between the agent’s internal capabilities and the 
external world, unlocking a broader range of possibilities.
The orchestration layer
The orchestration layer describes a cyclical process that governs how the agent takes in 
information, performs some internal reasoning, and uses that reasoning to inform its next 
action or decision. In general, this loop will continue until an agent has reached its goal or a 
stopping point. The complexity of the orchestration layer can vary greatly depending on the 
agent and task it’s performing. Some loops can be simple calculations with decision rules, 
while others may contain chained logic, involve additional machine learning algorithms, or 
implement other probabilistic reasoning techniques. We’ll discuss more about the detailed 
implementation of the agent orchestration layers in the cognitive architecture section.
Agents8
September 2024Agents vs. models
To gain a clearer understanding of the distinction between agents and models, consider the 
following chart:
Models Agents
Knowledge is limited to what is available in their 
training data.Knowledge is extended through the connection 
with external systems via tools
Single inference / prediction based on the 
user query. Unless explicitly implemented for 
the model, there is no management of session 
history or continuous context. (i.e. chat history)Managed session history (i.e. chat history) to 
allow for multi turn inference / prediction based 
on user queries and decisions made in the 
orchestration layer. In this context, a ‘turn’ is 
defined as an interaction between the interacting 
system and the agent. (i.e. 1 incoming event/
query and 1 agent response)
No native tool implementation. Tools are natively implemented in agent 
architecture.
No native logic layer implemented. Users can 
form prompts as simple questions or use 
reasoning frameworks (CoT, ReAct, etc.) to 
form complex prompts to guide the model in 
prediction.Native cognitive architecture that uses reasoning 
frameworks like CoT, ReAct, or other pre-built 
agent frameworks like LangChain.
Cognitive architectures: How agents operate 
Imagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant 
patrons which involves some cycle of planning, execution, and adjustment. 
Agents9
September 2024• They gather information, like the patron’s order and what ingredients are in the pantry 
and refrigerator.
• They perform some internal reasoning about what dishes and flavor profiles they can 
create based on the information they have just gathered.
• They take action to create the dish: chopping vegetables, blending spices, searing meat.
At each stage in the process the chef makes adjustments as needed, refining their plan as 
ingredients are depleted or customer feedback is received, and uses the set of previous 
outcomes to determine the next plan of action. This cycle of information intake, planning, 
executing, and adjusting describes a unique cognitive architecture that the chef employs to 
reach their goal.
Just like the chef, agents can use cognitive architectures to reach their end goals by 
iteratively processing information, making informed decisions, and refining next actions 
based on previous outputs. At the core of agent cognitive architectures lies the orchestration 
layer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly 
evolving field of prompt engineering and associated frameworks to guide reasoning and 
planning, enabling the agent to interact more effectively with its environment and complete 
tasks. Research in the area of prompt engineering frameworks and task planning for 
language models is rapidly evolving, yielding a variety of promising approaches. While not an 
exhaustive list, these are a few of the most popular frameworks and reasoning techniques 
available at the time of this publication:
• ReAct, a prompt engineering framework that provides a thought process strategy for 
language models to Reason and take action on a user query, with or without in-context 
examples. ReAct prompting has shown to outperform several SOTA baselines and improve 
human interoperability and trustworthiness of LLMs.
Agents10
September 2024• Chain-of-Thought (CoT) , a prompt engineering framework that enables reasoning 
capabilities through intermediate steps. There are various sub-techniques of CoT including 
self-consistency, active-prompt, and multimodal CoT that each have strengths and 
weaknesses depending on the specific application.
• Tree-of-thoughts (ToT) ,, a prompt engineering framework that is well suited for 
exploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting 
and allows the model to explore various thought chains that serve as intermediate steps 
for general problem solving with language models.
Agents can utilize one of the above reasoning techniques, or many other techniques, to 
choose the next best action for the given user request. For example, let’s consider an agent 
that is programmed to use the ReAct  framework to choose the correct actions and tools for 
the user query. The sequence of events might go something like this:
1. User sends query to the agent
2. Agent begins the ReAct sequence
3. The agent provides a prompt to the model, asking it to generate one of the next ReAct 
steps and its corresponding output:
a. Question:  The input question from the user query, provided with the prompt
b. Thought:  The model’s thoughts about what it should do next
c. Action: The model’s decision on what action to take next
i. This is where tool choice can occur
ii. For example, an action could be one of [Flights, Search, Code, None], where the first 
3 represent a known tool that the model can choose, and the last represents “no 
tool choice”
Agents11
September 2024d. Action input:  The model’s decision on what inputs to provide to the tool (if any)
e. Observation:  The result of the action / action input sequence
i. This thought / action / action input / observation could repeat N-times  as needed
f. Final answer:  The model’s final answer to provide to the original user query
4. The ReAct loop concludes and a final answer is provided back to the user
Figure 2. Example agent with ReAct reasoning in the orchestration layer
As shown in Figure 2, the model, tools, and agent configuration work together to provide 
a grounded, concise response back to the user based on the user’s original query. While 
the model could have guessed at an answer (hallucinated) based on its prior knowledge, 
it instead used a tool (Flights) to search for real-time external information. This additional 
information was provided to the model, allowing it to make a more informed decision based 
on real factual data and to summarize this information back to the user. 
Agents12
September 2024In summary, the quality of agent responses can be tied directly to the model’s ability to 
reason and act about these various tasks, including the ability to select the right tools, and 
how well that tools has been defined. Like a chef crafting a dish with fresh ingredients and 
attentive to customer feedback, agents rely on sound reasoning and reliable information to 
deliver optimal results. In the next section, we’ll dive into the various ways agents connect 
with fresh data. 
Tools: Our keys to the outside world
While language models excel at processing information, they lack the ability to directly 
perceive and influence the real world. This limits their usefulness in situations requiring 
interaction with external systems or data. This means that, in a sense, a language model 
is only as good as what it has learned from its training data. But regardless of how much 
data we throw at a model, they still lack the fundamental ability to interact with the outside 
world. So how can we empower our models to have real-time, context-aware interaction with 
external systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this 
critical capability to the model.
While they go by many names, tools are what create a link between our foundational models 
and the outside world. This link to external systems and data allows our agent to perform a 
wider variety of tasks and do so with more accuracy and reliability. For instance, tools can 
enable agents to adjust smart home settings, update calendars, fetch user information from 
a database, or send emails based on a specific set of instructions.
As of the date of this publication, there are three primary tool types that Google models are 
able to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools, 
we unlock a vast potential for them to not only understand the world but also act upon it, 
opening doors to a myriad of new applications and possibilities. 
Agents13
September 2024Extensions 
The easiest way to understand Extensions is to think of them as bridging the gap between 
an API and an agent in a standardized way, allowing agents to seamlessly execute APIs 
regardless of their underlying implementation. Let’s say that you’ve built an agent with a goal 
of helping users book flights. You know that you want to use the Google Flights API to retrieve 
flight information, but you’re not sure how you’re going to get your agent to make calls to this 
API endpoint.
Figure 3. How do Agents interact with External APIs?
One approach could be to implement custom code that would take the incoming user query, 
parse the query for relevant information, then make the API call. For example, in a flight 
booking use case a user might state “I want to book a flight from Austin to Zurich .” In this 
scenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant 
entities from the user query before attempting to make the API call. But what happens if the 
user says “I want to book a flight  to Zurich ” and never provides a departure city? The API call 
would fail without the required data and more code would need to be implemented in order 
to catch edge and corner cases like this. This approach is not scalable and could easily break 
in any scenario that falls outside of the implemented custom code.
Agents14
September 2024A more resilient approach would be to use an Extension. An Extension bridges the gap 
between an agent and an API by:
1. Teaching the agent how to use the API endpoint using examples.
2. Teaching the agent what arguments or parameters are needed to successfully call the 
API endpoint.
Figure 4. Extensions connect Agents to External APIs
Extensions can be crafted independently of the agent, but should be provided as part of the 
agent’s configuration. The agent uses the model and examples at run time to decide which 
Extension, if any, would be suitable for solving the user’s query. This highlights a key strength 
of Extensions, their built-in example types , that allow the agent to dynamically select the 
most appropriate Extension for the task. 
 
Figure 5. 1-to-many relationship between Agents, Extensions and APIs
Agents15
September 2024Think of this the same way that a software developer decides which API endpoints to use 
while solving and solutioning for a user’s problem. If the user wants to book a flight, the 
developer might use the Google Flights API. If the user wants to know where the nearest 
coffee shop is relative to their location, the developer might use the Google Maps API. In 
this same way, the agent / model stack uses a set of known Extensions to decide which one 
will be the best fit for the user’s query. If you’d like to see Extensions in action, you can try 
them out on the Gemini application by going to Settings > Extensions and then enabling any 
you would like to test. For example, you could enable the Google Flights extension then ask 
Gemini “Show me flights from Austin to Zurich leaving next Friday.”
Sample Extensions 
To simplify the usage of Extensions, Google provides some out of the box extensions that 
can be quickly imported into your project and used with minimal configurations. For example, 
the Code Interpreter extension in Snippet 1 allows you to generate and run Python code from 
a natural language description.
Agents16
September 2024Python
import vertexai
import pprint
PROJECT_ID = "YOUR_PROJECT_ID"
REGION = "us-central1"
vertexai.init(project=PROJECT_ID, location=REGION)
from vertexai.preview.extensions import Extension
extension_code_interpreter = Extension.from_hub("code_interpreter")
CODE_QUERY = """Write a python method to invert a binary tree in O(n) time."""
response = extension_code_interpreter.execute(
  operation_id = "generate_and_execute" ,
  operation_params = { "query": CODE_QUERY}
  )
print("Generated Code:" )
pprint.pprint({response[ 'generated_code' ]})
# The above snippet will generate the following code.
```
Generated Code:
class TreeNode:
  def __init__(self, val= 0, left=None, right= None):
    self.val = val
    self.left = left
    self.right = right
Continues next page...
Agents17
September 2024Python
def invert_binary_tree(root):
    """
    Inverts a binary tree.
    Args:
        root: The root of the binary tree.
    Returns:
        The root of the inverted binary tree.
    """
    if not root:
        return None
    # Swap the left and right children recursively
    root.left, root.right = 
invert_binary_tree(root.right), invert_binary_tree(root.left)
    return root
# Example usage:
# Construct a sample binary tree
root = TreeNode(4)
root.left = TreeNode(2)
root.right = TreeNode(7)
root.left.left = TreeNode(1)
root.left.right = TreeNode(3)
root.right.left = TreeNode(6)
root.right.right = TreeNode(9)
# Invert the binary tree
inverted_root = invert_binary_tree(root)
```
Snippet 1. Code Interpreter Extension can generate and run Python code
Agents18
September 2024To summarize, Extensions provide a way for agents to perceive, interact, and influence the 
outside world in a myriad of ways. The selection and invocation of these Extensions is guided 
by the use of Examples, all of which are defined as part of the Extension configuration.
Functions 
In the world of software engineering, functions are defined as self-contained modules 
of code that accomplish a specific task and can be reused as needed. When a software 
developer is writing a program, they will often create many functions to do various tasks. 
They will also define the logic for when to call function_a versus function_b, as well as the 
expected inputs and outputs.
Functions work very similarly in the world of agents, but we can replace the software 
developer with a model. A model can take a set of known functions and decide when to use 
each Function and what arguments the Function needs based on its specification. Functions 
differ from Extensions in a few ways, most notably:
1. A model outputs a Function and its arguments, but doesn’t make a live API call.
2. Functions are executed on the client-side , while Extensions are executed on 
the agent-side .
Using our Google Flights example again, a simple setup for functions might look like the 
example in Figure 7.
Agents19
September 2024
Figure 7. How do functions interact with external APIs?
Note that the main difference here is that neither the Function nor the agent interact directly 
with the Google Flights API. So how does the API call actually happen?
With functions, the logic and execution of calling the actual API endpoint is offloaded away 
from the agent and back to the client-side application as seen in Figure 8 and Figure 9 below. 
This offers the developer more granular control over the flow of data in the application. There 
are many reasons why a Developer might choose to use functions over Extensions, but a few 
common use cases are:
• API calls need to be made at another layer of the application stack, outside of the direct 
agent architecture flow (e.g. a middleware system, a front end framework, etc.)
• Security or Authentication restrictions that prevent the agent from calling an API directly 
(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)
• Timing or order-of-operations constraints that prevent the agent from making API calls in 
real-time. (i.e. batch operations, human-in-the-loop review, etc.)
Agents20
September 2024• Additional data transformation logic needs to be applied to the API Response that the 
agent cannot perform. For example, consider an API endpoint that doesn’t provide a 
filtering mechanism for limiting the number of results returned. Using Functions on the 
client-side provides the developer additional opportunities to make these transformations. 
• The developer wants to iterate on agent development without deploying additional 
infrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)
While the difference in internal architecture between the two approaches is subtle as seen in 
Figure 8, the additional control and decoupled dependency on external infrastructure makes 
Function Calling an appealing option for the Developer.
Figure 8. Delineating client vs. agent side control for extensions and function calling
Agents21
September 2024Use cases
A model can be used to invoke functions in order to handle complex, client-side execution 
flows for the end user, where the agent Developer might not want the language model to 
manage the API execution (as is the case with Extensions). Let’s consider the following 
example where an agent is being trained as a travel concierge to interact with users that want 
to book vacation trips. The goal is to get the agent to produce a list of cities that we can use 
in our middleware application to download images, data, etc. for the user’s trip planning. A 
user might say something like:
I’d like to take a ski trip with my family but I’m not sure where to go.
In a typical prompt to the model, the output might look like the following:
Sure, here’s a list of cities that you can consider for family ski trips:
• Crested Butte, Colorado, USA
• Whistler, BC, Canada
• Zermatt, Switzerland
While the above output contains the data that we need (city names), the format isn’t ideal 
for parsing. With Function Calling, we can teach a model to format this output in a structured 
style (like JSON) that’s more convenient for another system to parse. Given the same input 
prompt from the user, an example JSON output from a Function might look like Snippet 
5 instead.
Agents22
September 2024Unset
function_call {
  name: "display_cities"
  args: {
    "cities": ["Crested Butte", "Whistler", "Zermatt"],
    "preferences": "skiing"
    }
}
Snippet 5. Sample Function Call payload for displaying a list of cities and user preferences
This JSON payload is generated by the model, and then sent to our Client-side server to do 
whatever we would like to do with it. In this specific case, we’ll call the Google Places API to 
take the cities provided by the model and look up Images, then provide them as formatted 
rich content back to our User. Consider this sequence diagram in Figure 9 showing the above 
interaction in step by step detail.
Agents23
September 2024
 
Figure 9. Sequence diagram showing the lifecycle of a Function Call
The result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with 
the parameters required for the Client side UI to make the call to the Google Places API. The 
Client side UI manages the actual API call using the parameters provided by the model in the 
returned Function. This is just one use case for Function Calling, but there are many other 
scenarios to consider like:
• You want a language model to suggest a function that you can use in your code, but you 
don't want to include credentials in your code. Because function calling doesn't run the 
function, you don't need to include credentials in your code with the function information.
Agents24
September 2024• You are running asynchronous operations that can take more than a few seconds. These 
scenarios work well with function calling because it's an asynchronous operation.
• You want to run functions on a device that's different from the system producing the 
function calls and their arguments.
One key thing to remember about functions is that they are meant to offer the developer 
much more control over not only the execution of API calls, but also the entire flow of data 
in the application as a whole. In the example in Figure 9, the developer chose to not return 
API information back to the agent as it was not pertinent for future actions the agent might 
take. However, based on the architecture of the application, it may make sense to return the 
external API call data to the agent in order to influence future reasoning, logic, and action 
choices. Ultimately, it is up to the application developer to choose what is right for the 
specific application.
Function sample code
To achieve the above output from our ski vacation scenario, let’s build out each of the 
components to make this work with our gemini-1.5-flash-001 model. 
First, we’ll define our display_cities function as a simple Python method.
Agents25
September 2024Python
def display_cities (cities: list[str], preferences: Optional [str] = None):
 """Provides a list of cities based on the user's search query and preferences.
 Args:
  preferences (str): The user's preferences for the search, like skiing,
  beach, restaurants, bbq, etc.
  cities (list[str]): The list of cities being recommended to the user.
 Returns:
  list[str]: The list of cities being recommended to the user.
 """
 return  cities
 
Snippet 6. Sample python method for a function that will display a list of cities.
Next, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to 
the model. Executing the code below would result in the output as seen at the bottom of the 
code snippet.
Agents26
September 2024Python
from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration
model = GenerativeModel ("gemini-1.5-flash-001 ")
display_cities_function = FunctionDeclaration.from_func (display_cities )
tool = Tool (function_declarations =[display_cities_function ])
message = "I’d like to take a ski trip with my family but I’m not sure where 
to go."
res = model.generate_content (message, tools=[tool])
print(f"Function Name:  {res.candidates[ 0].content.parts[ 0].function_call.name }")
print(f"Function Args:  {res.candidates[ 0].content.parts[ 0].function_call.args }")
> Function Name: display_cities
> Function Args: {'preferences': 'skiing', 'cities': ['Aspen', 'Vail', 
'Park City']}
Snippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place
In summary, functions offer a straightforward framework that empowers application 
developers with fine-grained control over data flow and system execution, while effectively 
leveraging the agent/model for critical input generation. Developers can selectively choose 
whether to keep the agent “in the loop” by returning external data, or omit it based on 
specific application architecture requirements.
Agents27
September 2024Data stores
Imagine a language model as a vast library of books, containing its training data. But unlike 
a library that continuously acquires new volumes, this one remains static, holding only the 
knowledge it was initially trained on. This presents a challenge, as real-world knowledge is 
constantly evolving. Data Stores address this limitation by providing access to more dynamic 
and up-to-date information, and ensuring a model’s responses remain grounded in factuality 
and relevance.
Consider a common scenario where a developer might need to provide a small amount of 
additional data to a model, perhaps in the form of spreadsheets or PDFs.
Figure 10. How can Agents interact with structured and unstructured data?
Agents28
September 2024Data Stores  allow developers to provide additional data in its original format to an agent, 
eliminating the need for time-consuming data transformations, model retraining, or fine-
tuning. The Data Store converts the incoming document into a set of vector database 
embeddings  that the agent can use to extract the information it needs to supplement its next 
action or response to the user.
Figure 11. Data Stores connect Agents to new real-time data sources of various types.
Implementation and application
In the context of Generative AI agents, Data Stores are typically implemented as a vector 
database  that the developer wants the agent to have access to at runtime. While we won’t 
cover vector databases in depth here, the key point to understand is that they store data 
in the form of vector embeddings, a type of high-dimensional vector or mathematical 
representation of the data provided. One of the most prolific examples of Data Store usage 
with language models in recent times has been the implementation of Retrieval Augmented 
Agents29
September 2024Generation (RAG) based applications. These applications seek to extend the breadth and 
depth of a model’s knowledge beyond the foundational training data by giving the model 
access to data in various formats like:
• Website content
• Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.
• Unstructured Data in formats like HTML, PDF, TXT, etc.
Figure 12. 1-to-many relationship between agents and data stores, which can represent various types of 
pre-indexed data
The underlying process for each user request and agent response loop is generally modeled 
as seen in Figure 13.
1. A user query is sent to an embedding model to generate embeddings for the query
2. The query embeddings are then matched against the contents of the vector database 
using a matching algorithm like SCaNN
3. The matched content is retrieved from the vector database in text format and sent back to 
the agent
4. The agent receives both the user query and retrieved content, then formulates a response 
or action
Agents30
September 20245. A final response is sent to the user
Figure 13. The lifecycle of a user request and agent response in a RAG based application
The end result is an application that allows the agent to match a user’s query to a known data 
store through vector search, retrieve the original content, and provide it to the orchestration 
layer and model for further processing. The next action might be to provide a final answer to 
the user, or perform an additional vector search to further refine the results.
A sample interaction with an agent that implements RAG with ReAct reasoning/planning  can 
be seen in Figure 14.
Agents31
September 2024
Figure 14. Sample RAG based application w/ ReAct reasoning/planning
Agents32
September 2024Tools recap
To summarize, extensions, functions and data stores make up a few different tool types 
available for agents to use at runtime. Each has their own purpose and they can be used 
together or independently at the discretion of the agent developer. 
Extensions Function Calling Data Stores
Execution Agent-Side Execution Client-Side Execution Agent-Side Execution
Use Case • Developer wants 
agent to control 
interactions with the 
API endpoints
• Useful when 
leveraging native pre-
built Extensions (i.e. 
Vertex Search, Code 
Interpreter, etc.)
• Multi-hop planning 
and API calling 
(i.e. the next agent 
action depends on 
the outputs of the 
previous action / 
API call)• Security or 
Authentication 
restrictions prevent the 
agent from calling an 
API directly
• Timing constraints or 
order-of-operations 
constraints that 
prevent the agent 
from making API calls 
in real-time. (i.e. batch 
operations, human-in-
the-loop review, etc.)
• API that is not exposed 
to the internet, or 
non-accessible by 
Google systemsDeveloper wants to 
implement Retrieval 
Augmented Generation 
(RAG) with any of the 
following data types:
• Website Content from 
pre-indexed domains 
and URLs
• Structured Data in 
formats like PDF, 
Word Docs, CSV, 
Spreadsheets, etc.
• Relational / Non- 
Relational Databases
• Unstructured Data in 
formats like HTML, PDF, 
TXT, etc.
 
Agents33
September 2024Enhancing model performance with 
targeted learning
A crucial aspect of using models effectively is their ability to choose the right tools when 
generating output, especially when using tools at scale in production. While general training 
helps models develop this skill, real-world scenarios often require knowledge beyond the 
training data. Imagine this as the difference between basic cooking skills and mastering 
a specific cuisine. Both require foundational cooking knowledge, but the latter demands 
targeted learning for more nuanced results.
To help the model gain access to this type of specific knowledge, several approaches exist:
• In-context learning:  This method provides a generalized model with a prompt, tools, and 
few-shot examples at inference time which allows it to learn ‘on the fly' how and when to 
use those tools for a specific task. The ReAct framework is an example of this approach in 
natural language.
• Retrieval-based in-context learning:  This technique dynamically populates the model 
prompt with the most relevant information, tools, and associated examples by retrieving 
them from external memory. An example of this would be the ‘Example Store’ in Vertex AI 
extensions or the data stores RAG based architecture mentioned previously.
• Fine-tuning based learning: This method involves training a model using a larger dataset 
of specific examples prior to inference. This helps the model understand when and how to 
apply certain tools prior to receiving any user queries. 
To provide additional insights on each of the targeted learning approaches, let’s revisit our 
cooking analogy.
Agents34
September 2024• Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant 
tools) and some example dishes (few-shot examples) from a customer. Based on this 
limited information and the chef’s general knowledge of cooking, they will need to figure 
out how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the 
customer’s preferences. This is in-context learning.
• Now let’s imagine our chef in a kitchen that has a well-stocked pantry (external data 
stores) filled with various ingredients and cookbooks (examples and tools). The chef is now 
able to dynamically choose ingredients and cookbooks from the pantry and better align 
to the customer’s recipe and preferences. This allows the chef to create a more informed 
and refined dish leveraging both existing and new knowledge . This is retrieval-based 
in-context learning .
• Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of 
cuisines (pre-training on a larger dataset of specific examples). This allows the chef to 
approach future unseen customer recipes with deeper understanding. This approach is 
perfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-
tuning based learning . 
Each of these approaches offers unique advantages and disadvantages in terms of speed, 
cost, and latency. However, by combining these techniques in an agent framework, we can 
leverage the various strengths and minimize their weaknesses, allowing for a more robust and 
adaptable solution.
Agents35
September 2024Agent quick start with LangChain
In order to provide a real-world executable example of an agent in action, we’ll build a quick 
prototype with the LangChain and LangGraph libraries. These popular open source libraries 
allow users to build customer agents by “chaining” together sequences of logic, reasoning, 
and tool calls to answer a user’s query. We’ll use our gemini-1.5-flash-001  model and 
some simple tools to answer a multi-stage query from the user as seen in Snippet 8.
The tools we are using are the SerpAPI (for Google Search) and the Google Places API. After 
executing our program in Snippet 8, you can see the sample output in Snippet 9.
Agents36
September 2024Python
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_community.utilities import SerpAPIWrapper
from langchain_community.tools import GooglePlacesTool
os.environ ["SERPAPI_API_KEY" ] = "XXXXX"
os.environ ["GPLACES_API_KEY" ] = "XXXXX"
@tool
def search(query: str):
 """Use the SerpAPI to run a Google Search."" "
 search = SerpAPIWrapper ()
 return  search.run (query)
@tool
def places(query: str):
 """Use the Google Places API to run a Google Places Query."""
 places = GooglePlacesTool ()
 return  places.run (query)
model = ChatVertexAI (model="gemini-1.5-flash-001" )
tools = [search, places ]
query = "Who did the Texas Longhorns play in football last week? What is the 
address of the other team's stadium?"
agent = create_react_agent (model, tools )
input = {"messages" : [("human", query)]}
for s in agent.stream (input, stream_mode= "values" ):
 message = s ["messages" ][-1]
 if isinstance (message, tuple):
  print (message)
 else:
  message.pretty_print ()
Snippet 8. Sample LangChain and LangGraph based agent with tools
Agents37
September 2024Unset
=============================== Human Message ================================
Who did the Texas Longhorns play in football last week? What is the address 
of the other team's stadium?
================================= Ai Message =================================
Tool Calls: search
Args:
 query: Texas Longhorns football schedule
================================ Tool Message ================================
Name: search
{...Results: "NCAA Division I Football, Georgia, Date..."}
================================= Ai Message =================================
The Texas Longhorns played the Georgia Bulldogs last week.
Tool Calls: places
Args:
 query: Georgia Bulldogs stadium
================================ Tool Message ================================
Name: places
{...Sanford Stadium Address: 100 Sanford...}
================================= Ai Message =================================
The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 
30602, USA.
Snippet 9. Output from our program in Snippet 8
While this is a fairly simple agent example, it demonstrates the foundational components 
of Model, Orchestration, and tools all working together to achieve a specific goal. In the 
final section, we’ll explore how these components come together in Google-scale managed 
products like Vertex AI agents and Generative Playbooks.
Agents38
September 2024Production applications with Vertex 
AI agents
While this whitepaper explored the core components of agents, building production-grade 
applications requires integrating them with additional tools like user interfaces, evaluation 
frameworks, and continuous improvement mechanisms. Google’s Vertex AI  platform 
simplifies this process by offering a fully managed environment with all the fundamental 
elements covered earlier. Using a natural language interface , developers can rapidly 
define crucial elements of their agents - goals, task instructions, tools, sub-agents for task 
delegation, and examples - to easily construct the desired system behavior. In addition, the 
platform comes with a set of development tools that allow for testing, evaluation, measuring 
agent performance, debugging, and improving the overall quality of developed agents. This 
allows developers to focus on building and refining their agents while the complexities of 
infrastructure, deployment and maintenance are managed by the platform itself. 
In Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex 
AI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex 
Function Calling and Vertex Example Store to name a few. The architecture includes many of 
the various components necessary for a production ready application.
Agents39
September 2024
Figure 15. Sample end-to-end agent architecture built on Vertex AI platform
You can try a sample of this prebuilt agent architecture from our official documentation.
Agents40
September 2024Summary
In this whitepaper we’ve discussed the foundational building blocks of Generative AI 
agents, their compositions, and effective ways to implement them in the form of cognitive 
architectures. Some key takeaways from this whitepaper include:
1. Agents extend the capabilities of language models by leveraging tools to access real-
time information, suggest real-world actions, and plan and execute complex tasks 
autonomously. agents can leverage one or more language models to decide when and 
how to transition through states and use external tools to complete any number of 
complex tasks that would be difficult or impossible for the model to complete on its own.
2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that 
structures reasoning, planning, decision-making and guides its actions. Various reasoning 
techniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework 
for the orchestration layer to take in information, perform internal reasoning, and generate 
informed decisions or responses. 
3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside 
world for agents, allowing them to interact with external systems and access knowledge 
beyond their training data. Extensions provide a bridge between agents and external APIs, 
enabling the execution of API calls and retrieval of real-time information. functions provide 
a more nuanced control for the developer through the division of labor, allowing agents 
to generate Function parameters which can be executed client-side. Data Stores provide 
agents with access to structured or unstructured data, enabling data-driven applications.
The future of agents holds exciting advancements and we’ve only begun to scratch the 
surface of what is possible. As tools become more sophisticated and reasoning capabilities 
are enhanced, agents will be empowered to solve increasingly complex problems. 
Furthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By 
Agents41
September 2024combining specialized agents - each excelling in a particular domain or task - we can create 
a ‘mixture of agent experts’ approach, capable of delivering exceptional results across 
various industries and problem areas.
It’s important to remember that building complex agent architectures demands an iterative 
approach. Experimentation and refinement are key to finding solutions for specific business 
cases and organizational needs. No two agents are created alike due to the generative nature 
of the foundational models that underpin their architecture. However, by harnessing the 
strengths of each of these foundational components, we can create impactful applications 
that extend the capabilities of language models and drive real-world value.
Agents42
September 2024Endnotes
1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'. Available at:  
https://arxiv.org/abs/2210.03629
2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. 
Available at: https://arxiv.org/pdf/2201.11903.pdf .
3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  
Available at: https://arxiv.org/abs/2203.11171 .
4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  
https://arxiv.org/pdf/2302.12246.pdf .
5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  
https://arxiv.org/abs/2302.00923 .
6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  
https://arxiv.org/abs/2305.10601 .
7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  
https://arxiv.org/abs/2305.08291 .
8. Google. 'Google Gemini Application'. Available at: http://gemini.google.com .
9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/ .
10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences from 
traditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .
11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'. Available at:  
https://github.com/google-research/google-research/tree/master/scann .
12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .
